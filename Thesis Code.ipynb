{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%conda install geopandas osmnx\n",
    "%pip install tensorflow\n",
    "%pip install optree\n",
    "%pip install --upgrade tensorflow keras\n",
    "%pip install xgboost\n",
    "%pip install shap\n",
    "%pip install numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import osmnx as ox\n",
    "from shapely.geometry import Point\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datetime import timedelta\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "import numpy as np \n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy.stats import shapiro, f_oneway\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import calendar\n",
    "from datetime import datetime, timedelta\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "from keras.optimizers import Nadam  \n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau  \n",
    "import matplotlib.pyplot as plt  \n",
    "from keras.optimizers import Adam, SGD, RMSprop, Adagrad, Adamax, Nadam\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. EDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset\n",
    "df = pd.read_csv('/home/u894059/.local/dataset/EVChargingStationUsage.csv')\n",
    "\n",
    "# Separate the 'Start Date' into 'start_date' and 'start_time'\n",
    "df[['start_date', 'start_time']] = df['Start Date'].str.split(' ', expand=True)\n",
    "\n",
    "# Optional: Convert 'start_date' to datetime format\n",
    "df['start_date'] = pd.to_datetime(df['start_date'])\n",
    "\n",
    "# Define start and end dates\n",
    "start_year = '01/01/2016'\n",
    "end_year = '12/31/2020'\n",
    "\n",
    "# Filter rows for dates between 2016 and 2020\n",
    "df_filtered = df[(df['start_date'] >= start_year) & (df['start_date'] <= end_year)]\n",
    "\n",
    "df_filtered.to_csv('/home/u894059/.local/dataset/EVUsage_updated.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in each column\n",
    "missing_counts = df_filtered.isnull().sum()\n",
    "print(missing_counts[missing_counts > 0])  # Show only columns with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_percentage = (df_filtered.isnull().sum() / len(df_filtered)) * 100\n",
    "print(missing_percentage[missing_percentage > 0])  # Show percentage only for columns with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(df_filtered.isnull(), cbar=False, cmap='viridis')\n",
    "plt.title('Missing Values Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each column and calculate Z-scores\n",
    "for column in df_filtered.select_dtypes(include=[np.number]).columns:  # Only apply to numeric columns\n",
    "    z_scores = np.abs(stats.zscore(df_filtered[column]))\n",
    "    outliers = df_filtered[z_scores > 3]\n",
    "    print(f\"Outliers in {column}:\\n\", outliers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to detect outliers based on Z-score\n",
    "def detect_outliers_z(df_filtered, threshold=3):\n",
    "    z_scores = np.abs(stats.zscore(df_filtered.select_dtypes(include=[np.number])))\n",
    "    outliers = (z_scores > threshold)\n",
    "    return df_filtered[~outliers.any(axis=1)]  # Filter rows with no outliers\n",
    "\n",
    "# Original descriptive statistics\n",
    "print(\"Original Data Descriptive Statistics:\\n\")\n",
    "print(df_filtered.describe())\n",
    "\n",
    "# Detect and remove outliers\n",
    "df_no_outliers = detect_outliers_z(df_filtered)\n",
    "\n",
    "# Descriptive statistics after removing outliers\n",
    "print(\"\\nDescriptive Statistics Without Outliers:\\n\")\n",
    "print(df_no_outliers.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each numerical column in the DataFrame\n",
    "for column in df_filtered.select_dtypes(include=[np.number]).columns:\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    sns.boxplot(x=df_filtered[column])\n",
    "    plt.title(f'Box Plot of {column} with Potential Outliers')\n",
    "    plt.show()\n",
    "\n",
    "# Histograms for each numeric column\n",
    "df_filtered.select_dtypes(include=[np.number]).hist(bins=30, figsize=(15, 10))\n",
    "plt.suptitle('Histograms for Each Numeric Column')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for normality on original data (with outliers)\n",
    "for column in df_filtered.select_dtypes(include=[np.number]).columns:\n",
    "    stat, p = shapiro(df_filtered[column].dropna())  # dropna to ignore NaN values\n",
    "    print(f'Shapiro-Wilk Test for {column} (with outliers): Statistics={stat:.3f}, p={p:.3f}')\n",
    "\n",
    "print(\"\\nWithout Outliers:\\n\")\n",
    "# Test for normality on data without outliers\n",
    "for column in df_no_outliers.select_dtypes(include=[np.number]).columns:\n",
    "    stat, p = shapiro(df_no_outliers[column].dropna())\n",
    "    print(f'Shapiro-Wilk Test for {column} (without outliers): Statistics={stat:.3f}, p={p:.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print each column name and its data type\n",
    "for column in df_filtered.columns:\n",
    "    print(f\"{column}: {df_filtered[column].dtype}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the duration columns to timedelta\n",
    "df_filtered['Total Duration'] = pd.to_timedelta(df_filtered['Total Duration (hh:mm:ss)'])\n",
    "df_filtered['Charging Time'] = pd.to_timedelta(df_filtered['Charging Time (hh:mm:ss)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify continuous variables\n",
    "continuous_vars = df_filtered.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Plot each continuous variable\n",
    "for var in continuous_vars:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.histplot(df_filtered[var], kde=True, bins=10)\n",
    "    plt.title(f'Distribution of {var}')\n",
    "    plt.xlabel(var)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify continuous variables\n",
    "continuous_vars = df_filtered.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Loop through each continuous variable and calculate summary statistics\n",
    "for var in continuous_vars:\n",
    "    summary_stats = df_filtered[var].describe()\n",
    "    skewness = df_filtered[var].skew()\n",
    "    kurtosis = df_filtered[var].kurtosis()\n",
    "\n",
    "    print(f\"\\nSummary Statistics for {var}:\")\n",
    "    print(summary_stats)\n",
    "\n",
    "    print(f\"\\nSkewness for {var}: {skewness:.2f}\")\n",
    "    print(f\"Kurtosis for {var}: {kurtosis:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical variables\n",
    "categorical_vars = ['Station Name', 'MAC Address', 'Org Name', 'Port Type', 'Plug Type', 'Address 1', 'City', 'State/Province', 'Country',\n",
    "'Currency', 'Ended By', 'County', 'Model Number']  \n",
    "\n",
    "\n",
    "# Loop through each categorical variable and create a count plot\n",
    "for var in categorical_vars:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.countplot(x=var, data=df_filtered)\n",
    "    plt.title(f'Frequency of Categories in {var}')\n",
    "    plt.xlabel('Categories')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xticks(rotation=45)  # Rotate x-axis labels if needed\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Multivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only numeric columns\n",
    "numeric_df = df_filtered.select_dtypes(include=[np.number])\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = numeric_df.corr()\n",
    "\n",
    "# Visualize the correlation matrix\n",
    "plt.figure(figsize=(20, 15))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='coolwarm')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Target Variable Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(df_filtered['Energy (kWh)'], kde=True, bins=30)\n",
    "plt.title('Distribution of Target Variable')\n",
    "plt.xlabel('Target Variable')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_stats = df_filtered['Energy (kWh)'].describe()\n",
    "print(\"Summary Statistics for Target Variable:\")\n",
    "print(summary_stats)\n",
    "\n",
    "# Additional metrics: skewness and kurtosis\n",
    "skewness = df_filtered['Energy (kWh)'].skew()\n",
    "kurtosis = df_filtered['Energy (kWh)'].kurtosis()\n",
    "\n",
    "print(f\"\\nSkewness: {skewness:.2f}\")\n",
    "print(f\"Kurtosis: {kurtosis:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only non-numeric columns\n",
    "non_numeric_columns = df_filtered.select_dtypes(exclude=['number']).columns\n",
    "print(\"Non-Numeric Columns:\")\n",
    "print(non_numeric_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix\n",
    "correlation_matrix = df_filtered.corr()\n",
    "\n",
    "# Display the correlation of all variables with the target variable\n",
    "target_correlation = correlation_matrix['Energy (kWh)']\n",
    "print(\"Correlation of all variables with the target variable:\")\n",
    "print(target_correlation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered['start_date'] = pd.to_datetime(df_filtered['start_date'])\n",
    "daily_demand = df_filtered.groupby(df_filtered['start_date'].dt.date)['Energy (kWh)'].sum().reset_index()\n",
    "daily_demand.columns = ['Date', 'Total Energy (kWh)']\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "# sns.lineplot(data=daily_demand, x='Date', y='Total Energy (kWh)', marker='o')\n",
    "sns.lineplot(data=daily_demand, x='Date', y='Total Energy (kWh)')\n",
    "\n",
    "# Add a vertical line for March 1, 2020\n",
    "plt.axvline(pd.to_datetime('2020-03-01'), color='red', linestyle='--', label='COVID-19 Pandemic Start')\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Daily Energy Demand (kWh)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Total Energy (kWh)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Transaction Date to datetime if you are using it\n",
    "df_filtered['start_time'] = pd.to_datetime(df_filtered['start_time'])\n",
    "\n",
    "# Extract the hour from the Transaction Date (or start_date)\n",
    "df_filtered['Hour'] = df_filtered['start_time'].dt.hour\n",
    "\n",
    "# Group by hour and calculate the average energy demand\n",
    "hourly_demand = df_filtered.groupby('Hour')['Energy (kWh)'].mean().reset_index()\n",
    "hourly_demand.columns = ['Hour', 'Average Energy (kWh)']\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=hourly_demand, x='Hour', y='Average Energy (kWh)')\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Average Energy Demand by Hour of the Day')\n",
    "plt.xlabel('Hour of the Day')\n",
    "plt.ylabel('Average Energy (kWh)')\n",
    "plt.xticks(range(0, 24))  # Set x-ticks for all hours\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_hour = hourly_demand.loc[hourly_demand['Average Energy (kWh)'].idxmax()]\n",
    "print(f\"Peak hour is {peak_hour['Hour']} with an average energy demand of {peak_hour['Average Energy (kWh)']} kWh.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered['start_date'] = pd.to_datetime(df_filtered['start_date'])\n",
    "\n",
    "# Extract the day of the week (0=Monday, 6=Sunday)\n",
    "df_filtered['Day of Week'] = df_filtered['start_date'].dt.day_name()\n",
    "\n",
    "# Group by day of the week and calculate the average energy demand\n",
    "weekly_demand = df_filtered.groupby('Day of Week')['Energy (kWh)'].mean().reset_index()\n",
    "weekly_demand.columns = ['Day of Week', 'Average Energy (kWh)']\n",
    "\n",
    "# Define the order of the days for proper plotting\n",
    "day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "weekly_demand['Day of Week'] = pd.Categorical(weekly_demand['Day of Week'], categories=day_order, ordered=True)\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=weekly_demand, x='Day of Week', y='Average Energy (kWh)')\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Average Energy Demand by Day of the Week')\n",
    "plt.xlabel('Day of the Week')\n",
    "plt.ylabel('Average Energy (kWh)')\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = pd.read_csv('/Users/yeelam/Documents/Thesis/data/EVUsage_updated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine if the day is a weekend or weekday\n",
    "df_filtered['start_date'] = pd.to_datetime(df_filtered['start_date'])\n",
    "df_filtered['IsWeekend'] = df_filtered['start_date'].dt.dayofweek >= 5  # Saturday=5, Sunday=6\n",
    "\n",
    "# Group by weekend/weekday and calculate the average energy demand\n",
    "weekend_demand = df_filtered.groupby('IsWeekend')['Energy (kWh)'].mean().reset_index()\n",
    "weekend_demand['IsWeekend'] = weekend_demand['IsWeekend'].map({False: 'Weekday', True: 'Weekend'})\n",
    "weekend_demand.columns = ['DayType', 'Average Energy (kWh)']\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(data=weekend_demand, x='DayType', y='Average Energy (kWh)')\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Average Energy Demand: Weekday vs Weekend')\n",
    "plt.xlabel('Day Type')\n",
    "plt.ylabel('Average Energy (kWh)')\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by hour and day type\n",
    "hourly_daytype_demand = df_filtered.groupby(['Hour', 'IsWeekend'])['Energy (kWh)'].mean().reset_index()\n",
    "hourly_daytype_demand['IsWeekend'] = hourly_daytype_demand['IsWeekend'].map({False: 'Weekday', True: 'Weekend'})\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(data=hourly_daytype_demand, x='Hour', y='Energy (kWh)', hue='IsWeekend', marker='o')\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Average Energy Demand by Hour (Weekday vs Weekend)')\n",
    "plt.xlabel('Hour of the Day')\n",
    "plt.ylabel('Average Energy (kWh)')\n",
    "plt.xticks(range(0, 24))\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pivot table for heatmap\n",
    "heatmap_data = df_filtered.pivot_table(\n",
    "    index=df_filtered['start_date'].dt.day_name(), \n",
    "    columns=df_filtered['Hour'], \n",
    "    values='Energy (kWh)', \n",
    "    aggfunc='mean'\n",
    ")\n",
    "\n",
    "# Sort days of the week\n",
    "heatmap_data = heatmap_data.reindex(['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(14, 7))\n",
    "sns.heatmap(heatmap_data, cmap=\"YlGnBu\", annot=True, fmt=\".1f\", cbar_kws={'label': 'Average Energy (kWh)'})\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Average Energy Demand Heatmap (Hour vs. Day of Week)')\n",
    "plt.xlabel('Hour of the Day')\n",
    "plt.ylabel('Day of the Week')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# Aggregate data to daily level\n",
    "daily_data = df_filtered.resample('D', on='start_date')['Energy (kWh)'].sum()\n",
    "\n",
    "# Perform decomposition\n",
    "decomposition = seasonal_decompose(daily_data, model='additive', period=365)\n",
    "\n",
    "# Plot components\n",
    "decomposition.plot()\n",
    "plt.suptitle('Seasonality Decomposition of Daily Energy Demand', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import autocorrelation_plot\n",
    "\n",
    "# Aggregate to daily data\n",
    "daily_data = df_filtered.resample('D', on='start_date')['Energy (kWh)'].sum()\n",
    "\n",
    "# Plot auto-correlation\n",
    "plt.figure(figsize=(10, 6))\n",
    "autocorrelation_plot(daily_data)\n",
    "plt.title('Auto-Correlation of Daily Energy Demand')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract month and hour\n",
    "df_filtered['start_date'] = pd.to_datetime(df_filtered['start_date'])\n",
    "df_filtered['Month'] = df_filtered['start_date'].dt.month_name()\n",
    "hourly_monthly_data = df_filtered.groupby(['Month', 'Hour'])['Energy (kWh)'].mean().reset_index()\n",
    "\n",
    "# Sort months\n",
    "month_order = ['January', 'February', 'March', 'April', 'May', 'June', \n",
    "               'July', 'August', 'September', 'October', 'November', 'December']\n",
    "hourly_monthly_data['Month'] = pd.Categorical(hourly_monthly_data['Month'], categories=month_order, ordered=True)\n",
    "\n",
    "# Create Facet Grid\n",
    "g = sns.FacetGrid(hourly_monthly_data, col=\"Month\", col_wrap=4, height=3, aspect=1.5)\n",
    "g.map(sns.lineplot, 'Hour', 'Energy (kWh)', marker='o')\n",
    "\n",
    "# Add titles and labels\n",
    "g.set_titles(\"{col_name}\")\n",
    "g.set_axis_labels('Hour of the Day', 'Average Energy (kWh)')\n",
    "g.fig.suptitle('Hourly Trends by Month', y=1.02, fontsize=16)\n",
    "g.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate rolling average\n",
    "df_filtered['RollingMean'] = df_filtered['Energy (kWh)'].rolling(window=7).mean()\n",
    "\n",
    "# Plot trends\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df_filtered['start_date'], df_filtered['Energy (kWh)'], label='Daily Demand', alpha=0.5)\n",
    "plt.plot(df_filtered['start_date'], df_filtered['RollingMean'], label='7-Day Rolling Average', color='orange')\n",
    "plt.legend()\n",
    "plt.title('Rolling Average of Daily Energy Demand')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Energy (kWh)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the month\n",
    "df_filtered['start_date'] = pd.to_datetime(df_filtered['start_date'])\n",
    "df_filtered['Month'] = df_filtered['start_date'].dt.month_name()\n",
    "\n",
    "# Group by month and calculate the average energy demand\n",
    "monthly_demand = df_filtered.groupby('Month')['Energy (kWh)'].mean().reset_index()\n",
    "monthly_demand.columns = ['Month', 'Average Energy (kWh)']\n",
    "\n",
    "# Sort months\n",
    "month_order = ['January', 'February', 'March', 'April', 'May', 'June', \n",
    "               'July', 'August', 'September', 'October', 'November', 'December']\n",
    "monthly_demand['Month'] = pd.Categorical(monthly_demand['Month'], categories=month_order, ordered=True)\n",
    "monthly_demand = monthly_demand.sort_values('Month')\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=monthly_demand, x='Month', y='Average Energy (kWh)')\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Average Energy Demand by Month')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Average Energy (kWh)')\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the dataset\n",
    "df_filtered = pd.read_csv('/Users/yeelam/Documents/Thesis/data/EVUsage_updated.csv')\n",
    "\n",
    "# Check the first few rows to understand its structure\n",
    "print(df_filtered.head())\n",
    "\n",
    "# Check for missing values and data types\n",
    "print(df_filtered.info())\n",
    "\n",
    "# Convert 'start_date' to datetime format\n",
    "df_filtered['start_date'] = pd.to_datetime(df_filtered['start_date'])  # Convert the 'start_date' column to datetime\n",
    "\n",
    "# Extract the week number from the 'start_date' column\n",
    "df_filtered['Week'] = df_filtered['start_date'].dt.isocalendar().week  # Extract the ISO week number\n",
    "df_filtered['Week'] = df_filtered['Week'].astype(int)\n",
    "\n",
    "\n",
    "# Now, group by 'Week' and calculate the average energy usage\n",
    "weekly_demand = df_filtered.groupby('Week').agg({'Energy (kWh)': 'mean'}).reset_index()\n",
    "\n",
    "# Rename the column for clarity\n",
    "weekly_demand = weekly_demand.rename(columns={'Energy (kWh)': 'Average Energy (kWh)'})\n",
    "\n",
    "# Check the cleaned data\n",
    "print(weekly_demand.head())\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(data=weekly_demand, x='Week', y='Average Energy (kWh)', marker='o')\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Average Energy Demand by Week')\n",
    "plt.xlabel('Week')\n",
    "plt.ylabel('Average Energy (kWh)')\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "# Remove duplicate latitude and longitude pairs\n",
    "df_filtered_unique = df_filtered.drop_duplicates(subset=['Latitude', 'Longitude'])\n",
    "\n",
    "# Initialize a map centered around Palo Alto\n",
    "palo_alto_map = folium.Map(location=[37.4419, -122.1430], zoom_start=12)  # Palo Alto coordinates\n",
    "\n",
    "# Add charging station locations to the map\n",
    "for index, row in df_filtered_unique.iterrows():\n",
    "    folium.Marker(\n",
    "        location=[row['Latitude'], row['Longitude']],\n",
    "        popup=f\"Energy (kWh): {row['Energy (kWh)']}\"\n",
    "    ).add_to(palo_alto_map)\n",
    "\n",
    "# Save the map to an HTML file or display it\n",
    "palo_alto_map.save(\"ev_charging_stations_map_unique.html\")\n",
    "palo_alto_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the data types\n",
    "print(weekly_demand.dtypes)\n",
    "\n",
    "# Inspect the first few rows\n",
    "print(weekly_demand.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "\n",
    "# Assuming you have already filtered your dataframe (df_filtered) to include the relevant columns\n",
    "# and it contains the columns: 'Latitude', 'Longitude', 'Energy (kWh)'\n",
    "\n",
    "# Initialize a map centered around Palo Alto\n",
    "palo_alto_map = folium.Map(location=[37.4419, -122.1430], zoom_start=12)\n",
    "\n",
    "# Prepare data for HeatMap (latitude, longitude, and optionally, weights like energy usage)\n",
    "heat_data = df_filtered[['Latitude', 'Longitude', 'Energy (kWh)']].values.tolist()\n",
    "\n",
    "# Add HeatMap to the map\n",
    "HeatMap(heat_data, radius=15).add_to(palo_alto_map)\n",
    "\n",
    "# Save the map to an HTML file\n",
    "palo_alto_map.save(\"ev_charging_stations_heatmap.html\")\n",
    "\n",
    "# Display the map (in Jupyter or Python environments that support rendering)\n",
    "palo_alto_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df_filtered['Energy (kWh)'], bins=30, kde=True)\n",
    "plt.title('Distribution of Energy Consumption')\n",
    "plt.xlabel('Energy (kWh)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by date and sum energy consumption\n",
    "daily_consumption = df_filtered.groupby('start_date')['Energy (kWh)'].sum().reset_index()\n",
    "plt.plot(daily_consumption['start_date'], daily_consumption['Energy (kWh)'])\n",
    "plt.title('Daily Energy Consumption Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Total Energy (kWh)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='Port Type', data=df_filtered)\n",
    "plt.title('Count of Charging Stations by Port Type')\n",
    "plt.xlabel('Port Type')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Processing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Removing Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = pd.read_csv('/home/u894059/.local/dataset/EVUsage_updated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df = df_filtered.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constant_columns = base_df.columns[base_df.nunique() == 1]\n",
    "\n",
    "# Print the columns that are going to be dropped\n",
    "print(\"Columns with constant values (dropped):\", list(constant_columns))\n",
    "\n",
    "base_final = base_df.copy(deep=True)\n",
    "base_final = base_final.drop(columns=constant_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of unique values for each column\n",
    "unique_counts = base_final.nunique()\n",
    "\n",
    "# Display the number of unique values per column\n",
    "print(unique_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['Start Date', 'Start Time Zone', 'End Time Zone', 'Currency', 'Total Duration (hh:mm:ss)', 'Charging Time (hh:mm:ss)']\n",
    "base_final = base_final.drop(columns=columns_to_drop)\n",
    "\n",
    "# Save the cleaned DataFrame\n",
    "base_final.to_csv('/home/u894059/.local/dataset/final_dataset_cleaned.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Imputing Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the 'End Date' and 'Transaction Date (Pacific Time)'\n",
    "base_final = pd.read_csv('/home/u894059/.local/dataset/final_dataset_cleaned.csv')\n",
    "base_final[['end_date', 'end_time']] = base_final['End Date'].str.split(' ', expand=True)\n",
    "base_final[['transaction_date', 'transaction_time']] = base_final['Transaction Date (Pacific Time)'].str.split(' ', expand=True)\n",
    "base_final['end_date'] = base_final['end_date'].fillna(base_final['transaction_date'])\n",
    "base_final['end_time'] = base_final['end_time'].fillna(base_final['transaction_time'])\n",
    "base_final = base_final.drop(columns=['End Date','Transaction Date (Pacific Time)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values for transaction_date and transaction_time\n",
    "base_final['transaction_date'] = base_final['transaction_date'].fillna(base_final['end_date'])\n",
    "base_final['transaction_time'] = base_final['transaction_time'].fillna(base_final['end_time'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values for 'EVSE ID' and 'County'\n",
    "cleaning_df = base_final.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping for 'EVSE ID' and 'County' based on 'Address 1'\n",
    "evse_mapping = cleaning_df.dropna(subset=['EVSE ID']).groupby('Address 1')['EVSE ID'].first().to_dict()\n",
    "county_mapping = cleaning_df.dropna(subset=['County']).groupby('Address 1')['County'].first().to_dict()\n",
    "\n",
    "# Fill missing values in 'EVSE ID' and 'County' using the mapping\n",
    "cleaning_df['EVSE ID'] = cleaning_df['EVSE ID'].fillna(cleaning_df['Address 1'].map(evse_mapping))\n",
    "cleaning_df['County'] = cleaning_df['County'].fillna(cleaning_df['Address 1'].map(county_mapping))\n",
    "\n",
    "print(\"\\nDataFrame after filling missing values:\")\n",
    "print(cleaning_df[['EVSE ID','County','Address 1']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned DataFrame\n",
    "cleaning_df.to_csv('/home/u894059/.local/dataset/cleaning.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Converting Categorical Data to Numerical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaning_df = pd.read_csv('/home/u894059/.local/dataset/cleaning.csv')\n",
    "# Print each column name and its data type\n",
    "for column in cleaning_df.columns:\n",
    "    print(f\"{column}: {cleaning_df[column].dtype}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_numerical_cols = cleaning_df.select_dtypes(exclude=['number']).columns\n",
    "\n",
    "# Print the non-numerical columns\n",
    "print(\"Non-Numerical Columns:\")\n",
    "print(non_numerical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_time_columns = ['start_date', 'start_time', 'end_date', 'end_time', \n",
    "                     'transaction_date', 'transaction_time']\n",
    "                     \n",
    "for column in date_time_columns:\n",
    "    cleaning_df[column] = pd.to_datetime(cleaning_df[column], errors='coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaning_df['Station Name'] = cleaning_df['Station Name'].str.replace('PALO ALTO CA /', '', regex=False).str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over non-numerical columns and print unique values for each\n",
    "for col in non_numerical_cols:\n",
    "    print(f\"Unique values for {col}: {cleaning_df[col].unique()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaning_df = cleaning_df.drop('User ID', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with missing values in 'Ended By' and 'Energy (kWh)' columns\n",
    "df_clean = cleaning_df.dropna(subset=['Ended By', 'Energy (kWh)'])\n",
    "\n",
    "# Step 2: Group data by the 'Ended By' categories\n",
    "groups = [df_clean[df_clean['Ended By'] == category]['Energy (kWh)'] for category in df_clean['Ended By'].unique()]\n",
    "\n",
    "# Step 3: Perform ANOVA test\n",
    "f_stat, p_value = stats.f_oneway(*groups)\n",
    "\n",
    "# Step 4: Output the ANOVA result\n",
    "print(f\"ANOVA p-value for 'Ended By' column: {p_value}\")\n",
    "\n",
    "# Interpretation\n",
    "if p_value < 0.05:\n",
    "    print(\"The 'Ended By' categories have a statistically significant impact on 'Energy (kWh)'.\")\n",
    "else:\n",
    "    print(\"The 'Ended By' categories do not have a statistically significant impact on 'Energy (kWh)'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the columns to be one-hot encoded\n",
    "covert_columns = ['Port Type', 'Plug Type', 'Ended By', 'County', 'Model Number']\n",
    "\n",
    "# Initialize the OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse=False, dtype=int)\n",
    "\n",
    "# Perform one-hot encoding on the specified columns\n",
    "one_hot_encoded = encoder.fit_transform(cleaning_df[covert_columns])\n",
    "\n",
    "# Convert the result to a DataFrame with appropriate column names\n",
    "one_hot_encoded_df = pd.DataFrame(one_hot_encoded, columns=encoder.get_feature_names_out(covert_columns))\n",
    "\n",
    "# Concatenate the original DataFrame without the one-hot encoded columns and the one-hot encoded DataFrame\n",
    "df_one_hot = pd.concat([cleaning_df.drop(columns=covert_columns), one_hot_encoded_df], axis=1)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_one_hot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency encoding function\n",
    "def frequency_encode(df, column):\n",
    "    frequency = df[column].value_counts() / len(df)  # Calculate the frequency\n",
    "    df[column] = df[column].map(frequency)  # Map the frequencies to the column\n",
    "    return df\n",
    "\n",
    "# Apply frequency encoding to specified columns\n",
    "columns_to_encode = ['Station Name', 'MAC Address', 'Address 1']\n",
    "\n",
    "for column in columns_to_encode:\n",
    "    df_one_hot = frequency_encode(df_one_hot, column)\n",
    "\n",
    "# Check the result\n",
    "print(df_one_hot[['Station Name', 'MAC Address', 'Address 1']].tail(50))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one_hot.to_csv('/home/u894059/.local/dataset/cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix for all numeric columns\n",
    "correlations = df_one_hot.corr()\n",
    "\n",
    "# Set display options to show all rows\n",
    "pd.set_option('display.max_rows', None)  # None will display all rows\n",
    "\n",
    "# Display the correlation of all columns with 'Energy (kWh)', sorted in descending order\n",
    "print(correlations['Energy (kWh)'].sort_values(ascending=False))\n",
    "\n",
    "# Reset display options to default (optional)\n",
    "pd.reset_option('display.max_rows')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a correlation threshold (e.g., 0.1 or -0.1) to remove columns with low correlation\n",
    "correlation_threshold = 0.03\n",
    "\n",
    "# Identify columns with correlation below the threshold (including the target column itself)\n",
    "low_corr_cols = correlations['Energy (kWh)'][abs(correlations['Energy (kWh)']) < correlation_threshold].index\n",
    "\n",
    "# Drop these columns\n",
    "df_reduced = df_one_hot.drop(columns=low_corr_cols)\n",
    "\n",
    "# Display the remaining columns\n",
    "print(\"Columns after removing low correlation features:\", df_reduced.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the size of the figure (increase even further for better visibility)\n",
    "plt.figure(figsize=(24, 18))  # Increase the figure size significantly\n",
    "\n",
    "# Plot the heatmap\n",
    "sns.heatmap(correlations, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5, annot_kws={\"size\": 10})\n",
    "\n",
    "# Rotate column and row labels for better visibility\n",
    "plt.xticks(rotation=90, ha='center')  # Rotate x-axis labels (column names) and align them horizontally\n",
    "plt.yticks(rotation=0)  # Keep y-axis labels (row names) horizontal\n",
    "\n",
    "# Set plot title\n",
    "plt.title('Correlation Matrix of Features', fontsize=18)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only numerical columns\n",
    "X = df_one_hot.select_dtypes(include=['number']).drop(columns=['Energy (kWh)'])\n",
    "y = df_one_hot['Energy (kWh)']\n",
    "\n",
    "# Convert data to DMatrix format (XGBoost's optimized data format)\n",
    "dtrain = xgb.DMatrix(X, label=y, missing=np.nan)\n",
    "\n",
    "# Set the XGBoost parameters\n",
    "params = {\n",
    "    'objective': 'reg:squarederror',  # Use 'reg:squarederror' for regression problems\n",
    "    'eval_metric': 'rmse',            # Root mean squared error metric\n",
    "    'max_depth': 6,                   # Maximum depth of the trees\n",
    "    'learning_rate': 0.1              # Step size for updating weights\n",
    "}\n",
    "\n",
    "# Train the XGBoost model\n",
    "num_boost_round = 100  # Number of boosting rounds (trees)\n",
    "model = xgb.train(params, dtrain, num_boost_round=num_boost_round)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = model.get_fscore()\n",
    "\n",
    "# Convert to a DataFrame and sort by importance score\n",
    "importance_df = pd.DataFrame(list(feature_importances.items()), columns=[\"Feature\", \"Importance\"])\n",
    "importance_df = importance_df.sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# Display the feature importances\n",
    "print(importance_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importances as a bar plot\n",
    "importance_df = importance_df.sort_values(by=\"Importance\", ascending=False)\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(importance_df[\"Feature\"], importance_df[\"Importance\"])\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.title(\"Feature Importances\")\n",
    "plt.gca().invert_yaxis()  # Reverse the y-axis to show the most important features at the top\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced.to_csv('/home/u894059/.local/dataset/reduced.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Feature Engineering - Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Temporal Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = pd.read_csv('/home/u894059/.local/dataset/cleaned.csv', index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute 'end_date' with 'transaction_date' where 'end_date' is NaN\n",
    "df_cleaned['end_date'] = df_cleaned['end_date'].fillna(df_cleaned['start_date'])\n",
    "df_cleaned['transaction_date'] = df_cleaned['transaction_date'].fillna(df_cleaned['start_date'])\n",
    "\n",
    "# Impute 'end_time' with 'transaction_time' where 'end_time' is NaN\n",
    "df_cleaned['end_time'] = df_cleaned['end_time'].fillna(df_cleaned['start_time'])\n",
    "df_cleaned['transaction_time'] = df_cleaned['transaction_time'].fillna(df_cleaned['start_time'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal Features\n",
    "# Convert date and time columns to datetime format\n",
    "df_cleaned['start_date'] = pd.to_datetime(df_cleaned['start_date'], format='%Y/%m/%d')\n",
    "df_cleaned['end_date'] = pd.to_datetime(df_cleaned['end_date'], format='%Y/%m/%d')\n",
    "df_cleaned['start_time'] = pd.to_datetime(df_cleaned['start_time'], format='%Y/%m/%d')\n",
    "df_cleaned['end_time'] = pd.to_datetime(df_cleaned['end_time'], format='%Y/%m/%d')\n",
    "\n",
    "# Extract date features\n",
    "df_cleaned['start_year'] = df_cleaned['start_date'].dt.year\n",
    "df_cleaned['start_month'] = df_cleaned['start_date'].dt.month\n",
    "df_cleaned['start_day'] = df_cleaned['start_date'].dt.day\n",
    "df_cleaned['start_weekday'] = df_cleaned['start_date'].dt.dayofweek  # 0=Monday, 6=Sunday\n",
    "\n",
    "df_cleaned['end_year'] = df_cleaned['end_date'].dt.year\n",
    "df_cleaned['end_month'] = df_cleaned['end_date'].dt.month\n",
    "df_cleaned['end_day'] = df_cleaned['end_date'].dt.day\n",
    "df_cleaned['end_weekday'] = df_cleaned['end_date'].dt.dayofweek\n",
    "\n",
    "# Extract time features\n",
    "df_cleaned['start_hour'] = df_cleaned['start_time'].dt.hour\n",
    "df_cleaned['start_minute'] = df_cleaned['start_time'].dt.minute\n",
    "\n",
    "df_cleaned['end_hour'] = df_cleaned['end_time'].dt.hour\n",
    "df_cleaned['end_minute'] = df_cleaned['end_time'].dt.minute\n",
    "\n",
    "# Calculate transaction duration\n",
    "df_cleaned['transaction_duration'] = (df_cleaned['end_time'] - df_cleaned['start_time']).dt.total_seconds() / 60  # duration in minutes\n",
    "\n",
    "# Optional: Define seasonal features based on month\n",
    "def get_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'Winter'\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'Spring'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Summer'\n",
    "    else:\n",
    "        return 'Fall'\n",
    "\n",
    "df_cleaned['start_season'] = df_cleaned['start_month'].apply(get_season)\n",
    "df_cleaned['end_season'] = df_cleaned['end_month'].apply(get_season)\n",
    "\n",
    "# Display the updated DataFrame with new temporal features\n",
    "print(df_cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned['is_weekend'] = df_cleaned['start_weekday'] >= 5\n",
    "df_cleaned['day_part'] = pd.cut(df_cleaned['start_hour'], bins=[0, 6, 12, 18, 24], labels=['night', 'morning', 'afternoon', 'evening'], right = False)\n",
    "print(df_cleaned.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "print(df_cleaned.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Baseline Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['EVSE ID', 'Driver Postal Code','System S/N', 'County_San Mateo County',\t'County_Santa Clara County', 'County_nan', 'Model Number_CT2000-HD-CCR', 'Model Number_CT2000-HD-GW1-CCR', 'Model Number_CT2100-HD-CCR', 'Model Number_CT2100-HD-CDMA-CCR', 'Model Number_CT4010-HD-GW', 'Model Number_CT4020-HD', 'Model Number_CT4020-HD-GW', 'Model Number_CTHCR-S', 'Model Number_CTHDR', 'Model Number_CTHDR-S', 'Model Number_nan']\n",
    "baseline_df = df_cleaned.drop(columns=cols_to_drop, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_df['hour_sin'] = np.sin(2 * np.pi * baseline_df['start_hour'] / 24)\n",
    "baseline_df['hour_cos'] = np.cos(2 * np.pi * baseline_df['start_hour'] / 24)\n",
    "baseline_df['weekday_sin'] = np.sin(2 * np.pi * baseline_df['start_weekday'] / 7)\n",
    "baseline_df['weekday_cos'] = np.cos(2 * np.pi * baseline_df['start_weekday'] / 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert boolean to integer\n",
    "baseline_df['is_weekend'] = baseline_df['is_weekend'].astype(int)\n",
    "baseline_df['transaction_date'] = pd.to_datetime(baseline_df['transaction_date'], format='%Y/%m/%d')\n",
    "baseline_df['transaction_time'] = pd.to_datetime(baseline_df['transaction_time'], format='%Y/%m/%d %H:%M')\n",
    "# Extract useful features from 'transaction_date'\n",
    "baseline_df['transaction_year'] = baseline_df['transaction_date'].dt.year\n",
    "baseline_df['transaction_month'] = baseline_df['transaction_date'].dt.month\n",
    "baseline_df['transaction_day'] = baseline_df['transaction_date'].dt.day\n",
    "baseline_df['transaction_weekday'] = baseline_df['transaction_date'].dt.weekday  # Monday=0, Sunday=6\n",
    "# Extract useful features from 'transaction_time'\n",
    "baseline_df['transaction_hour'] = baseline_df['transaction_time'].dt.hour\n",
    "baseline_df['transaction_minute'] = baseline_df['transaction_time'].dt.minute\n",
    "baseline_df['transaction_second'] = baseline_df['transaction_time'].dt.second\n",
    "\n",
    "categorical_columns = ['start_season', 'end_season','day_part']\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False, dtype=int, handle_unknown='ignore')\n",
    "\n",
    "# Fit and transform the selected categorical columns\n",
    "encoded_features = encoder.fit_transform(baseline_df[categorical_columns])\n",
    "\n",
    "encoded_features_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names_out(categorical_columns))\n",
    "\n",
    "\n",
    "# Concatenate the encoded columns with the original DataFrame (excluding the original categorical columns)\n",
    "baseline_df = pd.concat([baseline_df.drop(columns=categorical_columns), encoded_features_df], axis=1)\n",
    "\n",
    "baseline_df = baseline_df.drop(['start_date', 'start_time', 'end_date', 'end_time', 'transaction_date', 'transaction_time'], axis=1)\n",
    "\n",
    "# Check for missing values after encoding\n",
    "print(baseline_df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_df.to_csv('/home/u894059/.local/dataset/baseline_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Additional Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Holiday Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of public holidays\n",
    "public_holidays = [\n",
    "    # 2016 Holidays\n",
    "    '01/01/2016', '01/18/2016', '02/15/2016', '03/31/2016', \n",
    "    '05/30/2016', '07/04/2016', '09/05/2016', '11/11/2016', \n",
    "    '11/24/2016', '11/25/2016', '12/26/2016',\n",
    "    \n",
    "    # 2017 Holidays\n",
    "    '01/02/2017', '01/16/2017', '02/20/2017', '03/31/2017', \n",
    "    '05/29/2017', '07/04/2017', '09/04/2017', '11/10/2017', \n",
    "    '11/23/2017', '11/24/2017', '12/25/2017',\n",
    "    \n",
    "    # 2018 Holidays\n",
    "    '01/01/2018', '01/15/2018', '02/19/2018', '03/31/2018', \n",
    "    '05/28/2018', '07/04/2018', '09/03/2018', '10/08/2018', \n",
    "    '11/12/2018', '11/22/2018', '11/23/2018', '12/25/2018',\n",
    "    \n",
    "    # 2019 Holidays\n",
    "    '01/01/2019', '01/21/2019', '02/18/2019', '04/01/2019', \n",
    "    '05/27/2019', '07/04/2019', '09/02/2019', '11/11/2019', \n",
    "    '11/28/2019', '11/29/2019', '12/25/2019',\n",
    "    \n",
    "    # 2020 Holidays\n",
    "    '01/01/2020', '01/20/2020', '02/17/2020', '03/31/2020', \n",
    "    '05/25/2020', '07/04/2020', '09/07/2020', '11/11/2020', \n",
    "    '11/26/2020', '11/27/2020', '12/25/2020'\n",
    "]\n",
    "\n",
    "# Convert the list to datetime format\n",
    "public_holidays = pd.to_datetime(public_holidays)\n",
    "\n",
    "# Add a new column called 'holiday' and set values based on the public holidays\n",
    "df_cleaned['holiday'] = df_cleaned['start_date'].isin(public_holidays).astype(int)\n",
    "\n",
    "# Save the updated DataFrame back to a CSV file\n",
    "df_cleaned.to_csv('/home/u894059/.local/dataset/EVUsage_updated.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Weather Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather = pd.read_csv('/home/u894059/.local/dataset/weather_conditions.csv')\n",
    "\n",
    "# Separate the 'Start Date' into 'start_date' and 'start_time'\n",
    "df_weather[['weather_date', 'weather_time']] = df_weather['time'].str.split('T', expand=True)\n",
    "\n",
    "# Save the updated DataFrame back to a CSV file\n",
    "df_weather.to_csv('/home/u894059/.local/dataset/weather_updated.csv', index=False)\n",
    "\n",
    "df_weather['weather_date'] = pd.to_datetime(df_weather['weather_date'])\n",
    "\n",
    "df_weather['weather_hour'] = pd.to_datetime(df_weather['weather_time'], format='%H:%M').dt.hour\n",
    "\n",
    "result = pd.merge(df_cleaned, df_weather, left_on=['start_date', 'start_hour'], right_on=['weather_date', 'weather_hour'], how='left')\n",
    "\n",
    "result.to_csv('/home/u894059/.local/dataset/combined_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['weather_date', 'weather_time', 'weather_hour']\n",
    "base_final = result.drop(columns=columns_to_drop)\n",
    "\n",
    "# Save the cleaned DataFrame\n",
    "base_final.to_csv('/home/u894059/.local/dataset/final_dataset_cleaned.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Geo Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your DataFrame is named df\n",
    "unique_pairs = base_final[['Latitude', 'Longitude']].drop_duplicates()\n",
    "\n",
    "num_unique_pairs = unique_pairs.shape[0]  # or len(unique_pairs)\n",
    "print(\"Unique Pairs:\")\n",
    "print(unique_pairs)\n",
    "print(f\"\\nNumber of unique pairs: {num_unique_pairs}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a distance in meters\n",
    "radius = 10000\n",
    "\n",
    "# Initialize a results list\n",
    "results = []\n",
    "\n",
    "# Loop through unique pairs to query OSM\n",
    "for index, row in unique_pairs.iterrows():\n",
    "    lat, lon = row['Latitude'], row['Longitude']\n",
    "\n",
    "    # Create a Point geometry for the location\n",
    "    point = Point(lon, lat)  # Note: (lon, lat) order for Point\n",
    "\n",
    "    # Create a GeoDataFrame for the point\n",
    "    gdf_point = gpd.GeoDataFrame(geometry=[point], crs='EPSG:4326')\n",
    "\n",
    "    try:\n",
    "        # Use OSMnx to get amenities within the buffer\n",
    "        amenities = ox.features_from_point((lat, lon), tags={\n",
    "            'amenity': ['university','train_station', 'bus_stop', 'taxi', 'bicycle_parking', 'school',\n",
    "            'college', 'library', 'kindergarten', 'hospital', 'clinic', 'cinema', 'theatre', 'sports_centre', 'stadium', 'park', 'swimming_pool','bank',\n",
    "            'restaurant', 'cafe', 'bar', 'pub', 'fast_food', 'supermarket', 'convenience', 'department_store', 'bakery', 'shopping_mall', 'post_office',\n",
    "            'hotel']\n",
    "        }, dist=radius)\n",
    "\n",
    "        amenity_types = ['bus_stop', 'taxi', 'train_station', 'university', 'bicycle_parking', \n",
    "                 'school', 'college', 'library', 'kindergarten', 'hospital', 'clinic', \n",
    "                 'cinema', 'theatre', 'sports_centre', 'stadium', 'park', 'swimming_pool', \n",
    "                 'bank', 'restaurant', 'cafe', 'bar', 'pub', 'fast_food', 'supermarket', \n",
    "                 'convenience', 'department_store', 'bakery', 'shopping_mall', \n",
    "                 'post_office', 'hotel']\n",
    "        # Dictionary to hold counts\n",
    "        amenity_counts = {}\n",
    "\n",
    "        # Count specific amenities\n",
    "        for amenity in amenity_types:\n",
    "            amenity_counts[amenity] = amenities[amenities['amenity'] == amenity].shape[0]        \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching amenities for ({lat}, {lon}): {e}\")\n",
    "        continue  # Skip to the next iteration\n",
    "\n",
    "    # Store results for this unique pair\n",
    "    results.append({\n",
    "        'Latitude': lat,\n",
    "        'Longitude': lon,\n",
    "        **{amenity.replace('_', ' ').title(): count for amenity, count in amenity_counts.items()}\n",
    "    })\n",
    "\n",
    "# Create a DataFrame to display the results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Print the results\n",
    "if results_df.empty:\n",
    "    print(\"\\nNo amenities found for the provided locations.\")\n",
    "else:\n",
    "    print(\"\\nResults:\")\n",
    "    print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('/home/u894059/.local/dataset/spatial_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_csv('/home/u894059/.local/dataset/spatial_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_final = pd.read_csv('/home/u894059/.local/dataset/final_dataset_cleaned.csv')\n",
    "\n",
    "base_final['temp_index'] = base_final.index\n",
    "\n",
    "# Perform the merge\n",
    "merged_df = pd.merge(base_final, results_df, on=['Latitude', 'Longitude'])\n",
    "\n",
    "# Sort by the temporary index to preserve the order of base_final\n",
    "merged_df = merged_df.sort_values('temp_index').drop(columns=['temp_index'])\n",
    "\n",
    "# Save the merged DataFrame\n",
    "merged_df.to_csv('/home/u894059/.local/dataset/final_dataset.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_columns_mask = (merged_df == 0).all(axis=0)\n",
    "\n",
    "# Check the boolean mask\n",
    "print(\"Columns with all zeros:\", zero_columns_mask[zero_columns_mask].index.tolist())\n",
    "\n",
    "# Filter the DataFrame to keep only columns that are not all zeros\n",
    "base_cleaned = merged_df.loc[:, ~zero_columns_mask]\n",
    "\n",
    "base_cleaned = base_cleaned.dropna(axis=1, how='all')\n",
    "\n",
    "# Display the merged DataFrame\n",
    "base_cleaned.to_csv('/home/u894059/.local/dataset/base_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Interaction Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_cleaned = pd.read_csv('/home/u894059/.local/dataset/base_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['EVSE ID', 'Driver Postal Code','System S/N', 'County_San Mateo County',\t'County_Santa Clara County', 'County_nan', 'Model Number_CT2000-HD-CCR', 'Model Number_CT2000-HD-GW1-CCR', 'Model Number_CT2100-HD-CCR', 'Model Number_CT2100-HD-CDMA-CCR', 'Model Number_CT4010-HD-GW', 'Model Number_CT4020-HD', 'Model Number_CT4020-HD-GW', 'Model Number_CTHCR-S', 'Model Number_CTHDR', 'Model Number_CTHDR-S', 'Model Number_nan']\n",
    "final_reduced = final_cleaned.drop(columns=cols_to_drop, errors='ignore')\n",
    "final_reduced.to_csv('/home/u894059/.local/dataset/final_reduced.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_reduced = pd.read_csv('/home/u894059/.local/dataset/final_reduced.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_reduced['fee_start_hour'] = final_reduced['Fee'] * final_reduced['start_hour']\n",
    "final_reduced['start_hour_is_weekend'] = final_reduced['start_hour'] * final_reduced['is_weekend']\n",
    "final_reduced['start_hour_holiday'] = final_reduced['start_hour'] * final_reduced['holiday']\n",
    "final_reduced['temperature_hour'] = final_reduced['temperature_2m (°C)'] * final_reduced['start_hour']\n",
    "final_reduced['rain_hour'] = final_reduced['rain (mm)'] * final_reduced['start_hour']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Pearson correlation between each feature and the target variable\n",
    "correlations = final_reduced.corr()['Energy (kWh)'].sort_values(ascending=False)\n",
    "\n",
    "# Set display options to show more rows\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Display the correlations again\n",
    "print(correlations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Aggregated Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode hour and day of week as cyclical features\n",
    "final_reduced['hour_sin'] = np.sin(2 * np.pi * final_reduced['start_hour'] / 24)\n",
    "final_reduced['hour_cos'] = np.cos(2 * np.pi * final_reduced['start_hour'] / 24)\n",
    "final_reduced['weekday_sin'] = np.sin(2 * np.pi * final_reduced['start_weekday'] / 7)\n",
    "final_reduced['weekday_cos'] = np.cos(2 * np.pi * final_reduced['start_weekday'] / 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "print(final_reduced.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert boolean to integer\n",
    "final_reduced['is_weekend'] = final_reduced['is_weekend'].astype(int)\n",
    "final_reduced['transaction_date'] = pd.to_datetime(final_reduced['transaction_date'], format='%Y/%m/%d')\n",
    "final_reduced['transaction_time'] = pd.to_datetime(final_reduced['transaction_time'], format='%Y/%m/%d %H:%M')\n",
    "# Extract useful features from 'transaction_date'\n",
    "final_reduced['transaction_year'] = final_reduced['transaction_date'].dt.year\n",
    "final_reduced['transaction_month'] = final_reduced['transaction_date'].dt.month\n",
    "final_reduced['transaction_day'] = final_reduced['transaction_date'].dt.day\n",
    "final_reduced['transaction_weekday'] = final_reduced['transaction_date'].dt.weekday  # Monday=0, Sunday=6\n",
    "# Extract useful features from 'transaction_time'\n",
    "final_reduced['transaction_hour'] = final_reduced['transaction_time'].dt.hour\n",
    "final_reduced['transaction_minute'] = final_reduced['transaction_time'].dt.minute\n",
    "final_reduced['transaction_second'] = final_reduced['transaction_time'].dt.second\n",
    "\n",
    "categorical_columns = ['start_season', 'end_season','day_part']\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False, dtype=int, handle_unknown='ignore')\n",
    "\n",
    "# Fit and transform the selected categorical columns\n",
    "encoded_features = encoder.fit_transform(final_reduced[categorical_columns])\n",
    "\n",
    "encoded_features_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names_out(categorical_columns))\n",
    "\n",
    "\n",
    "# Concatenate the encoded columns with the original DataFrame (excluding the original categorical columns)\n",
    "final_reduced = pd.concat([final_reduced.drop(columns=categorical_columns), encoded_features_df], axis=1)\n",
    "\n",
    "final_reduced = final_reduced.drop(['time', 'start_date', 'start_time', 'end_date', 'end_time', 'transaction_date', 'transaction_time'], axis=1)\n",
    "\n",
    "# Check for missing values after encoding\n",
    "print(final_reduced.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (206597, 94)\n",
      "Shape after encoding: (206597, 12)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original shape: {final_reduced.shape}\")\n",
    "print(f\"Shape after encoding: {encoded_features_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_reduced.to_csv('/home/u894059/.local/dataset/final_reduced2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection\n",
    "# Prepare data (X and y)\n",
    "X = final_reduced.drop(columns=['Energy (kWh)'])  # Drop target and non-feature columns\n",
    "y = final_reduced['Energy (kWh)']\n",
    "\n",
    "# Train an XGBoost model\n",
    "model = xgb.XGBRegressor()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Use SHAP to interpret feature importance\n",
    "explainer = shap.Explainer(model)\n",
    "shap_values = explainer(X)\n",
    "\n",
    "# Plot feature importance\n",
    "shap.summary_plot(shap_values, X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature importance as a dataframe\n",
    "shap_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'SHAP Importance': np.abs(shap_values.values).mean(axis=0)  # Mean absolute SHAP value for each feature\n",
    "})\n",
    "\n",
    "# Sort features by SHAP importance (descending order)\n",
    "shap_importance = shap_importance.sort_values(by='SHAP Importance', ascending=False)\n",
    "\n",
    "# Display the ranked features\n",
    "print(shap_importance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the top 20 features based on SHAP ranking\n",
    "top_features = shap_importance['Feature'].head(20).values\n",
    "print(top_features)\n",
    "# Select the corresponding columns from your dataset\n",
    "X_selected = final_reduced[top_features]\n",
    "y = final_reduced['Energy (kWh)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data using the selected top features\n",
    "X_selected = final_reduced.drop('Energy (kWh)', axis=1)\n",
    "y = final_reduced['Energy (kWh)']\n",
    "\n",
    "# Create sequences for the LSTM model (window size = 30)\n",
    "def create_sequences(X, y, window_size=30):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X) - window_size):\n",
    "        X_seq.append(X.iloc[i:i+window_size].values)\n",
    "        y_seq.append(y.iloc[i+window_size])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "# Create the sequences\n",
    "X_seq, y_seq = create_sequences(X_selected, y, window_size=30)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Target Variable Aggregation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1.1 Short-term Forecasting (Daily Horizon) - with Additional Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total daily energy consumption\n",
    "\n",
    "# Load your dataset\n",
    "# Assuming 'timestamp' is the column with session timestamps and 'Energy (kWh)' is the energy consumption per session\n",
    "stf = pd.read_csv('/home/u894059/.local/dataset/final_reduced2.csv')\n",
    "\n",
    "# Group by the 'date' column and sum up the 'Energy (kWh)' for each day\n",
    "daily_energy = stf.groupby(['start_year', 'start_month', 'start_day'])['Energy (kWh)'].sum().reset_index()\n",
    "\n",
    "# Rename the aggregated column to make it clear\n",
    "daily_energy = daily_energy.rename(columns={'Energy (kWh)': 'total_daily_energy_consumption'})\n",
    "\n",
    "# daily_energy now contains the total energy consumption per day\n",
    "print(daily_energy.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "stf = pd.merge(stf, daily_energy, on=['start_year', 'start_month', 'start_day'], how='left')\n",
    "stf = stf.drop(columns=['Energy (kWh)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "stf.to_csv('/home/u894059/.local/dataset/short_term.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1.2 Short-term Forecasting (Daily Horizon) - with Baseline Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total daily energy consumption\n",
    "\n",
    "# Load your dataset\n",
    "# Assuming 'timestamp' is the column with session timestamps and 'Energy (kWh)' is the energy consumption per session\n",
    "stf_baseline = pd.read_csv('/home/u894059/.local/dataset/baseline_df.csv')\n",
    "\n",
    "# Group by the 'date' column and sum up the 'Energy (kWh)' for each day\n",
    "daily_energy_baseline = stf_baseline.groupby(['start_year', 'start_month', 'start_day'])['Energy (kWh)'].sum().reset_index()\n",
    "\n",
    "# Rename the aggregated column to make it clear\n",
    "daily_energy_baseline = daily_energy_baseline.rename(columns={'Energy (kWh)': 'total_daily_energy_baseline_consumption'})\n",
    "\n",
    "# daily_energy_baseline now contains the total energy consumption per day\n",
    "print(daily_energy_baseline.head())\n",
    "\n",
    "stf_baseline = pd.merge(stf_baseline, daily_energy_baseline, on=['start_year', 'start_month', 'start_day'], how='left')\n",
    "stf_baseline = stf_baseline.drop(columns=['Energy (kWh)'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "stf_baseline.to_csv('/home/u894059/.local/dataset/short_term_baseline.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2.1 Mid-term Forecasting (Weekly Horizon) - with Additional Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total weekly energy consumption\n",
    "\n",
    "# Load your dataset\n",
    "# Assuming 'timestamp' is the column with session timestamps and 'Energy (kWh)' is the energy consumption per session\n",
    "mtf = pd.read_csv('/home/u894059/.local/dataset/final_reduced2.csv')\n",
    "\n",
    "# Create a date column from start_year, start_month, and start_day\n",
    "mtf['date'] = pd.to_datetime(mtf['start_year'].astype(str) + '-' + \n",
    "                             mtf['start_month'].astype(str) + '-' + \n",
    "                             mtf['start_day'].astype(str), errors='coerce')\n",
    "\n",
    "\n",
    "# Extract the ISO week number from the new date column\n",
    "mtf['week'] = mtf['date'].dt.week  # Standard week numbering without ISO adjustments\n",
    "\n",
    "mtf.loc[(mtf['week'] == 53) & (mtf['date'].dt.month == 1), 'week'] = 1\n",
    "\n",
    "# Aggregate energy by year, month, and week\n",
    "weekly_energy = mtf.groupby(['start_year', 'start_month', 'week'])['Energy (kWh)'].sum().reset_index()\n",
    "weekly_energy.rename(columns={'Energy (kWh)': 'total_weekly_energy_consumption'}, inplace=True)\n",
    "\n",
    "# Merge weekly totals back to your main DataFrame\n",
    "mtf = pd.merge(mtf, weekly_energy, on=['start_year', 'start_month', 'week'], how='left').drop(columns=['Energy (kWh)', 'date'])\n",
    "mtf = mtf.drop(columns=['week'])\n",
    "print(mtf.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtf.to_csv('/home/u894059/.local/dataset/mid_term.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2.2 Mid-term Forecasting (Weekly Horizon) - with Baseline Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total weekly energy consumption\n",
    "\n",
    "# Load your dataset\n",
    "# Assuming 'timestamp' is the column with session timestamps and 'Energy (kWh)' is the energy consumption per session\n",
    "mtf_baseline = pd.read_csv('/home/u894059/.local/dataset/baseline_df.csv')\n",
    "\n",
    "# Create a date column from start_year, start_month, and start_day\n",
    "mtf_baseline['date'] = pd.to_datetime(mtf_baseline['start_year'].astype(str) + '-' + \n",
    "                             mtf_baseline['start_month'].astype(str) + '-' + \n",
    "                             mtf_baseline['start_day'].astype(str), errors='coerce')\n",
    "\n",
    "\n",
    "# Extract the ISO week number from the new date column\n",
    "mtf_baseline['week'] = mtf_baseline['date'].dt.week  # Standard week numbering without ISO adjustments\n",
    "\n",
    "mtf_baseline.loc[(mtf_baseline['week'] == 53) & (mtf_baseline['date'].dt.month == 1), 'week'] = 1\n",
    "\n",
    "# Aggregate energy by year, month, and week\n",
    "weekly_energy = mtf_baseline.groupby(['start_year', 'start_month', 'week'])['Energy (kWh)'].sum().reset_index()\n",
    "weekly_energy.rename(columns={'Energy (kWh)': 'total_weekly_energy_consumption'}, inplace=True)\n",
    "\n",
    "# Merge weekly totals back to your main DataFrame\n",
    "mtf_baseline = pd.merge(mtf_baseline, weekly_energy, on=['start_year', 'start_month', 'week'], how='left').drop(columns=['Energy (kWh)', 'date'])\n",
    "\n",
    "print(mtf_baseline.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtf_baseline.to_csv('/home/u894059/.local/dataset/mid_term_baseline.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Input Features Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Hourly Aggregated Features - Short Term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "stf_haf = pd.read_csv('/home/u894059/.local/dataset/short_term.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "print(stf_haf.head())  # Display the first few rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns for which we want sum and mean aggregation\n",
    "columns_to_sum = ['GHG Savings (kg)', 'Gasoline Savings (gallons)', 'transaction_duration']\n",
    "columns_to_mean = ['temperature_2m (°C)', 'relative_humidity_2m (%)', 'rain (mm)', \n",
    "                   'snowfall (cm)', 'cloud_cover (%)', 'wind_speed_100m (km/h)']\n",
    "\n",
    "input_features = stf_haf[['start_year', 'start_month', 'start_day', 'start_hour'] + columns_to_sum + columns_to_mean]\n",
    "\n",
    "# Aggregate the columns separately for sum and mean\n",
    "hourly_features_sum = input_features.groupby(\n",
    "    ['start_year', 'start_month', 'start_day', 'start_hour']\n",
    ")[columns_to_sum].sum().reset_index()\n",
    "\n",
    "hourly_features_mean = input_features.groupby(\n",
    "    ['start_year', 'start_month', 'start_day', 'start_hour']\n",
    ")[columns_to_mean].mean().reset_index()\n",
    "\n",
    "# Merge the sum and mean dataframes\n",
    "hourly_features = pd.merge(hourly_features_sum, hourly_features_mean, \n",
    "                            on=['start_year', 'start_month', 'start_day', 'start_hour'])\n",
    "\n",
    "print(hourly_features.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the common columns from stf_haf before merging (to avoid duplication)\n",
    "stf_haf_without_common_columns = stf_haf.drop(columns=columns_to_sum + columns_to_mean)\n",
    "\n",
    "# Merge the hourly_features with stf_haf (without the common columns)\n",
    "stf_haf = pd.merge(stf_haf_without_common_columns, hourly_features, \n",
    "                   on=['start_year', 'start_month', 'start_day', 'start_hour'], \n",
    "                   how='left', \n",
    "                   suffixes=('_original', '_aggregated'))\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(stf_haf.head())\n",
    "\n",
    "# Optionally, save the result to a new CSV file\n",
    "stf_haf.to_csv('/home/u894059/.local/dataset/short_term_haf.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2.1 Daily Aggregated Features - Short Term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "stf_daf = pd.read_csv('/home/u894059/.local/dataset/short_term.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns for which we want sum and mean aggregation\n",
    "columns_to_sum = ['GHG Savings (kg)', 'Gasoline Savings (gallons)', 'transaction_duration']\n",
    "columns_to_mean = ['temperature_2m (°C)', 'relative_humidity_2m (%)', 'rain (mm)', \n",
    "                   'snowfall (cm)', 'cloud_cover (%)', 'wind_speed_100m (km/h)']\n",
    "\n",
    "input_features = stf_daf[['start_year', 'start_month', 'start_day'] + columns_to_sum + columns_to_mean]\n",
    "\n",
    "# Aggregate the columns separately for sum and mean\n",
    "daily_features_sum = input_features.groupby(\n",
    "    ['start_year', 'start_month', 'start_day']\n",
    ")[columns_to_sum].sum().reset_index()\n",
    "\n",
    "daily_features_mean = input_features.groupby(\n",
    "    ['start_year', 'start_month', 'start_day']\n",
    ")[columns_to_mean].mean().reset_index()\n",
    "\n",
    "# Merge the sum and mean dataframes\n",
    "daily_features = pd.merge(daily_features_sum, daily_features_mean, \n",
    "                            on=['start_year', 'start_month', 'start_day'])\n",
    "\n",
    "print(daily_features.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the common columns from stf_daf before merging (to avoid duplication)\n",
    "stf_daf_without_common_columns = stf_daf.drop(columns=columns_to_sum + columns_to_mean)\n",
    "\n",
    "# Merge the daily_features with stf_daf (without the common columns)\n",
    "stf_daf = pd.merge(stf_daf_without_common_columns, daily_features, \n",
    "                   on=['start_year', 'start_month', 'start_day'], \n",
    "                   how='left', \n",
    "                   suffixes=('_original', '_aggregated'))\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(stf_daf.head())\n",
    "\n",
    "# Optionally, save the result to a new CSV file\n",
    "stf_daf.to_csv('/home/u894059/.local/dataset/short_term_daf.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2.2 Daily Aggregated Features - Mid Term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtf_daf = pd.read_csv('/home/u894059/.local/dataset/mid_term.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns for which we want sum and mean aggregation\n",
    "columns_to_sum = ['GHG Savings (kg)', 'Gasoline Savings (gallons)', 'transaction_duration']\n",
    "columns_to_mean = ['temperature_2m (°C)', 'relative_humidity_2m (%)', 'rain (mm)', \n",
    "                   'snowfall (cm)', 'cloud_cover (%)', 'wind_speed_100m (km/h)']\n",
    "\n",
    "input_features = mtf_daf[['start_year', 'start_month', 'start_day'] + columns_to_sum + columns_to_mean]\n",
    "\n",
    "# Aggregate the columns separately for sum and mean\n",
    "daily_features_sum = input_features.groupby(\n",
    "    ['start_year', 'start_month', 'start_day']\n",
    ")[columns_to_sum].sum().reset_index()\n",
    "\n",
    "daily_features_mean = input_features.groupby(\n",
    "    ['start_year', 'start_month', 'start_day']\n",
    ")[columns_to_mean].mean().reset_index()\n",
    "\n",
    "# Merge the sum and mean dataframes\n",
    "daily_features = pd.merge(daily_features_sum, daily_features_mean, \n",
    "                            on=['start_year', 'start_month', 'start_day'])\n",
    "\n",
    "print(daily_features.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the common columns from mtf_daf before merging (to avoid duplication)\n",
    "mtf_daf_without_common_columns = mtf_daf.drop(columns=columns_to_sum + columns_to_mean)\n",
    "\n",
    "# Merge the daily_features with mtf_daf (without the common columns)\n",
    "mtf_daf = pd.merge(mtf_daf_without_common_columns, daily_features, \n",
    "                   on=['start_year', 'start_month', 'start_day'], \n",
    "                   how='left', \n",
    "                   suffixes=('_original', '_aggregated'))\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(mtf_daf.head())\n",
    "\n",
    "# Optionally, save the result to a new CSV file\n",
    "mtf_daf.to_csv('/home/u894059/.local/dataset/mid_term_daf.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Weekly Aggregated Features - Mid Term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtf_waf = pd.read_csv('/home/u894059/.local/dataset/mid_term.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns for which we want sum and mean aggregation\n",
    "columns_to_sum = ['GHG Savings (kg)', 'Gasoline Savings (gallons)', 'transaction_duration']\n",
    "columns_to_mean = ['temperature_2m (°C)', 'relative_humidity_2m (%)', 'rain (mm)', \n",
    "                   'snowfall (cm)', 'cloud_cover (%)', 'wind_speed_100m (km/h)']\n",
    "\n",
    "# Create a date column from start_year, start_month, and start_day\n",
    "mtf_waf['date'] = pd.to_datetime(mtf_waf['start_year'].astype(str) + '-' + \n",
    "                             mtf_waf['start_month'].astype(str) + '-' + \n",
    "                             mtf_waf['start_day'].astype(str), errors='coerce')\n",
    "\n",
    "\n",
    "# Extract the ISO week number from the new date column\n",
    "mtf_waf['week'] = mtf_waf['date'].dt.week  # Standard week numbering without ISO adjustments\n",
    "\n",
    "mtf_waf.loc[(mtf_waf['week'] == 53) & (mtf_waf['date'].dt.month == 1), 'week'] = 1\n",
    "\n",
    "# Define the columns you want to aggregate\n",
    "input_features = mtf_waf[['start_year', 'start_month', 'start_day', 'week'] + columns_to_sum + columns_to_mean]\n",
    "\n",
    "# Aggregate the columns separately for sum and mean\n",
    "weekly_features_sum = input_features.groupby(['start_year', 'start_month', 'week'])[columns_to_sum].sum().reset_index()\n",
    "weekly_features_mean = input_features.groupby(['start_year', 'start_month', 'week'])[columns_to_mean].mean().reset_index()\n",
    "\n",
    "# Merge the sum and mean dataframes\n",
    "weekly_features = pd.merge(weekly_features_sum, weekly_features_mean, on=['start_year', 'start_month', 'week'])\n",
    "\n",
    "# # Drop the week column if not needed in the final output\n",
    "# weekly_features = weekly_features.drop(columns='week')\n",
    "\n",
    "# Display the first few rows of the resulting DataFrame\n",
    "print(weekly_features.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the common columns from mtf_waf before merging (to avoid duplication)\n",
    "mtf_waf_without_common_columns = mtf_waf.drop(columns=columns_to_sum + columns_to_mean)\n",
    "\n",
    "# Merge the daily_features with mtf_waf (without the common columns)\n",
    "mtf_waf = pd.merge(mtf_waf_without_common_columns, weekly_features, \n",
    "                   on=['start_year', 'start_month', 'week'], \n",
    "                   how='left', \n",
    "                   suffixes=('_original', '_aggregated'))\n",
    "                   \n",
    "mtf_waf = mtf_waf.drop(columns=['week', 'date'])\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(mtf_waf.head())\n",
    "\n",
    "# Optionally, save the result to a new CSV file\n",
    "mtf_waf.to_csv('/home/u894059/.local/dataset/mid_term_waf.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Models - Additional Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1.1 LSTM Model - Short Term "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "stf = pd.read_csv('/home/u894059/.local/dataset/short_term.csv')\n",
    "\n",
    "# Select the features and target variable\n",
    "features = stf.drop(columns=['total_daily_energy_consumption'])\n",
    "target = stf['total_daily_energy_consumption'].values.reshape(-1, 1)  # reshape for scaling\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(features, target, test_size=0.2, shuffle=False)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.125, shuffle=False)\n",
    "\n",
    "# Initialize scalers for features and target\n",
    "feature_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "target_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Fit the scaler on the training data and transform\n",
    "X_train_scaled = feature_scaler.fit_transform(X_train)\n",
    "X_val_scaled = feature_scaler.transform(X_val)  # Use transform, not fit_transform\n",
    "X_test_scaled = feature_scaler.transform(X_test)  # Use transform, not fit_transform\n",
    "\n",
    "# Scale the target variable using the target scaler\n",
    "y_train_scaled = target_scaler.fit_transform(y_train).flatten()\n",
    "y_val_scaled = target_scaler.transform(y_val).flatten()  # Use transform, not fit_transform\n",
    "y_test_scaled = target_scaler.transform(y_test).flatten()  # Use transform, not fit_transform\n",
    "\n",
    "# Convert the scaled features to sequences (after scaling)\n",
    "def create_sequences(data, target, sequence_length=5):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        sequences.append(data[i:i+sequence_length])\n",
    "        targets.append(target[i+sequence_length])\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "# Create sequences\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled, sequence_length=5)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val_scaled, y_val_scaled, sequence_length=5)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_scaled, sequence_length=5)\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(64, return_sequences=True, input_shape=(X_train_seq.shape[1], X_train_seq.shape[2])),  # LSTM layer with 64 units\n",
    "    Dropout(0.3),  # Dropout rate set to 0.3\n",
    "    LSTM(64, return_sequences=False),  # Second LSTM layer with 64 units\n",
    "    Dropout(0.3),  # Dropout rate set to 0.3\n",
    "    Dense(1)  # Final output layer for regression (predicting continuous values)\n",
    "])\n",
    "\n",
    "# Compile the model with a low learning rate for stable training\n",
    "model.compile(optimizer=Nadam(learning_rate=0.001), loss='mse')\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n",
    "\n",
    "# Train the model using the sequences\n",
    "history = model.fit(\n",
    "    X_train_seq, y_train_seq,  # Use sequences for training\n",
    "    epochs=50, \n",
    "    batch_size=128, \n",
    "    validation_data=(X_val_seq, y_val_seq),  # Use sequences for validation\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n",
    "\n",
    "# Get predictions on the test set (scaled values)\n",
    "y_pred_scaled = model.predict(X_test_seq)\n",
    "\n",
    "# Calculate metrics on the scaled values\n",
    "mse_scaled = mean_squared_error(y_test_seq, y_pred_scaled)\n",
    "rmse_scaled = np.sqrt(mse_scaled)\n",
    "mae_scaled = mean_absolute_error(y_test_seq, y_pred_scaled)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE) on Scaled Data: {mse_scaled}\")\n",
    "print(f\"Root Mean Squared Error (RMSE) on Scaled Data: {rmse_scaled}\")\n",
    "print(f\"Mean Absolute Error (MAE) on Scaled Data: {mae_scaled}\")\n",
    "\n",
    "# Define the path where you want to save the plot\n",
    "save_path = '/home/u894059/.local/plot1.png'\n",
    "\n",
    "# Evaluate the model on the scaled test set\n",
    "test_loss = model.evaluate(X_test_seq, y_test_seq)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "\n",
    "# Make predictions on the scaled test set\n",
    "predictions_scaled = model.predict(X_test_seq)\n",
    "\n",
    "# Optionally, inverse transform predictions and actual values to the original scale\n",
    "predictions = target_scaler.inverse_transform(predictions_scaled.reshape(-1, 1)).flatten()\n",
    "y_test_original = target_scaler.inverse_transform(y_test_seq.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Plot the scaled values (0-1 range) or original values\n",
    "plt.plot(y_test_original, label='Actual')\n",
    "plt.plot(predictions, label='Predicted')\n",
    "plt.legend()\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Energy Consumption')\n",
    "plt.title('Actual vs Predicted Energy Consumption')\n",
    "\n",
    "# Save the plot to the specified path\n",
    "plt.savefig(save_path)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1.2 LSTM Model - Mid Term "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "mtf = pd.read_csv('/home/u894059/.local/dataset/mid_term.csv')\n",
    "\n",
    "# Select the features and target variable\n",
    "features = mtf.drop(columns=['total_weekly_energy_consumption'])\n",
    "target = mtf['total_weekly_energy_consumption'].values.reshape(-1, 1)  # reshape for scaling\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(features, target, test_size=0.2, shuffle=False)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.125, shuffle=False)\n",
    "\n",
    "# Initialize scalers for features and target\n",
    "feature_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "target_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Fit the scaler on the training data and transform\n",
    "X_train_scaled = feature_scaler.fit_transform(X_train)\n",
    "X_val_scaled = feature_scaler.transform(X_val)  # Use transform, not fit_transform\n",
    "X_test_scaled = feature_scaler.transform(X_test)  # Use transform, not fit_transform\n",
    "\n",
    "# Scale the target variable using the target scaler\n",
    "y_train_scaled = target_scaler.fit_transform(y_train).flatten()\n",
    "y_val_scaled = target_scaler.transform(y_val).flatten()  # Use transform, not fit_transform\n",
    "y_test_scaled = target_scaler.transform(y_test).flatten()  # Use transform, not fit_transform\n",
    "\n",
    "# Convert the scaled features to sequences (after scaling)\n",
    "def create_sequences(data, target, sequence_length=10):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        sequences.append(data[i:i+sequence_length])\n",
    "        targets.append(target[i+sequence_length])\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "# Create sequences\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled, sequence_length=10)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val_scaled, y_val_scaled, sequence_length=10)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_scaled, sequence_length=10)\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(128, return_sequences=True, input_shape=(X_train_seq.shape[1], X_train_seq.shape[2])),  # LSTM layer with 128 units\n",
    "    Dropout(0.2),  # Dropout rate set to 0.2\n",
    "    LSTM(16, return_sequences=False),  # Second LSTM layer with 16 units\n",
    "    Dropout(0.2),  # Dropout rate set to 0.2\n",
    "    Dense(1)  # Final output layer for regression (predicting continuous values)\n",
    "])\n",
    "\n",
    "# Compile the model with a low learning rate for stable training\n",
    "model.compile(optimizer=Nadam(learning_rate=0.001), loss='mse')\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n",
    "\n",
    "# Train the model using the sequences\n",
    "history = model.fit(\n",
    "    X_train_seq, y_train_seq,  # Use sequences for training\n",
    "    epochs=50, \n",
    "    batch_size=128, \n",
    "    validation_data=(X_val_seq, y_val_seq),  # Use sequences for validation\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n",
    "\n",
    "# Get predictions on the test set (scaled values)\n",
    "y_pred_scaled = model.predict(X_test_seq)\n",
    "\n",
    "# Calculate metrics on the scaled values\n",
    "mse_scaled = mean_squared_error(y_test_seq, y_pred_scaled)\n",
    "rmse_scaled = np.sqrt(mse_scaled)\n",
    "mae_scaled = mean_absolute_error(y_test_seq, y_pred_scaled)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE) on Scaled Data: {mse_scaled}\")\n",
    "print(f\"Root Mean Squared Error (RMSE) on Scaled Data: {rmse_scaled}\")\n",
    "print(f\"Mean Absolute Error (MAE) on Scaled Data: {mae_scaled}\")\n",
    "\n",
    "# Define the path where you want to save the plot\n",
    "save_path = '/home/u894059/.local/plot2.png'\n",
    "\n",
    "# Evaluate the model on the scaled test set\n",
    "test_loss = model.evaluate(X_test_seq, y_test_seq)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "\n",
    "# Make predictions on the scaled test set\n",
    "predictions_scaled = model.predict(X_test_seq)\n",
    "\n",
    "# Optionally, inverse transform predictions and actual values to the original scale\n",
    "predictions = target_scaler.inverse_transform(predictions_scaled.reshape(-1, 1)).flatten()\n",
    "y_test_original = target_scaler.inverse_transform(y_test_seq.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Plot the scaled values (0-1 range) or original values\n",
    "plt.plot(y_test_original, label='Actual')\n",
    "plt.plot(predictions, label='Predicted')\n",
    "plt.legend()\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Energy Consumption')\n",
    "plt.title('Actual vs Predicted Energy Consumption')\n",
    "\n",
    "# Save the plot to the specified path\n",
    "plt.savefig(save_path)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2.1 Transformer - Short Term "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "stf = pd.read_csv('/home/u894059/.local/dataset/short_term.csv')\n",
    "\n",
    "# Select the features and target variable\n",
    "features = stf.drop(columns=['total_daily_energy_consumption'])\n",
    "target = stf['total_daily_energy_consumption'].values.reshape(-1, 1)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(features, target, test_size=0.2, shuffle=False)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.125, shuffle=False)\n",
    "\n",
    "# Initialize scalers for features and target\n",
    "feature_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "target_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Fit the scaler on the training data and transform\n",
    "X_train_scaled = feature_scaler.fit_transform(X_train)\n",
    "X_val_scaled = feature_scaler.transform(X_val)\n",
    "X_test_scaled = feature_scaler.transform(X_test)\n",
    "\n",
    "# Scale the target variable using the target scaler\n",
    "y_train_scaled = target_scaler.fit_transform(y_train).flatten()\n",
    "y_val_scaled = target_scaler.transform(y_val).flatten()\n",
    "y_test_scaled = target_scaler.transform(y_test).flatten()\n",
    "\n",
    "# Create sequences\n",
    "def create_sequences(data, target, sequence_length=5):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        sequences.append(data[i:i+sequence_length])\n",
    "        targets.append(target[i+sequence_length])\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "# Create sequences\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled, sequence_length=5)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val_scaled, y_val_scaled, sequence_length=5)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_scaled, sequence_length=5)\n",
    "\n",
    "# Positional Encoding Function\n",
    "def get_positional_encoding(seq_len, d_model):\n",
    "    pos = np.arange(seq_len)[:, np.newaxis]\n",
    "    i = np.arange(d_model)[np.newaxis, :]\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    angle_rads = pos * angle_rates\n",
    "    pos_encoding = np.concatenate([np.sin(angle_rads[:, 0::2]), np.cos(angle_rads[:, 1::2])], axis=-1)\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "# Define Transformer Encoder block with Positional Encoding\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout_rate, seq_len, d_model):\n",
    "    pos_encoding = get_positional_encoding(seq_len, d_model)\n",
    "    inputs += pos_encoding\n",
    "    \n",
    "    attention = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout_rate\n",
    "    )(inputs, inputs)\n",
    "    \n",
    "    attention = layers.LayerNormalization(epsilon=1e-6)(attention + inputs)\n",
    "    \n",
    "    ff = layers.Dense(ff_dim, activation='relu')(attention)\n",
    "    ff = layers.Dense(inputs.shape[-1])(ff)\n",
    "    \n",
    "    return layers.LayerNormalization(epsilon=1e-6)(ff + attention)\n",
    "\n",
    "# Define Transformer Decoder block\n",
    "def transformer_decoder(inputs, encoder_outputs, head_size, num_heads, ff_dim, dropout_rate):\n",
    "    attention_1 = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout_rate\n",
    "    )(inputs, inputs)\n",
    "    \n",
    "    attention_1 = layers.LayerNormalization(epsilon=1e-6)(attention_1 + inputs)\n",
    "    \n",
    "    attention_2 = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout_rate\n",
    "    )(attention_1, encoder_outputs)\n",
    "    \n",
    "    attention_2 = layers.LayerNormalization(epsilon=1e-6)(attention_2 + attention_1)\n",
    "    \n",
    "    ff = layers.Dense(ff_dim, activation='relu')(attention_2)\n",
    "    ff = layers.Dense(inputs.shape[-1])(ff)\n",
    "    \n",
    "    return layers.LayerNormalization(epsilon=1e-6)(ff + attention_2)\n",
    "\n",
    "# Define the Input Layer for Encoder (3D input: samples, time_steps, features)\n",
    "encoder_inputs = layers.Input(shape=(X_train_seq.shape[1], X_train_seq.shape[2]))  # (time_steps, features)\n",
    "encoder_outputs = transformer_encoder(encoder_inputs, head_size=32, num_heads=4, ff_dim=64, dropout_rate=0.1,\n",
    "                                      seq_len=X_train_seq.shape[1], d_model=X_train_seq.shape[2])\n",
    "\n",
    "# Define the Input Layer for Decoder (3D input: samples, time_steps, features)\n",
    "decoder_inputs = layers.Input(shape=(X_train_seq.shape[1], X_train_seq.shape[2]))  # (time_steps, features)\n",
    "decoder_outputs = transformer_decoder(decoder_inputs, encoder_outputs, head_size=32, num_heads=4, ff_dim=64, dropout_rate=0.1)\n",
    "\n",
    "# Global Average Pooling to reduce the time dimension\n",
    "x = layers.GlobalAveragePooling1D()(decoder_outputs)\n",
    "\n",
    "# Output layer (for regression, a single neuron)\n",
    "outputs = layers.Dense(1)(x)\n",
    "\n",
    "# Create the model\n",
    "model = keras.Model(inputs=[encoder_inputs, decoder_inputs], outputs=outputs)\n",
    "\n",
    "# Compile the model\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', patience=5, restore_best_weights=True\n",
    ")\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    [X_train_seq, X_train_seq], y_train_seq,\n",
    "    epochs=50,\n",
    "    batch_size=128,\n",
    "    validation_data=([X_val_seq, X_val_seq], y_val_seq),\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n",
    "\n",
    "# Get predictions on the test set\n",
    "y_pred_scaled = model.predict([X_test_seq, X_test_seq])\n",
    "\n",
    "# Calculate metrics on the scaled values\n",
    "mse_scaled = mean_squared_error(y_test_seq, y_pred_scaled)\n",
    "rmse_scaled = np.sqrt(mse_scaled)\n",
    "mae_scaled = mean_absolute_error(y_test_seq, y_pred_scaled)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE) on Scaled Data: {mse_scaled}\")\n",
    "print(f\"Root Mean Squared Error (RMSE) on Scaled Data: {rmse_scaled}\")\n",
    "print(f\"Mean Absolute Error (MAE) on Scaled Data: {mae_scaled}\")\n",
    "\n",
    "# Reverse the scaling for predictions and actual values\n",
    "y_pred = target_scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_original = target_scaler.inverse_transform(y_test_seq.reshape(-1, 1))\n",
    "\n",
    "# Plot the actual vs predicted values\n",
    "plt.plot(y_test_original, label='Actual')\n",
    "plt.plot(y_pred, label='Predicted')\n",
    "plt.legend()\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Energy Consumption')\n",
    "plt.title('Actual vs Predicted Energy Consumption')\n",
    "plt.savefig('/home/u894059/.local/plot1.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2.2 Transformer - Mid Term "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "stf = pd.read_csv('/home/u894059/.local/dataset/mid_term.csv')\n",
    "\n",
    "# Select the features and target variable\n",
    "features = stf.drop(columns=['total_weekly_energy_consumption'])\n",
    "target = stf['total_weekly_energy_consumption'].values.reshape(-1, 1)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(features, target, test_size=0.2, shuffle=False)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.125, shuffle=False)\n",
    "\n",
    "# Initialize scalers for features and target\n",
    "feature_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "target_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Fit the scaler on the training data and transform\n",
    "X_train_scaled = feature_scaler.fit_transform(X_train)\n",
    "X_val_scaled = feature_scaler.transform(X_val)\n",
    "X_test_scaled = feature_scaler.transform(X_test)\n",
    "\n",
    "# Scale the target variable using the target scaler\n",
    "y_train_scaled = target_scaler.fit_transform(y_train).flatten()\n",
    "y_val_scaled = target_scaler.transform(y_val).flatten()\n",
    "y_test_scaled = target_scaler.transform(y_test).flatten()\n",
    "\n",
    "# Create sequences\n",
    "def create_sequences(data, target, sequence_length=10):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        sequences.append(data[i:i+sequence_length])\n",
    "        targets.append(target[i+sequence_length])\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "# Create sequences\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled, sequence_length=10)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val_scaled, y_val_scaled, sequence_length=10)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_scaled, sequence_length=10)\n",
    "\n",
    "# Positional Encoding Function\n",
    "def get_positional_encoding(seq_len, d_model):\n",
    "    pos = np.arange(seq_len)[:, np.newaxis]\n",
    "    i = np.arange(d_model)[np.newaxis, :]\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    angle_rads = pos * angle_rates\n",
    "    pos_encoding = np.concatenate([np.sin(angle_rads[:, 0::2]), np.cos(angle_rads[:, 1::2])], axis=-1)\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "# Define Transformer Encoder block with Positional Encoding\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout_rate, seq_len, d_model):\n",
    "    pos_encoding = get_positional_encoding(seq_len, d_model)\n",
    "    inputs += pos_encoding\n",
    "    \n",
    "    attention = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout_rate\n",
    "    )(inputs, inputs)\n",
    "    \n",
    "    attention = layers.LayerNormalization(epsilon=1e-6)(attention + inputs)\n",
    "    \n",
    "    ff = layers.Dense(ff_dim, activation='relu')(attention)\n",
    "    ff = layers.Dense(inputs.shape[-1])(ff)\n",
    "    \n",
    "    return layers.LayerNormalization(epsilon=1e-6)(ff + attention)\n",
    "\n",
    "# Define Transformer Decoder block\n",
    "def transformer_decoder(inputs, encoder_outputs, head_size, num_heads, ff_dim, dropout_rate):\n",
    "    attention_1 = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout_rate\n",
    "    )(inputs, inputs)\n",
    "    \n",
    "    attention_1 = layers.LayerNormalization(epsilon=1e-6)(attention_1 + inputs)\n",
    "    \n",
    "    attention_2 = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout_rate\n",
    "    )(attention_1, encoder_outputs)\n",
    "    \n",
    "    attention_2 = layers.LayerNormalization(epsilon=1e-6)(attention_2 + attention_1)\n",
    "    \n",
    "    ff = layers.Dense(ff_dim, activation='relu')(attention_2)\n",
    "    ff = layers.Dense(inputs.shape[-1])(ff)\n",
    "    \n",
    "    return layers.LayerNormalization(epsilon=1e-6)(ff + attention_2)\n",
    "\n",
    "# Define the Input Layer for Encoder (3D input: samples, time_steps, features)\n",
    "encoder_inputs = layers.Input(shape=(X_train_seq.shape[1], X_train_seq.shape[2]))  # (time_steps, features)\n",
    "encoder_outputs = transformer_encoder(encoder_inputs, head_size=32, num_heads=4, ff_dim=64, dropout_rate=0.1,\n",
    "                                      seq_len=X_train_seq.shape[1], d_model=X_train_seq.shape[2])\n",
    "\n",
    "# Define the Input Layer for Decoder (3D input: samples, time_steps, features)\n",
    "decoder_inputs = layers.Input(shape=(X_train_seq.shape[1], X_train_seq.shape[2]))  # (time_steps, features)\n",
    "decoder_outputs = transformer_decoder(decoder_inputs, encoder_outputs, head_size=32, num_heads=4, ff_dim=64, dropout_rate=0.1)\n",
    "\n",
    "# Global Average Pooling to reduce the time dimension\n",
    "x = layers.GlobalAveragePooling1D()(decoder_outputs)\n",
    "\n",
    "# Output layer (for regression, a single neuron)\n",
    "outputs = layers.Dense(1)(x)\n",
    "\n",
    "# Create the model\n",
    "model = keras.Model(inputs=[encoder_inputs, decoder_inputs], outputs=outputs)\n",
    "\n",
    "# Compile the model\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', patience=5, restore_best_weights=True\n",
    ")\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    [X_train_seq, X_train_seq], y_train_seq,\n",
    "    epochs=50,\n",
    "    batch_size=128,\n",
    "    validation_data=([X_val_seq, X_val_seq], y_val_seq),\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n",
    "\n",
    "# Get predictions on the test set\n",
    "y_pred_scaled = model.predict([X_test_seq, X_test_seq])\n",
    "\n",
    "# Calculate metrics on the scaled values\n",
    "mse_scaled = mean_squared_error(y_test_seq, y_pred_scaled)\n",
    "rmse_scaled = np.sqrt(mse_scaled)\n",
    "mae_scaled = mean_absolute_error(y_test_seq, y_pred_scaled)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE) on Scaled Data: {mse_scaled}\")\n",
    "print(f\"Root Mean Squared Error (RMSE) on Scaled Data: {rmse_scaled}\")\n",
    "print(f\"Mean Absolute Error (MAE) on Scaled Data: {mae_scaled}\")\n",
    "\n",
    "# Reverse the scaling for predictions and actual values\n",
    "y_pred = target_scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_original = target_scaler.inverse_transform(y_test_seq.reshape(-1, 1))\n",
    "\n",
    "# Plot the actual vs predicted values\n",
    "plt.plot(y_test_original, label='Actual')\n",
    "plt.plot(y_pred, label='Predicted')\n",
    "plt.legend()\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Energy Consumption')\n",
    "plt.title('Actual vs Predicted Energy Consumption')\n",
    "plt.savefig('/home/u894059/.local/plot1.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3.1 Hybrid Model - Short Term "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "stf = pd.read_csv('/home/u894059/.local/dataset/short_term.csv')\n",
    "\n",
    "# Select the features and target variable\n",
    "features = stf.drop(columns=['total_daily_energy_consumption'])\n",
    "target = stf['total_daily_energy_consumption'].values.reshape(-1, 1)  # reshape for scaling\n",
    "\n",
    "# Preprocess the data\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(features, target, test_size=0.2, shuffle=False)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.125, shuffle=False)\n",
    "\n",
    "# Normalize the data\n",
    "feature_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "target_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "X_train_scaled = feature_scaler.fit_transform(X_train)\n",
    "X_val_scaled = feature_scaler.transform(X_val)\n",
    "X_test_scaled = feature_scaler.transform(X_test)\n",
    "\n",
    "y_train_scaled = target_scaler.fit_transform(y_train).flatten()\n",
    "y_val_scaled = target_scaler.transform(y_val).flatten()\n",
    "y_test_scaled = target_scaler.transform(y_test).flatten()\n",
    "\n",
    "# Create sliding window sequences\n",
    "def create_sequences(data, target, sequence_length):\n",
    "    sequences, targets = [], []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        sequences.append(data[i:i + sequence_length])\n",
    "        targets.append(target[i + sequence_length])\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "# Experiment with longer sequences\n",
    "sequence_length = 5\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled, sequence_length)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val_scaled, y_val_scaled, sequence_length)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_scaled, sequence_length)\n",
    "\n",
    "# LSTM Block\n",
    "def lstm_block(inputs, lstm_units):\n",
    "    x = layers.LSTM(lstm_units, return_sequences=True, kernel_regularizer='l2')(inputs)\n",
    "    x = layers.Dropout(0.3)(x)  # Increased dropout for regularization\n",
    "    return x\n",
    "\n",
    "# Transformer Encoder\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout_rate, num_blocks):\n",
    "    for _ in range(num_blocks):\n",
    "        # Multi-Head Attention\n",
    "        x = layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads)(inputs, inputs)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(x + inputs)  # Residual Connection\n",
    "        \n",
    "        # Feedforward Block\n",
    "        ff = layers.Conv1D(filters=ff_dim, kernel_size=1, activation='relu')(x)\n",
    "        ff = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(ff)\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(x + ff)  # Residual Connection\n",
    "    \n",
    "    return x\n",
    "\n",
    "# Build the RS-LSTM-Transformer Model\n",
    "def build_model(input_shape, lstm_units, head_size, num_heads, ff_dim, num_blocks, dropout_rate):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # LSTM Block\n",
    "    lstm_out = lstm_block(inputs, lstm_units)\n",
    "    \n",
    "    # Transformer Encoder\n",
    "    transformer_out = transformer_encoder(lstm_out, head_size, num_heads, ff_dim, dropout_rate, num_blocks)\n",
    "    \n",
    "    # Global Average Pooling + Dense Layers\n",
    "    gap = layers.GlobalAveragePooling1D()(transformer_out)\n",
    "    dense = layers.Dense(64, activation='relu')(gap)\n",
    "    outputs = layers.Dense(1)(dense)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# Example Hyperparameters\n",
    "lstm_units = 128\n",
    "head_size = 64\n",
    "num_heads = 8\n",
    "ff_dim = 256\n",
    "num_blocks = 3\n",
    "dropout_rate = 0.2\n",
    "input_shape = (sequence_length, features.shape[1])\n",
    "\n",
    "model = build_model(input_shape, lstm_units, head_size, num_heads, ff_dim, num_blocks, dropout_rate)\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(optimizer=optimizer, \n",
    "              loss='mse', \n",
    "              metrics=['mae'])\n",
    "\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "\n",
    "# Train the Model\n",
    "history = model.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    validation_data=(X_val_seq, y_val_seq),\n",
    "    epochs=50,\n",
    "    batch_size=32,  # Experiment with batch size if needed\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n",
    "\n",
    "# Get predictions on the test set\n",
    "y_pred_scaled = model.predict(X_test_seq)  # Fixing this line\n",
    "\n",
    "# Calculate metrics on the scaled values\n",
    "mse_scaled = mean_squared_error(y_test_seq, y_pred_scaled)\n",
    "rmse_scaled = np.sqrt(mse_scaled)\n",
    "mae_scaled = mean_absolute_error(y_test_seq, y_pred_scaled)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE) on Scaled Data: {mse_scaled}\")\n",
    "print(f\"Root Mean Squared Error (RMSE) on Scaled Data: {rmse_scaled}\")\n",
    "print(f\"Mean Absolute Error (MAE) on Scaled Data: {mae_scaled}\")\n",
    "\n",
    "# Reverse the scaling for predictions and actual values\n",
    "y_pred = target_scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_original = target_scaler.inverse_transform(y_test_seq.reshape(-1, 1))\n",
    "\n",
    "# Plot the actual vs predicted values\n",
    "plt.plot(y_test_original, label='Actual')\n",
    "plt.plot(y_pred, label='Predicted')\n",
    "plt.legend()\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Energy Consumption')\n",
    "plt.title('Actual vs Predicted Energy Consumption')\n",
    "plt.savefig('/home/u894059/.local/plot1.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3.2 Hybrid Model - Mid Term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "mtf = pd.read_csv('/home/u894059/.local/dataset/mid_term.csv')\n",
    "\n",
    "# Select the features and target variable\n",
    "features = mtf.drop(columns=['total_weekly_energy_consumption'])\n",
    "target = mtf['total_weekly_energy_consumption'].values.reshape(-1, 1)  # reshape for scaling\n",
    "\n",
    "# Preprocess the data\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(features, target, test_size=0.2, shuffle=False)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.125, shuffle=False)\n",
    "\n",
    "# Normalize the data\n",
    "feature_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "target_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "X_train_scaled = feature_scaler.fit_transform(X_train)\n",
    "X_val_scaled = feature_scaler.transform(X_val)\n",
    "X_test_scaled = feature_scaler.transform(X_test)\n",
    "\n",
    "y_train_scaled = target_scaler.fit_transform(y_train).flatten()\n",
    "y_val_scaled = target_scaler.transform(y_val).flatten()\n",
    "y_test_scaled = target_scaler.transform(y_test).flatten()\n",
    "\n",
    "# Create sliding window sequences\n",
    "def create_sequences(data, target, sequence_length):\n",
    "    sequences, targets = [], []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        sequences.append(data[i:i + sequence_length])\n",
    "        targets.append(target[i + sequence_length])\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "# Experiment with longer sequences\n",
    "sequence_length = 5\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled, sequence_length)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val_scaled, y_val_scaled, sequence_length)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_scaled, sequence_length)\n",
    "\n",
    "# LSTM Block\n",
    "def lstm_block(inputs, lstm_units):\n",
    "    x = layers.LSTM(lstm_units, return_sequences=True, kernel_regularizer='l2')(inputs)\n",
    "    x = layers.Dropout(0.3)(x)  # Increased dropout for regularization\n",
    "    return x\n",
    "\n",
    "# Transformer Encoder\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout_rate, num_blocks):\n",
    "    for _ in range(num_blocks):\n",
    "        # Multi-Head Attention\n",
    "        x = layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads)(inputs, inputs)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(x + inputs)  # Residual Connection\n",
    "        \n",
    "        # Feedforward Block\n",
    "        ff = layers.Conv1D(filters=ff_dim, kernel_size=1, activation='relu')(x)\n",
    "        ff = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(ff)\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(x + ff)  # Residual Connection\n",
    "    \n",
    "    return x\n",
    "\n",
    "# Build the RS-LSTM-Transformer Model\n",
    "def build_model(input_shape, lstm_units, head_size, num_heads, ff_dim, num_blocks, dropout_rate):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # LSTM Block\n",
    "    lstm_out = lstm_block(inputs, lstm_units)\n",
    "    \n",
    "    # Transformer Encoder\n",
    "    transformer_out = transformer_encoder(lstm_out, head_size, num_heads, ff_dim, dropout_rate, num_blocks)\n",
    "    \n",
    "    # Global Average Pooling + Dense Layers\n",
    "    gap = layers.GlobalAveragePooling1D()(transformer_out)\n",
    "    dense = layers.Dense(64, activation='relu')(gap)\n",
    "    outputs = layers.Dense(1)(dense)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# Example Hyperparameters\n",
    "lstm_units = 128\n",
    "head_size = 64\n",
    "num_heads = 8\n",
    "ff_dim = 256\n",
    "num_blocks = 3\n",
    "dropout_rate = 0.2\n",
    "input_shape = (sequence_length, features.shape[1])\n",
    "\n",
    "model = build_model(input_shape, lstm_units, head_size, num_heads, ff_dim, num_blocks, dropout_rate)\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(optimizer=optimizer, \n",
    "              loss='mse', \n",
    "              metrics=['mae'])\n",
    "\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "\n",
    "# Train the Model\n",
    "history = model.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    validation_data=(X_val_seq, y_val_seq),\n",
    "    epochs=50,\n",
    "    batch_size=32,  # Experiment with batch size if needed\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n",
    "\n",
    "# Get predictions on the test set\n",
    "y_pred_scaled = model.predict(X_test_seq)  # Fixing this line\n",
    "\n",
    "# Calculate metrics on the scaled values\n",
    "mse_scaled = mean_squared_error(y_test_seq, y_pred_scaled)\n",
    "rmse_scaled = np.sqrt(mse_scaled)\n",
    "mae_scaled = mean_absolute_error(y_test_seq, y_pred_scaled)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE) on Scaled Data: {mse_scaled}\")\n",
    "print(f\"Root Mean Squared Error (RMSE) on Scaled Data: {rmse_scaled}\")\n",
    "print(f\"Mean Absolute Error (MAE) on Scaled Data: {mae_scaled}\")\n",
    "\n",
    "# Reverse the scaling for predictions and actual values\n",
    "y_pred = target_scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_original = target_scaler.inverse_transform(y_test_seq.reshape(-1, 1))\n",
    "\n",
    "# Plot the actual vs predicted values\n",
    "plt.plot(y_test_original, label='Actual')\n",
    "plt.plot(y_pred, label='Predicted')\n",
    "plt.legend()\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Energy Consumption')\n",
    "plt.title('Actual vs Predicted Energy Consumption')\n",
    "plt.savefig('/home/u894059/.local/plot1.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Models - Baseline Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1.1 LSTM - Short Term "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "stf = pd.read_csv('/home/u894059/.local/dataset/short_term_baseline.csv')\n",
    "\n",
    "# Select the features and target variable\n",
    "features = stf.drop(columns=['total_daily_energy_baseline_consumption'])\n",
    "target = stf['total_daily_energy_baseline_consumption'].values.reshape(-1, 1)  # reshape for scaling\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(features, target, test_size=0.2, shuffle=False)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.125, shuffle=False)\n",
    "\n",
    "# Initialize scalers for features and target\n",
    "feature_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "target_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Fit the scaler on the training data and transform\n",
    "X_train_scaled = feature_scaler.fit_transform(X_train)\n",
    "X_val_scaled = feature_scaler.transform(X_val)  # Use transform, not fit_transform\n",
    "X_test_scaled = feature_scaler.transform(X_test)  # Use transform, not fit_transform\n",
    "\n",
    "# Scale the target variable using the target scaler\n",
    "y_train_scaled = target_scaler.fit_transform(y_train).flatten()\n",
    "y_val_scaled = target_scaler.transform(y_val).flatten()  # Use transform, not fit_transform\n",
    "y_test_scaled = target_scaler.transform(y_test).flatten()  # Use transform, not fit_transform\n",
    "\n",
    "# Convert the scaled features to sequences (after scaling)\n",
    "def create_sequences(data, target, sequence_length=10):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        sequences.append(data[i:i+sequence_length])\n",
    "        targets.append(target[i+sequence_length])\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "# Create sequences\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled, sequence_length=10)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val_scaled, y_val_scaled, sequence_length=10)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_scaled, sequence_length=10)\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(64, return_sequences=True, input_shape=(X_train_seq.shape[1], X_train_seq.shape[2])),  # LSTM layer with 64 units\n",
    "    Dropout(0.3),  # Dropout rate set to 0.3\n",
    "    LSTM(64, return_sequences=False),  # Second LSTM layer with 64 units\n",
    "    Dropout(0.3),  # Dropout rate set to 0.3\n",
    "    Dense(1)  # Final output layer for regression (predicting continuous values)\n",
    "])\n",
    "\n",
    "# Compile the model with a low learning rate for stable training\n",
    "model.compile(optimizer=Nadam(learning_rate=0.001), loss='mse')\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n",
    "\n",
    "# Train the model using the sequences\n",
    "history = model.fit(\n",
    "    X_train_seq, y_train_seq,  # Use sequences for training\n",
    "    epochs=50, \n",
    "    batch_size=128, \n",
    "    validation_data=(X_val_seq, y_val_seq),  # Use sequences for validation\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n",
    "\n",
    "# Get predictions on the test set (scaled values)\n",
    "y_pred_scaled = model.predict(X_test_seq)\n",
    "\n",
    "# Calculate metrics on the scaled values\n",
    "mse_scaled = mean_squared_error(y_test_seq, y_pred_scaled)\n",
    "rmse_scaled = np.sqrt(mse_scaled)\n",
    "mae_scaled = mean_absolute_error(y_test_seq, y_pred_scaled)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE) on Scaled Data: {mse_scaled}\")\n",
    "print(f\"Root Mean Squared Error (RMSE) on Scaled Data: {rmse_scaled}\")\n",
    "print(f\"Mean Absolute Error (MAE) on Scaled Data: {mae_scaled}\")\n",
    "\n",
    "# Define the path where you want to save the plot\n",
    "save_path = '/home/u894059/.local/plot1.png'\n",
    "\n",
    "# Evaluate the model on the scaled test set\n",
    "test_loss = model.evaluate(X_test_seq, y_test_seq)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "\n",
    "# Make predictions on the scaled test set\n",
    "predictions_scaled = model.predict(X_test_seq)\n",
    "\n",
    "# Optionally, inverse transform predictions and actual values to the original scale\n",
    "predictions = target_scaler.inverse_transform(predictions_scaled.reshape(-1, 1)).flatten()\n",
    "y_test_original = target_scaler.inverse_transform(y_test_seq.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Plot the scaled values (0-1 range) or original values\n",
    "plt.plot(y_test_original, label='Actual')\n",
    "plt.plot(predictions, label='Predicted')\n",
    "plt.legend()\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Energy Consumption')\n",
    "plt.title('Actual vs Predicted Energy Consumption')\n",
    "\n",
    "# Save the plot to the specified path\n",
    "plt.savefig(save_path)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1.2 LSTM - Mid Term "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "mtf = pd.read_csv('/home/u894059/.local/dataset/mid_term_baseline.csv')\n",
    "\n",
    "# Select the features and target variable\n",
    "features = mtf.drop(columns=['total_weekly_energy_consumption'])\n",
    "target = mtf['total_weekly_energy_consumption'].values.reshape(-1, 1)  # reshape for scaling\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(features, target, test_size=0.2, shuffle=False)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.125, shuffle=False)\n",
    "\n",
    "# Initialize scalers for features and target\n",
    "feature_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "target_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Fit the scaler on the training data and transform\n",
    "X_train_scaled = feature_scaler.fit_transform(X_train)\n",
    "X_val_scaled = feature_scaler.transform(X_val)  # Use transform, not fit_transform\n",
    "X_test_scaled = feature_scaler.transform(X_test)  # Use transform, not fit_transform\n",
    "\n",
    "# Scale the target variable using the target scaler\n",
    "y_train_scaled = target_scaler.fit_transform(y_train).flatten()\n",
    "y_val_scaled = target_scaler.transform(y_val).flatten()  # Use transform, not fit_transform\n",
    "y_test_scaled = target_scaler.transform(y_test).flatten()  # Use transform, not fit_transform\n",
    "\n",
    "# Convert the scaled features to sequences (after scaling)\n",
    "def create_sequences(data, target, sequence_length=10):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        sequences.append(data[i:i+sequence_length])\n",
    "        targets.append(target[i+sequence_length])\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "# Create sequences\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled, sequence_length=10)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val_scaled, y_val_scaled, sequence_length=10)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_scaled, sequence_length=10)\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(128, return_sequences=True, input_shape=(X_train_seq.shape[1], X_train_seq.shape[2])),  # LSTM layer with 128 units\n",
    "    Dropout(0.2),  # Dropout rate set to 0.2\n",
    "    LSTM(16, return_sequences=False),  # Second LSTM layer with 16 units\n",
    "    Dropout(0.2),  # Dropout rate set to 0.2\n",
    "    Dense(1)  # Final output layer for regression (predicting continuous values)\n",
    "])\n",
    "\n",
    "# Compile the model with a low learning rate for stable training\n",
    "model.compile(optimizer=Nadam(learning_rate=0.001), loss='mse')\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n",
    "\n",
    "# Train the model using the sequences\n",
    "history = model.fit(\n",
    "    X_train_seq, y_train_seq,  # Use sequences for training\n",
    "    epochs=50, \n",
    "    batch_size=128, \n",
    "    validation_data=(X_val_seq, y_val_seq),  # Use sequences for validation\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n",
    "\n",
    "# Get predictions on the test set (scaled values)\n",
    "y_pred_scaled = model.predict(X_test_seq)\n",
    "\n",
    "# Calculate metrics on the scaled values\n",
    "mse_scaled = mean_squared_error(y_test_seq, y_pred_scaled)\n",
    "rmse_scaled = np.sqrt(mse_scaled)\n",
    "mae_scaled = mean_absolute_error(y_test_seq, y_pred_scaled)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE) on Scaled Data: {mse_scaled}\")\n",
    "print(f\"Root Mean Squared Error (RMSE) on Scaled Data: {rmse_scaled}\")\n",
    "print(f\"Mean Absolute Error (MAE) on Scaled Data: {mae_scaled}\")\n",
    "\n",
    "# Define the path where you want to save the plot\n",
    "save_path = '/home/u894059/.local/plot2.png'\n",
    "\n",
    "# Evaluate the model on the scaled test set\n",
    "test_loss = model.evaluate(X_test_seq, y_test_seq)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "\n",
    "# Make predictions on the scaled test set\n",
    "predictions_scaled = model.predict(X_test_seq)\n",
    "\n",
    "# Optionally, inverse transform predictions and actual values to the original scale\n",
    "predictions = target_scaler.inverse_transform(predictions_scaled.reshape(-1, 1)).flatten()\n",
    "y_test_original = target_scaler.inverse_transform(y_test_seq.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Plot the scaled values (0-1 range) or original values\n",
    "plt.plot(y_test_original, label='Actual')\n",
    "plt.plot(predictions, label='Predicted')\n",
    "plt.legend()\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Energy Consumption')\n",
    "plt.title('Actual vs Predicted Energy Consumption')\n",
    "\n",
    "# Save the plot to the specified path\n",
    "plt.savefig(save_path)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2.1 Transformer - Short Term "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "stf = pd.read_csv('/home/u894059/.local/dataset/short_term_baseline.csv')\n",
    "\n",
    "# Select the features and target variable\n",
    "features = stf.drop(columns=['total_daily_energy_baseline_consumption'])\n",
    "target = stf['total_daily_energy_baseline_consumption'].values.reshape(-1, 1)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(features, target, test_size=0.2, shuffle=False)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.125, shuffle=False)\n",
    "\n",
    "# Initialize scalers for features and target\n",
    "feature_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "target_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Fit the scaler on the training data and transform\n",
    "X_train_scaled = feature_scaler.fit_transform(X_train)\n",
    "X_val_scaled = feature_scaler.transform(X_val)\n",
    "X_test_scaled = feature_scaler.transform(X_test)\n",
    "\n",
    "# Scale the target variable using the target scaler\n",
    "y_train_scaled = target_scaler.fit_transform(y_train).flatten()\n",
    "y_val_scaled = target_scaler.transform(y_val).flatten()\n",
    "y_test_scaled = target_scaler.transform(y_test).flatten()\n",
    "\n",
    "# Create sequences\n",
    "def create_sequences(data, target, sequence_length=10):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        sequences.append(data[i:i+sequence_length])\n",
    "        targets.append(target[i+sequence_length])\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "# Create sequences\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled, sequence_length=10)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val_scaled, y_val_scaled, sequence_length=10)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_scaled, sequence_length=10)\n",
    "\n",
    "# Positional Encoding Function\n",
    "def get_positional_encoding(seq_len, d_model):\n",
    "    pos = np.arange(seq_len)[:, np.newaxis]\n",
    "    i = np.arange(d_model)[np.newaxis, :]\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    angle_rads = pos * angle_rates\n",
    "    pos_encoding = np.concatenate([np.sin(angle_rads[:, 0::2]), np.cos(angle_rads[:, 1::2])], axis=-1)\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "# Define Transformer Encoder block with Positional Encoding\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout_rate, seq_len, d_model):\n",
    "    pos_encoding = get_positional_encoding(seq_len, d_model)\n",
    "    inputs += pos_encoding\n",
    "    \n",
    "    attention = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout_rate\n",
    "    )(inputs, inputs)\n",
    "    \n",
    "    attention = layers.LayerNormalization(epsilon=1e-6)(attention + inputs)\n",
    "    \n",
    "    ff = layers.Dense(ff_dim, activation='relu')(attention)\n",
    "    ff = layers.Dense(inputs.shape[-1])(ff)\n",
    "    \n",
    "    return layers.LayerNormalization(epsilon=1e-6)(ff + attention)\n",
    "\n",
    "# Define Transformer Decoder block\n",
    "def transformer_decoder(inputs, encoder_outputs, head_size, num_heads, ff_dim, dropout_rate):\n",
    "    attention_1 = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout_rate\n",
    "    )(inputs, inputs)\n",
    "    \n",
    "    attention_1 = layers.LayerNormalization(epsilon=1e-6)(attention_1 + inputs)\n",
    "    \n",
    "    attention_2 = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout_rate\n",
    "    )(attention_1, encoder_outputs)\n",
    "    \n",
    "    attention_2 = layers.LayerNormalization(epsilon=1e-6)(attention_2 + attention_1)\n",
    "    \n",
    "    ff = layers.Dense(ff_dim, activation='relu')(attention_2)\n",
    "    ff = layers.Dense(inputs.shape[-1])(ff)\n",
    "    \n",
    "    return layers.LayerNormalization(epsilon=1e-6)(ff + attention_2)\n",
    "\n",
    "# Define the Input Layer for Encoder (3D input: samples, time_steps, features)\n",
    "encoder_inputs = layers.Input(shape=(X_train_seq.shape[1], X_train_seq.shape[2]))  # (time_steps, features)\n",
    "encoder_outputs = transformer_encoder(encoder_inputs, head_size=32, num_heads=4, ff_dim=64, dropout_rate=0.1,\n",
    "                                      seq_len=X_train_seq.shape[1], d_model=X_train_seq.shape[2])\n",
    "\n",
    "# Define the Input Layer for Decoder (3D input: samples, time_steps, features)\n",
    "decoder_inputs = layers.Input(shape=(X_train_seq.shape[1], X_train_seq.shape[2]))  # (time_steps, features)\n",
    "decoder_outputs = transformer_decoder(decoder_inputs, encoder_outputs, head_size=32, num_heads=4, ff_dim=64, dropout_rate=0.1)\n",
    "\n",
    "# Global Average Pooling to reduce the time dimension\n",
    "x = layers.GlobalAveragePooling1D()(decoder_outputs)\n",
    "\n",
    "# Output layer (for regression, a single neuron)\n",
    "outputs = layers.Dense(1)(x)\n",
    "\n",
    "# Create the model\n",
    "model = keras.Model(inputs=[encoder_inputs, decoder_inputs], outputs=outputs)\n",
    "\n",
    "# Compile the model\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', patience=5, restore_best_weights=True\n",
    ")\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    [X_train_seq, X_train_seq], y_train_seq,\n",
    "    epochs=50,\n",
    "    batch_size=128,\n",
    "    validation_data=([X_val_seq, X_val_seq], y_val_seq),\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n",
    "\n",
    "# Get predictions on the test set\n",
    "y_pred_scaled = model.predict([X_test_seq, X_test_seq])\n",
    "\n",
    "# Calculate metrics on the scaled values\n",
    "mse_scaled = mean_squared_error(y_test_seq, y_pred_scaled)\n",
    "rmse_scaled = np.sqrt(mse_scaled)\n",
    "mae_scaled = mean_absolute_error(y_test_seq, y_pred_scaled)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE) on Scaled Data: {mse_scaled}\")\n",
    "print(f\"Root Mean Squared Error (RMSE) on Scaled Data: {rmse_scaled}\")\n",
    "print(f\"Mean Absolute Error (MAE) on Scaled Data: {mae_scaled}\")\n",
    "\n",
    "# Reverse the scaling for predictions and actual values\n",
    "y_pred = target_scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_original = target_scaler.inverse_transform(y_test_seq.reshape(-1, 1))\n",
    "\n",
    "# Plot the actual vs predicted values\n",
    "plt.plot(y_test_original, label='Actual')\n",
    "plt.plot(y_pred, label='Predicted')\n",
    "plt.legend()\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Energy Consumption')\n",
    "plt.title('Actual vs Predicted Energy Consumption')\n",
    "plt.savefig('/home/u894059/.local/plot1.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2.2 Transformer - Mid Term "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "stf = pd.read_csv('/home/u894059/.local/dataset/mid_term_baseline.csv')\n",
    "\n",
    "# Select the features and target variable\n",
    "features = stf.drop(columns=['total_weekly_energy_consumption'])\n",
    "target = stf['total_weekly_energy_consumption'].values.reshape(-1, 1)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(features, target, test_size=0.2, shuffle=False)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.125, shuffle=False)\n",
    "\n",
    "# Initialize scalers for features and target\n",
    "feature_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "target_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Fit the scaler on the training data and transform\n",
    "X_train_scaled = feature_scaler.fit_transform(X_train)\n",
    "X_val_scaled = feature_scaler.transform(X_val)\n",
    "X_test_scaled = feature_scaler.transform(X_test)\n",
    "\n",
    "# Scale the target variable using the target scaler\n",
    "y_train_scaled = target_scaler.fit_transform(y_train).flatten()\n",
    "y_val_scaled = target_scaler.transform(y_val).flatten()\n",
    "y_test_scaled = target_scaler.transform(y_test).flatten()\n",
    "\n",
    "# Create sequences\n",
    "def create_sequences(data, target, sequence_length=10):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        sequences.append(data[i:i+sequence_length])\n",
    "        targets.append(target[i+sequence_length])\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "# Create sequences\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled, sequence_length=10)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val_scaled, y_val_scaled, sequence_length=10)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_scaled, sequence_length=10)\n",
    "\n",
    "# Positional Encoding Function\n",
    "def get_positional_encoding(seq_len, d_model):\n",
    "    pos = np.arange(seq_len)[:, np.newaxis]\n",
    "    i = np.arange(d_model)[np.newaxis, :]\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    angle_rads = pos * angle_rates\n",
    "    pos_encoding = np.concatenate([np.sin(angle_rads[:, 0::2]), np.cos(angle_rads[:, 1::2])], axis=-1)\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "# Define Transformer Encoder block with Positional Encoding\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout_rate, seq_len, d_model):\n",
    "    pos_encoding = get_positional_encoding(seq_len, d_model)\n",
    "    inputs += pos_encoding\n",
    "    \n",
    "    attention = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout_rate\n",
    "    )(inputs, inputs)\n",
    "    \n",
    "    attention = layers.LayerNormalization(epsilon=1e-6)(attention + inputs)\n",
    "    \n",
    "    ff = layers.Dense(ff_dim, activation='relu')(attention)\n",
    "    ff = layers.Dense(inputs.shape[-1])(ff)\n",
    "    \n",
    "    return layers.LayerNormalization(epsilon=1e-6)(ff + attention)\n",
    "\n",
    "# Define Transformer Decoder block\n",
    "def transformer_decoder(inputs, encoder_outputs, head_size, num_heads, ff_dim, dropout_rate):\n",
    "    attention_1 = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout_rate\n",
    "    )(inputs, inputs)\n",
    "    \n",
    "    attention_1 = layers.LayerNormalization(epsilon=1e-6)(attention_1 + inputs)\n",
    "    \n",
    "    attention_2 = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout_rate\n",
    "    )(attention_1, encoder_outputs)\n",
    "    \n",
    "    attention_2 = layers.LayerNormalization(epsilon=1e-6)(attention_2 + attention_1)\n",
    "    \n",
    "    ff = layers.Dense(ff_dim, activation='relu')(attention_2)\n",
    "    ff = layers.Dense(inputs.shape[-1])(ff)\n",
    "    \n",
    "    return layers.LayerNormalization(epsilon=1e-6)(ff + attention_2)\n",
    "\n",
    "# Define the Input Layer for Encoder (3D input: samples, time_steps, features)\n",
    "encoder_inputs = layers.Input(shape=(X_train_seq.shape[1], X_train_seq.shape[2]))  # (time_steps, features)\n",
    "encoder_outputs = transformer_encoder(encoder_inputs, head_size=32, num_heads=4, ff_dim=64, dropout_rate=0.1,\n",
    "                                      seq_len=X_train_seq.shape[1], d_model=X_train_seq.shape[2])\n",
    "\n",
    "# Define the Input Layer for Decoder (3D input: samples, time_steps, features)\n",
    "decoder_inputs = layers.Input(shape=(X_train_seq.shape[1], X_train_seq.shape[2]))  # (time_steps, features)\n",
    "decoder_outputs = transformer_decoder(decoder_inputs, encoder_outputs, head_size=32, num_heads=4, ff_dim=64, dropout_rate=0.1)\n",
    "\n",
    "# Global Average Pooling to reduce the time dimension\n",
    "x = layers.GlobalAveragePooling1D()(decoder_outputs)\n",
    "\n",
    "# Output layer (for regression, a single neuron)\n",
    "outputs = layers.Dense(1)(x)\n",
    "\n",
    "# Create the model\n",
    "model = keras.Model(inputs=[encoder_inputs, decoder_inputs], outputs=outputs)\n",
    "\n",
    "# Compile the model\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', patience=5, restore_best_weights=True\n",
    ")\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    [X_train_seq, X_train_seq], y_train_seq,\n",
    "    epochs=50,\n",
    "    batch_size=128,\n",
    "    validation_data=([X_val_seq, X_val_seq], y_val_seq),\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n",
    "\n",
    "# Get predictions on the test set\n",
    "y_pred_scaled = model.predict([X_test_seq, X_test_seq])\n",
    "\n",
    "# Calculate metrics on the scaled values\n",
    "mse_scaled = mean_squared_error(y_test_seq, y_pred_scaled)\n",
    "rmse_scaled = np.sqrt(mse_scaled)\n",
    "mae_scaled = mean_absolute_error(y_test_seq, y_pred_scaled)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE) on Scaled Data: {mse_scaled}\")\n",
    "print(f\"Root Mean Squared Error (RMSE) on Scaled Data: {rmse_scaled}\")\n",
    "print(f\"Mean Absolute Error (MAE) on Scaled Data: {mae_scaled}\")\n",
    "\n",
    "# Reverse the scaling for predictions and actual values\n",
    "y_pred = target_scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_original = target_scaler.inverse_transform(y_test_seq.reshape(-1, 1))\n",
    "\n",
    "# Plot the actual vs predicted values\n",
    "plt.plot(y_test_original, label='Actual')\n",
    "plt.plot(y_pred, label='Predicted')\n",
    "plt.legend()\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Energy Consumption')\n",
    "plt.title('Actual vs Predicted Energy Consumption')\n",
    "plt.savefig('/home/u894059/.local/plot1.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3.1 Hybrid Model - Short Term "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "stf = pd.read_csv('/home/u894059/.local/dataset/short_term_baseline.csv')\n",
    "\n",
    "# Select the features and target variable\n",
    "features = stf.drop(columns=['total_daily_energy_baseline_consumption'])\n",
    "target = stf['total_daily_energy_baseline_consumption'].values.reshape(-1, 1)  # reshape for scaling\n",
    "\n",
    "# Preprocess the data\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(features, target, test_size=0.2, shuffle=False)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.125, shuffle=False)\n",
    "\n",
    "# Normalize the data\n",
    "feature_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "target_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "X_train_scaled = feature_scaler.fit_transform(X_train)\n",
    "X_val_scaled = feature_scaler.transform(X_val)\n",
    "X_test_scaled = feature_scaler.transform(X_test)\n",
    "\n",
    "y_train_scaled = target_scaler.fit_transform(y_train).flatten()\n",
    "y_val_scaled = target_scaler.transform(y_val).flatten()\n",
    "y_test_scaled = target_scaler.transform(y_test).flatten()\n",
    "\n",
    "# Create sliding window sequences\n",
    "def create_sequences(data, target, sequence_length):\n",
    "    sequences, targets = [], []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        sequences.append(data[i:i + sequence_length])\n",
    "        targets.append(target[i + sequence_length])\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "# Experiment with longer sequences\n",
    "sequence_length = 5\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled, sequence_length)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val_scaled, y_val_scaled, sequence_length)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_scaled, sequence_length)\n",
    "\n",
    "# LSTM Block\n",
    "def lstm_block(inputs, lstm_units):\n",
    "    x = layers.LSTM(lstm_units, return_sequences=True, kernel_regularizer='l2')(inputs)\n",
    "    x = layers.Dropout(0.3)(x)  # Increased dropout for regularization\n",
    "    return x\n",
    "\n",
    "# Transformer Encoder\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout_rate, num_blocks):\n",
    "    for _ in range(num_blocks):\n",
    "        # Multi-Head Attention\n",
    "        x = layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads)(inputs, inputs)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(x + inputs)  # Residual Connection\n",
    "        \n",
    "        # Feedforward Block\n",
    "        ff = layers.Conv1D(filters=ff_dim, kernel_size=1, activation='relu')(x)\n",
    "        ff = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(ff)\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(x + ff)  # Residual Connection\n",
    "    \n",
    "    return x\n",
    "\n",
    "# Build the RS-LSTM-Transformer Model\n",
    "def build_model(input_shape, lstm_units, head_size, num_heads, ff_dim, num_blocks, dropout_rate):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # LSTM Block\n",
    "    lstm_out = lstm_block(inputs, lstm_units)\n",
    "    \n",
    "    # Transformer Encoder\n",
    "    transformer_out = transformer_encoder(lstm_out, head_size, num_heads, ff_dim, dropout_rate, num_blocks)\n",
    "    \n",
    "    # Global Average Pooling + Dense Layers\n",
    "    gap = layers.GlobalAveragePooling1D()(transformer_out)\n",
    "    dense = layers.Dense(64, activation='relu')(gap)\n",
    "    outputs = layers.Dense(1)(dense)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# Example Hyperparameters\n",
    "lstm_units = 128\n",
    "head_size = 64\n",
    "num_heads = 8\n",
    "ff_dim = 256\n",
    "num_blocks = 3\n",
    "dropout_rate = 0.2\n",
    "input_shape = (sequence_length, features.shape[1])\n",
    "\n",
    "model = build_model(input_shape, lstm_units, head_size, num_heads, ff_dim, num_blocks, dropout_rate)\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(optimizer=optimizer, \n",
    "              loss='mse', \n",
    "              metrics=['mae'])\n",
    "\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "\n",
    "# Train the Model\n",
    "history = model.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    validation_data=(X_val_seq, y_val_seq),\n",
    "    epochs=50,\n",
    "    batch_size=32,  # Experiment with batch size if needed\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n",
    "\n",
    "# Get predictions on the test set\n",
    "y_pred_scaled = model.predict(X_test_seq)  # Fixing this line\n",
    "\n",
    "# Calculate metrics on the scaled values\n",
    "mse_scaled = mean_squared_error(y_test_seq, y_pred_scaled)\n",
    "rmse_scaled = np.sqrt(mse_scaled)\n",
    "mae_scaled = mean_absolute_error(y_test_seq, y_pred_scaled)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE) on Scaled Data: {mse_scaled}\")\n",
    "print(f\"Root Mean Squared Error (RMSE) on Scaled Data: {rmse_scaled}\")\n",
    "print(f\"Mean Absolute Error (MAE) on Scaled Data: {mae_scaled}\")\n",
    "\n",
    "# Reverse the scaling for predictions and actual values\n",
    "y_pred = target_scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_original = target_scaler.inverse_transform(y_test_seq.reshape(-1, 1))\n",
    "\n",
    "# Plot the actual vs predicted values\n",
    "plt.plot(y_test_original, label='Actual')\n",
    "plt.plot(y_pred, label='Predicted')\n",
    "plt.legend()\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Energy Consumption')\n",
    "plt.title('Actual vs Predicted Energy Consumption')\n",
    "plt.savefig('/home/u894059/.local/plot1.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3.2 Hybrid Model - Mid Term "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "mtf = pd.read_csv('/home/u894059/.local/dataset/mid_term_baseline.csv')\n",
    "\n",
    "# Select the features and target variable\n",
    "features = mtf.drop(columns=['total_weekly_energy_baseline_consumption'])\n",
    "target = mtf['total_weekly_energy_baseline_consumption'].values.reshape(-1, 1)  # reshape for scaling\n",
    "\n",
    "# Preprocess the data\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(features, target, test_size=0.2, shuffle=False)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.125, shuffle=False)\n",
    "\n",
    "# Normalize the data\n",
    "feature_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "target_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "X_train_scaled = feature_scaler.fit_transform(X_train)\n",
    "X_val_scaled = feature_scaler.transform(X_val)\n",
    "X_test_scaled = feature_scaler.transform(X_test)\n",
    "\n",
    "y_train_scaled = target_scaler.fit_transform(y_train).flatten()\n",
    "y_val_scaled = target_scaler.transform(y_val).flatten()\n",
    "y_test_scaled = target_scaler.transform(y_test).flatten()\n",
    "\n",
    "# Create sliding window sequences\n",
    "def create_sequences(data, target, sequence_length):\n",
    "    sequences, targets = [], []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        sequences.append(data[i:i + sequence_length])\n",
    "        targets.append(target[i + sequence_length])\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "# Experiment with longer sequences\n",
    "sequence_length = 5\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled, sequence_length)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val_scaled, y_val_scaled, sequence_length)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_scaled, sequence_length)\n",
    "\n",
    "# LSTM Block\n",
    "def lstm_block(inputs, lstm_units):\n",
    "    x = layers.LSTM(lstm_units, return_sequences=True, kernel_regularizer='l2')(inputs)\n",
    "    x = layers.Dropout(0.3)(x)  # Increased dropout for regularization\n",
    "    return x\n",
    "\n",
    "# Transformer Encoder\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout_rate, num_blocks):\n",
    "    for _ in range(num_blocks):\n",
    "        # Multi-Head Attention\n",
    "        x = layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads)(inputs, inputs)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(x + inputs)  # Residual Connection\n",
    "        \n",
    "        # Feedforward Block\n",
    "        ff = layers.Conv1D(filters=ff_dim, kernel_size=1, activation='relu')(x)\n",
    "        ff = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(ff)\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(x + ff)  # Residual Connection\n",
    "    \n",
    "    return x\n",
    "\n",
    "# Build the RS-LSTM-Transformer Model\n",
    "def build_model(input_shape, lstm_units, head_size, num_heads, ff_dim, num_blocks, dropout_rate):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # LSTM Block\n",
    "    lstm_out = lstm_block(inputs, lstm_units)\n",
    "    \n",
    "    # Transformer Encoder\n",
    "    transformer_out = transformer_encoder(lstm_out, head_size, num_heads, ff_dim, dropout_rate, num_blocks)\n",
    "    \n",
    "    # Global Average Pooling + Dense Layers\n",
    "    gap = layers.GlobalAveragePooling1D()(transformer_out)\n",
    "    dense = layers.Dense(64, activation='relu')(gap)\n",
    "    outputs = layers.Dense(1)(dense)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# Example Hyperparameters\n",
    "lstm_units = 128\n",
    "head_size = 64\n",
    "num_heads = 8\n",
    "ff_dim = 256\n",
    "num_blocks = 3\n",
    "dropout_rate = 0.2\n",
    "input_shape = (sequence_length, features.shape[1])\n",
    "\n",
    "model = build_model(input_shape, lstm_units, head_size, num_heads, ff_dim, num_blocks, dropout_rate)\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(optimizer=optimizer, \n",
    "              loss='mse', \n",
    "              metrics=['mae'])\n",
    "\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "\n",
    "# Train the Model\n",
    "history = model.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    validation_data=(X_val_seq, y_val_seq),\n",
    "    epochs=50,\n",
    "    batch_size=32,  # Experiment with batch size if needed\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n",
    "\n",
    "# Get predictions on the test set\n",
    "y_pred_scaled = model.predict(X_test_seq)  # Fixing this line\n",
    "\n",
    "# Calculate metrics on the scaled values\n",
    "mse_scaled = mean_squared_error(y_test_seq, y_pred_scaled)\n",
    "rmse_scaled = np.sqrt(mse_scaled)\n",
    "mae_scaled = mean_absolute_error(y_test_seq, y_pred_scaled)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE) on Scaled Data: {mse_scaled}\")\n",
    "print(f\"Root Mean Squared Error (RMSE) on Scaled Data: {rmse_scaled}\")\n",
    "print(f\"Mean Absolute Error (MAE) on Scaled Data: {mae_scaled}\")\n",
    "\n",
    "# Reverse the scaling for predictions and actual values\n",
    "y_pred = target_scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_original = target_scaler.inverse_transform(y_test_seq.reshape(-1, 1))\n",
    "\n",
    "# Plot the actual vs predicted values\n",
    "plt.plot(y_test_original, label='Actual')\n",
    "plt.plot(y_pred, label='Predicted')\n",
    "plt.legend()\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Energy Consumption')\n",
    "plt.title('Actual vs Predicted Energy Consumption')\n",
    "plt.savefig('/home/u894059/.local/plot1.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Models - Short Term Granularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1.1 LSTM - Hourly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "stf = pd.read_csv('/home/u894059/.local/dataset/short_term_haf.csv')\n",
    "\n",
    "# Select the features and target variable\n",
    "features = stf.drop(columns=['total_daily_energy_consumption'])\n",
    "target = stf['total_daily_energy_consumption'].values.reshape(-1, 1)  # reshape for scaling\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(features, target, test_size=0.2, shuffle=False)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.125, shuffle=False)\n",
    "\n",
    "# Initialize scalers for features and target\n",
    "feature_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "target_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Fit the scaler on the training data and transform\n",
    "X_train_scaled = feature_scaler.fit_transform(X_train)\n",
    "X_val_scaled = feature_scaler.transform(X_val)  # Use transform, not fit_transform\n",
    "X_test_scaled = feature_scaler.transform(X_test)  # Use transform, not fit_transform\n",
    "\n",
    "# Scale the target variable using the target scaler\n",
    "y_train_scaled = target_scaler.fit_transform(y_train).flatten()\n",
    "y_val_scaled = target_scaler.transform(y_val).flatten()  # Use transform, not fit_transform\n",
    "y_test_scaled = target_scaler.transform(y_test).flatten()  # Use transform, not fit_transform\n",
    "\n",
    "# Convert the scaled features to sequences (after scaling)\n",
    "def create_sequences(data, target, sequence_length=10):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        sequences.append(data[i:i+sequence_length])\n",
    "        targets.append(target[i+sequence_length])\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "# Create sequences\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled, sequence_length=10)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val_scaled, y_val_scaled, sequence_length=10)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_scaled, sequence_length=10)\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(64, return_sequences=True, input_shape=(X_train_seq.shape[1], X_train_seq.shape[2])),  # LSTM layer with 64 units\n",
    "    Dropout(0.3),  # Dropout rate set to 0.3\n",
    "    LSTM(64, return_sequences=False),  # Second LSTM layer with 64 units\n",
    "    Dropout(0.3),  # Dropout rate set to 0.3\n",
    "    Dense(1)  # Final output layer for regression (predicting continuous values)\n",
    "])\n",
    "\n",
    "# Compile the model with a low learning rate for stable training\n",
    "model.compile(optimizer=Nadam(learning_rate=0.001), loss='mse')\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n",
    "\n",
    "# Train the model using the sequences\n",
    "history = model.fit(\n",
    "    X_train_seq, y_train_seq,  # Use sequences for training\n",
    "    epochs=50, \n",
    "    batch_size=128, \n",
    "    validation_data=(X_val_seq, y_val_seq),  # Use sequences for validation\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n",
    "\n",
    "# Get predictions on the test set (scaled values)\n",
    "y_pred_scaled = model.predict(X_test_seq)\n",
    "\n",
    "# Calculate metrics on the scaled values\n",
    "mse_scaled = mean_squared_error(y_test_seq, y_pred_scaled)\n",
    "rmse_scaled = np.sqrt(mse_scaled)\n",
    "mae_scaled = mean_absolute_error(y_test_seq, y_pred_scaled)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE) on Scaled Data: {mse_scaled}\")\n",
    "print(f\"Root Mean Squared Error (RMSE) on Scaled Data: {rmse_scaled}\")\n",
    "print(f\"Mean Absolute Error (MAE) on Scaled Data: {mae_scaled}\")\n",
    "\n",
    "# Define the path where you want to save the plot\n",
    "save_path = '/home/u894059/.local/plot1.png'\n",
    "\n",
    "# Evaluate the model on the scaled test set\n",
    "test_loss = model.evaluate(X_test_seq, y_test_seq)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "\n",
    "# Make predictions on the scaled test set\n",
    "predictions_scaled = model.predict(X_test_seq)\n",
    "\n",
    "# Optionally, inverse transform predictions and actual values to the original scale\n",
    "predictions = target_scaler.inverse_transform(predictions_scaled.reshape(-1, 1)).flatten()\n",
    "y_test_original = target_scaler.inverse_transform(y_test_seq.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Plot the scaled values (0-1 range) or original values\n",
    "plt.plot(y_test_original, label='Actual')\n",
    "plt.plot(predictions, label='Predicted')\n",
    "plt.legend()\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Energy Consumption')\n",
    "plt.title('Actual vs Predicted Energy Consumption')\n",
    "\n",
    "# Save the plot to the specified path\n",
    "plt.savefig(save_path)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1.2 LSTM - Daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "stf = pd.read_csv('/home/u894059/.local/dataset/short_term_daf.csv')\n",
    "\n",
    "# Select the features and target variable\n",
    "features = stf.drop(columns=['total_daily_energy_consumption'])\n",
    "target = stf['total_daily_energy_consumption'].values.reshape(-1, 1)  # reshape for scaling\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(features, target, test_size=0.2, shuffle=False)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.125, shuffle=False)\n",
    "\n",
    "# Initialize scalers for features and target\n",
    "feature_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "target_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Fit the scaler on the training data and transform\n",
    "X_train_scaled = feature_scaler.fit_transform(X_train)\n",
    "X_val_scaled = feature_scaler.transform(X_val)  # Use transform, not fit_transform\n",
    "X_test_scaled = feature_scaler.transform(X_test)  # Use transform, not fit_transform\n",
    "\n",
    "# Scale the target variable using the target scaler\n",
    "y_train_scaled = target_scaler.fit_transform(y_train).flatten()\n",
    "y_val_scaled = target_scaler.transform(y_val).flatten()  # Use transform, not fit_transform\n",
    "y_test_scaled = target_scaler.transform(y_test).flatten()  # Use transform, not fit_transform\n",
    "\n",
    "# Convert the scaled features to sequences (after scaling)\n",
    "def create_sequences(data, target, sequence_length=10):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        sequences.append(data[i:i+sequence_length])\n",
    "        targets.append(target[i+sequence_length])\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "# Create sequences\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled, sequence_length=10)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val_scaled, y_val_scaled, sequence_length=10)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_scaled, sequence_length=10)\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(64, return_sequences=True, input_shape=(X_train_seq.shape[1], X_train_seq.shape[2])),  # LSTM layer with 64 units\n",
    "    Dropout(0.3),  # Dropout rate set to 0.3\n",
    "    LSTM(64, return_sequences=False),  # Second LSTM layer with 64 units\n",
    "    Dropout(0.3),  # Dropout rate set to 0.3\n",
    "    Dense(1)  # Final output layer for regression (predicting continuous values)\n",
    "])\n",
    "\n",
    "# Compile the model with a low learning rate for stable training\n",
    "model.compile(optimizer=Nadam(learning_rate=0.001), loss='mse')\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n",
    "\n",
    "# Train the model using the sequences\n",
    "history = model.fit(\n",
    "    X_train_seq, y_train_seq,  # Use sequences for training\n",
    "    epochs=50, \n",
    "    batch_size=128, \n",
    "    validation_data=(X_val_seq, y_val_seq),  # Use sequences for validation\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n",
    "\n",
    "# Get predictions on the test set (scaled values)\n",
    "y_pred_scaled = model.predict(X_test_seq)\n",
    "\n",
    "# Calculate metrics on the scaled values\n",
    "mse_scaled = mean_squared_error(y_test_seq, y_pred_scaled)\n",
    "rmse_scaled = np.sqrt(mse_scaled)\n",
    "mae_scaled = mean_absolute_error(y_test_seq, y_pred_scaled)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE) on Scaled Data: {mse_scaled}\")\n",
    "print(f\"Root Mean Squared Error (RMSE) on Scaled Data: {rmse_scaled}\")\n",
    "print(f\"Mean Absolute Error (MAE) on Scaled Data: {mae_scaled}\")\n",
    "\n",
    "# Define the path where you want to save the plot\n",
    "save_path = '/home/u894059/.local/plot2.png'\n",
    "\n",
    "# Evaluate the model on the scaled test set\n",
    "test_loss = model.evaluate(X_test_seq, y_test_seq)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "\n",
    "# Make predictions on the scaled test set\n",
    "predictions_scaled = model.predict(X_test_seq)\n",
    "\n",
    "# Optionally, inverse transform predictions and actual values to the original scale\n",
    "predictions = target_scaler.inverse_transform(predictions_scaled.reshape(-1, 1)).flatten()\n",
    "y_test_original = target_scaler.inverse_transform(y_test_seq.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Plot the scaled values (0-1 range) or original values\n",
    "plt.plot(y_test_original, label='Actual')\n",
    "plt.plot(predictions, label='Predicted')\n",
    "plt.legend()\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Energy Consumption')\n",
    "plt.title('Actual vs Predicted Energy Consumption')\n",
    "\n",
    "# Save the plot to the specified path\n",
    "plt.savefig(save_path)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2.1 Transformer - Hourly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "stf = pd.read_csv('/home/u894059/.local/dataset/short_term_haf.csv')\n",
    "\n",
    "# Select the features and target variable\n",
    "features = stf.drop(columns=['total_daily_energy_consumption'])\n",
    "target = stf['total_daily_energy_consumption'].values.reshape(-1, 1)  # reshape for scaling\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(features, target, test_size=0.2, shuffle=False)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.125, shuffle=False)\n",
    "\n",
    "# Initialize scalers for features and target\n",
    "feature_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "target_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Fit the scaler on the training data and transform\n",
    "X_train_scaled = feature_scaler.fit_transform(X_train)\n",
    "X_val_scaled = feature_scaler.transform(X_val)  # Use transform, not fit_transform\n",
    "X_test_scaled = feature_scaler.transform(X_test)  # Use transform, not fit_transform\n",
    "\n",
    "# Scale the target variable using the target scaler\n",
    "y_train_scaled = target_scaler.fit_transform(y_train).flatten()\n",
    "y_val_scaled = target_scaler.transform(y_val).flatten()  # Use transform, not fit_transform\n",
    "y_test_scaled = target_scaler.transform(y_test).flatten()  # Use transform, not fit_transform\n",
    "\n",
    "# Convert the scaled features to sequences (after scaling)\n",
    "def create_sequences(data, target, sequence_length=10):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        sequences.append(data[i:i+sequence_length])\n",
    "        targets.append(target[i+sequence_length])\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "# Create sequences\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled, sequence_length=10)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val_scaled, y_val_scaled, sequence_length=10)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_scaled, sequence_length=10)\n",
    "\n",
    "# Define Transformer Encoder block\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout_rate):\n",
    "    # Multi-head self-attention layer\n",
    "    attention = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout_rate\n",
    "    )(inputs, inputs)\n",
    "    \n",
    "    # Add & Normalize\n",
    "    attention = layers.LayerNormalization(epsilon=1e-6)(attention + inputs)\n",
    "    \n",
    "    # Feed-forward layer\n",
    "    ff = layers.Dense(ff_dim, activation='relu')(attention)\n",
    "    ff = layers.Dense(inputs.shape[-1])(ff)\n",
    "    \n",
    "    # Add & Normalize\n",
    "    return layers.LayerNormalization(epsilon=1e-6)(ff + attention)\n",
    "\n",
    "# Define the Input Layer for Transformer (3D input: samples, time_steps, features)\n",
    "inputs = layers.Input(shape=(X_train_seq.shape[1], X_train_seq.shape[2]))  # Updated input shape to use X_train_seq\n",
    "\n",
    "# Add a transformer encoder block\n",
    "x = transformer_encoder(inputs, head_size=32, num_heads=4, ff_dim=64, dropout_rate=0.1)\n",
    "x = transformer_encoder(inputs, head_size=32, num_heads=4, ff_dim=64, dropout_rate=0.1)\n",
    "x = transformer_encoder(inputs, head_size=32, num_heads=4, ff_dim=64, dropout_rate=0.1)\n",
    "\n",
    "# Global Average Pooling to reduce the time dimension\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "\n",
    "# Output layer (for regression, a single neuron)\n",
    "outputs = layers.Dense(1)(x)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "# Compile the model\n",
    "initial_learning_rate = 0.0001  # You can change this value\n",
    "\n",
    "# Compile the model with the specified learning rate\n",
    "optimizer = Adam(learning_rate=initial_learning_rate)\n",
    "model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # Monitor validation loss\n",
    "    patience=5,          # Stop after 5 epochs of no improvement\n",
    "    restore_best_weights=True  # Restore the best weights when stopping\n",
    ")\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss', \n",
    "    factor=0.5, \n",
    "    patience=5, \n",
    "    min_lr=1e-6, \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train the model with the early stopping callback\n",
    "history = model.fit(\n",
    "    X_train_seq, y_train_seq,  # Use X_train_seq and y_train_seq\n",
    "    epochs=50, \n",
    "    batch_size=128, \n",
    "    validation_data=(X_val_seq, y_val_seq),  # Use X_val_seq and y_val_seq\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n",
    "\n",
    "# Get predictions on the test set (scaled values)\n",
    "y_pred_scaled = model.predict(X_test_seq)\n",
    "\n",
    "# Calculate metrics on the scaled values\n",
    "mse_scaled = mean_squared_error(y_test_seq, y_pred_scaled)\n",
    "rmse_scaled = np.sqrt(mse_scaled)\n",
    "mae_scaled = mean_absolute_error(y_test_seq, y_pred_scaled)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE) on Scaled Data: {mse_scaled}\")\n",
    "print(f\"Root Mean Squared Error (RMSE) on Scaled Data: {rmse_scaled}\")\n",
    "print(f\"Mean Absolute Error (MAE) on Scaled Data: {mae_scaled}\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss = model.evaluate(X_test_seq, y_test_seq)  # Use X_test_seq and y_test_seq for evaluation\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_scaled = model.predict(X_test_seq)  # Use X_test_seq for prediction\n",
    "\n",
    "# Reverse the scaling for predictions and actual values\n",
    "y_pred = target_scaler.inverse_transform(y_pred_scaled)  # Inverse transform predictions to original scale\n",
    "y_test_original = target_scaler.inverse_transform(y_test_seq.reshape(-1, 1))  # Inverse transform test data to original scale\n",
    "\n",
    "# Plot the actual vs predicted values\n",
    "plt.plot(y_test_original, label='Actual')\n",
    "plt.plot(y_pred, label='Predicted')\n",
    "plt.legend()\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Energy Consumption')\n",
    "plt.title('Actual vs Predicted Energy Consumption')\n",
    "\n",
    "# Define the path where you want to save the plot\n",
    "save_path = '/home/u894059/.local/plot1.png'\n",
    "\n",
    "# Save the plot to the specified path\n",
    "plt.savefig(save_path)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2.2 Transformer - Daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "stf = pd.read_csv('/home/u894059/.local/dataset/short_term_daf.csv')\n",
    "\n",
    "# Select the features and target variable\n",
    "features = stf.drop(columns=['total_daily_energy_consumption'])\n",
    "target = stf['total_daily_energy_consumption'].values.reshape(-1, 1)  # reshape for scaling\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(features, target, test_size=0.2, shuffle=False)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.125, shuffle=False)\n",
    "\n",
    "# Initialize scalers for features and target\n",
    "feature_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "target_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Fit the scaler on the training data and transform\n",
    "X_train_scaled = feature_scaler.fit_transform(X_train)\n",
    "X_val_scaled = feature_scaler.transform(X_val)  # Use transform, not fit_transform\n",
    "X_test_scaled = feature_scaler.transform(X_test)  # Use transform, not fit_transform\n",
    "\n",
    "# Scale the target variable using the target scaler\n",
    "y_train_scaled = target_scaler.fit_transform(y_train).flatten()\n",
    "y_val_scaled = target_scaler.transform(y_val).flatten()  # Use transform, not fit_transform\n",
    "y_test_scaled = target_scaler.transform(y_test).flatten()  # Use transform, not fit_transform\n",
    "\n",
    "# Convert the scaled features to sequences (after scaling)\n",
    "def create_sequences(data, target, sequence_length=10):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        sequences.append(data[i:i+sequence_length])\n",
    "        targets.append(target[i+sequence_length])\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "# Create sequences\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled, sequence_length=10)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val_scaled, y_val_scaled, sequence_length=10)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_scaled, sequence_length=10)\n",
    "\n",
    "# Define Transformer Encoder block\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout_rate):\n",
    "    # Multi-head self-attention layer\n",
    "    attention = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout_rate\n",
    "    )(inputs, inputs)\n",
    "    \n",
    "    # Add & Normalize\n",
    "    attention = layers.LayerNormalization(epsilon=1e-6)(attention + inputs)\n",
    "    \n",
    "    # Feed-forward layer\n",
    "    ff = layers.Dense(ff_dim, activation='relu')(attention)\n",
    "    ff = layers.Dense(inputs.shape[-1])(ff)\n",
    "    \n",
    "    # Add & Normalize\n",
    "    return layers.LayerNormalization(epsilon=1e-6)(ff + attention)\n",
    "\n",
    "# Define the Input Layer for Transformer (3D input: samples, time_steps, features)\n",
    "inputs = layers.Input(shape=(X_train_seq.shape[1], X_train_seq.shape[2]))  # Updated input shape to use X_train_seq\n",
    "\n",
    "# Add a transformer encoder block\n",
    "x = transformer_encoder(inputs, head_size=32, num_heads=4, ff_dim=64, dropout_rate=0.1)\n",
    "x = transformer_encoder(inputs, head_size=32, num_heads=4, ff_dim=64, dropout_rate=0.1)\n",
    "x = transformer_encoder(inputs, head_size=32, num_heads=4, ff_dim=64, dropout_rate=0.1)\n",
    "\n",
    "# Global Average Pooling to reduce the time dimension\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "\n",
    "# Output layer (for regression, a single neuron)\n",
    "outputs = layers.Dense(1)(x)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "# Compile the model\n",
    "initial_learning_rate = 0.0001  # You can change this value\n",
    "\n",
    "# Compile the model with the specified learning rate\n",
    "optimizer = Adam(learning_rate=initial_learning_rate)\n",
    "model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # Monitor validation loss\n",
    "    patience=5,          # Stop after 5 epochs of no improvement\n",
    "    restore_best_weights=True  # Restore the best weights when stopping\n",
    ")\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss', \n",
    "    factor=0.5, \n",
    "    patience=5, \n",
    "    min_lr=1e-6, \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train the model with the early stopping callback\n",
    "history = model.fit(\n",
    "    X_train_seq, y_train_seq,  # Use X_train_seq and y_train_seq\n",
    "    epochs=50, \n",
    "    batch_size=128, \n",
    "    validation_data=(X_val_seq, y_val_seq),  # Use X_val_seq and y_val_seq\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n",
    "\n",
    "# Get predictions on the test set (scaled values)\n",
    "y_pred_scaled = model.predict(X_test_seq)\n",
    "\n",
    "# Calculate metrics on the scaled values\n",
    "mse_scaled = mean_squared_error(y_test_seq, y_pred_scaled)\n",
    "rmse_scaled = np.sqrt(mse_scaled)\n",
    "mae_scaled = mean_absolute_error(y_test_seq, y_pred_scaled)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE) on Scaled Data: {mse_scaled}\")\n",
    "print(f\"Root Mean Squared Error (RMSE) on Scaled Data: {rmse_scaled}\")\n",
    "print(f\"Mean Absolute Error (MAE) on Scaled Data: {mae_scaled}\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss = model.evaluate(X_test_seq, y_test_seq)  # Use X_test_seq and y_test_seq for evaluation\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_scaled = model.predict(X_test_seq)  # Use X_test_seq for prediction\n",
    "\n",
    "# Reverse the scaling for predictions and actual values\n",
    "y_pred = target_scaler.inverse_transform(y_pred_scaled)  # Inverse transform predictions to original scale\n",
    "y_test_original = target_scaler.inverse_transform(y_test_seq.reshape(-1, 1))  # Inverse transform test data to original scale\n",
    "\n",
    "# Plot the actual vs predicted values\n",
    "plt.plot(y_test_original, label='Actual')\n",
    "plt.plot(y_pred, label='Predicted')\n",
    "plt.legend()\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Energy Consumption')\n",
    "plt.title('Actual vs Predicted Energy Consumption')\n",
    "\n",
    "# Define the path where you want to save the plot\n",
    "save_path = '/home/u894059/.local/plot2.png'\n",
    "\n",
    "# Save the plot to the specified path\n",
    "plt.savefig(save_path)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3.1 Hybrid Model - Hourly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "stf = pd.read_csv('/home/u894059/.local/dataset/short_term_haf.csv')\n",
    "\n",
    "# Select the features and target variable\n",
    "features = stf.drop(columns=['total_daily_energy_consumption'])\n",
    "target = stf['total_daily_energy_consumption'].values.reshape(-1, 1)  # reshape for scaling\n",
    "\n",
    "# Preprocess the data\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(features, target, test_size=0.2, shuffle=False)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.125, shuffle=False)\n",
    "\n",
    "# Normalize the data\n",
    "feature_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "target_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "X_train_scaled = feature_scaler.fit_transform(X_train)\n",
    "X_val_scaled = feature_scaler.transform(X_val)\n",
    "X_test_scaled = feature_scaler.transform(X_test)\n",
    "\n",
    "y_train_scaled = target_scaler.fit_transform(y_train).flatten()\n",
    "y_val_scaled = target_scaler.transform(y_val).flatten()\n",
    "y_test_scaled = target_scaler.transform(y_test).flatten()\n",
    "\n",
    "# Create sliding window sequences\n",
    "def create_sequences(data, target, sequence_length):\n",
    "    sequences, targets = [], []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        sequences.append(data[i:i + sequence_length])\n",
    "        targets.append(target[i + sequence_length])\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "# Experiment with longer sequences\n",
    "sequence_length = 5\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled, sequence_length)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val_scaled, y_val_scaled, sequence_length)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_scaled, sequence_length)\n",
    "\n",
    "# LSTM Block\n",
    "def lstm_block(inputs, lstm_units):\n",
    "    x = layers.LSTM(lstm_units, return_sequences=True, kernel_regularizer='l2')(inputs)\n",
    "    x = layers.Dropout(0.3)(x)  # Increased dropout for regularization\n",
    "    return x\n",
    "\n",
    "# Transformer Encoder\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout_rate, num_blocks):\n",
    "    for _ in range(num_blocks):\n",
    "        # Multi-Head Attention\n",
    "        x = layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads)(inputs, inputs)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(x + inputs)  # Residual Connection\n",
    "        \n",
    "        # Feedforward Block\n",
    "        ff = layers.Conv1D(filters=ff_dim, kernel_size=1, activation='relu')(x)\n",
    "        ff = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(ff)\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(x + ff)  # Residual Connection\n",
    "    \n",
    "    return x\n",
    "\n",
    "# Build the RS-LSTM-Transformer Model\n",
    "def build_model(input_shape, lstm_units, head_size, num_heads, ff_dim, num_blocks, dropout_rate):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # LSTM Block\n",
    "    lstm_out = lstm_block(inputs, lstm_units)\n",
    "    \n",
    "    # Transformer Encoder\n",
    "    transformer_out = transformer_encoder(lstm_out, head_size, num_heads, ff_dim, dropout_rate, num_blocks)\n",
    "    \n",
    "    # Global Average Pooling + Dense Layers\n",
    "    gap = layers.GlobalAveragePooling1D()(transformer_out)\n",
    "    dense = layers.Dense(64, activation='relu')(gap)\n",
    "    outputs = layers.Dense(1)(dense)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# Example Hyperparameters\n",
    "lstm_units = 128\n",
    "head_size = 64\n",
    "num_heads = 8\n",
    "ff_dim = 256\n",
    "num_blocks = 3\n",
    "dropout_rate = 0.2\n",
    "input_shape = (sequence_length, features.shape[1])\n",
    "\n",
    "model = build_model(input_shape, lstm_units, head_size, num_heads, ff_dim, num_blocks, dropout_rate)\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(optimizer=optimizer, \n",
    "              loss='mse', \n",
    "              metrics=['mae'])\n",
    "\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "\n",
    "# Train the Model\n",
    "history = model.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    validation_data=(X_val_seq, y_val_seq),\n",
    "    epochs=50,\n",
    "    batch_size=32,  # Experiment with batch size if needed\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n",
    "\n",
    "# Get predictions on the test set\n",
    "y_pred_scaled = model.predict(X_test_seq)  # Fixing this line\n",
    "\n",
    "# Calculate metrics on the scaled values\n",
    "mse_scaled = mean_squared_error(y_test_seq, y_pred_scaled)\n",
    "rmse_scaled = np.sqrt(mse_scaled)\n",
    "mae_scaled = mean_absolute_error(y_test_seq, y_pred_scaled)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE) on Scaled Data: {mse_scaled}\")\n",
    "print(f\"Root Mean Squared Error (RMSE) on Scaled Data: {rmse_scaled}\")\n",
    "print(f\"Mean Absolute Error (MAE) on Scaled Data: {mae_scaled}\")\n",
    "\n",
    "# Reverse the scaling for predictions and actual values\n",
    "y_pred = target_scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_original = target_scaler.inverse_transform(y_test_seq.reshape(-1, 1))\n",
    "\n",
    "# Plot the actual vs predicted values\n",
    "plt.plot(y_test_original, label='Actual')\n",
    "plt.plot(y_pred, label='Predicted')\n",
    "plt.legend()\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Energy Consumption')\n",
    "plt.title('Actual vs Predicted Energy Consumption')\n",
    "plt.savefig('/home/u894059/.local/plot1.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3.2 Hybrid Model - Daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "stf = pd.read_csv('/home/u894059/.local/dataset/short_term_daf.csv')\n",
    "\n",
    "# Select the features and target variable\n",
    "features = stf.drop(columns=['total_daily_energy_consumption'])\n",
    "target = stf['total_daily_energy_consumption'].values.reshape(-1, 1)  # reshape for scaling\n",
    "\n",
    "# Preprocess the data\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(features, target, test_size=0.2, shuffle=False)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.125, shuffle=False)\n",
    "\n",
    "# Normalize the data\n",
    "feature_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "target_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "X_train_scaled = feature_scaler.fit_transform(X_train)\n",
    "X_val_scaled = feature_scaler.transform(X_val)\n",
    "X_test_scaled = feature_scaler.transform(X_test)\n",
    "\n",
    "y_train_scaled = target_scaler.fit_transform(y_train).flatten()\n",
    "y_val_scaled = target_scaler.transform(y_val).flatten()\n",
    "y_test_scaled = target_scaler.transform(y_test).flatten()\n",
    "\n",
    "# Create sliding window sequences\n",
    "def create_sequences(data, target, sequence_length):\n",
    "    sequences, targets = [], []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        sequences.append(data[i:i + sequence_length])\n",
    "        targets.append(target[i + sequence_length])\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "# Experiment with longer sequences\n",
    "sequence_length = 5\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled, sequence_length)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val_scaled, y_val_scaled, sequence_length)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_scaled, sequence_length)\n",
    "\n",
    "# LSTM Block\n",
    "def lstm_block(inputs, lstm_units):\n",
    "    x = layers.LSTM(lstm_units, return_sequences=True, kernel_regularizer='l2')(inputs)\n",
    "    x = layers.Dropout(0.3)(x)  # Increased dropout for regularization\n",
    "    return x\n",
    "\n",
    "# Transformer Encoder\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout_rate, num_blocks):\n",
    "    for _ in range(num_blocks):\n",
    "        # Multi-Head Attention\n",
    "        x = layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads)(inputs, inputs)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(x + inputs)  # Residual Connection\n",
    "        \n",
    "        # Feedforward Block\n",
    "        ff = layers.Conv1D(filters=ff_dim, kernel_size=1, activation='relu')(x)\n",
    "        ff = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(ff)\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(x + ff)  # Residual Connection\n",
    "    \n",
    "    return x\n",
    "\n",
    "# Build the RS-LSTM-Transformer Model\n",
    "def build_model(input_shape, lstm_units, head_size, num_heads, ff_dim, num_blocks, dropout_rate):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # LSTM Block\n",
    "    lstm_out = lstm_block(inputs, lstm_units)\n",
    "    \n",
    "    # Transformer Encoder\n",
    "    transformer_out = transformer_encoder(lstm_out, head_size, num_heads, ff_dim, dropout_rate, num_blocks)\n",
    "    \n",
    "    # Global Average Pooling + Dense Layers\n",
    "    gap = layers.GlobalAveragePooling1D()(transformer_out)\n",
    "    dense = layers.Dense(64, activation='relu')(gap)\n",
    "    outputs = layers.Dense(1)(dense)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# Example Hyperparameters\n",
    "lstm_units = 128\n",
    "head_size = 64\n",
    "num_heads = 8\n",
    "ff_dim = 256\n",
    "num_blocks = 3\n",
    "dropout_rate = 0.2\n",
    "input_shape = (sequence_length, features.shape[1])\n",
    "\n",
    "model = build_model(input_shape, lstm_units, head_size, num_heads, ff_dim, num_blocks, dropout_rate)\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(optimizer=optimizer, \n",
    "              loss='mse', \n",
    "              metrics=['mae'])\n",
    "\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "\n",
    "# Train the Model\n",
    "history = model.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    validation_data=(X_val_seq, y_val_seq),\n",
    "    epochs=50,\n",
    "    batch_size=32,  # Experiment with batch size if needed\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n",
    "\n",
    "# Get predictions on the test set\n",
    "y_pred_scaled = model.predict(X_test_seq)  # Fixing this line\n",
    "\n",
    "# Calculate metrics on the scaled values\n",
    "mse_scaled = mean_squared_error(y_test_seq, y_pred_scaled)\n",
    "rmse_scaled = np.sqrt(mse_scaled)\n",
    "mae_scaled = mean_absolute_error(y_test_seq, y_pred_scaled)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE) on Scaled Data: {mse_scaled}\")\n",
    "print(f\"Root Mean Squared Error (RMSE) on Scaled Data: {rmse_scaled}\")\n",
    "print(f\"Mean Absolute Error (MAE) on Scaled Data: {mae_scaled}\")\n",
    "\n",
    "# Reverse the scaling for predictions and actual values\n",
    "y_pred = target_scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_original = target_scaler.inverse_transform(y_test_seq.reshape(-1, 1))\n",
    "\n",
    "# Plot the actual vs predicted values\n",
    "plt.plot(y_test_original, label='Actual')\n",
    "plt.plot(y_pred, label='Predicted')\n",
    "plt.legend()\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Energy Consumption')\n",
    "plt.title('Actual vs Predicted Energy Consumption')\n",
    "plt.savefig('/home/u894059/.local/plot1.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Models - Mid Term Granularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.1.1 LSTM - Daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "stf = pd.read_csv('/home/u894059/.local/dataset/mid_term_daf.csv')\n",
    "stf = stf.drop(columns=['date', 'week'])\n",
    "\n",
    "# Select the features and target variable\n",
    "features = stf.drop(columns=['total_weekly_energy_consumption'])\n",
    "target = stf['total_weekly_energy_consumption'].values.reshape(-1, 1)  # reshape for scaling\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(features, target, test_size=0.2, shuffle=False)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.125, shuffle=False)\n",
    "\n",
    "# Initialize scalers for features and target\n",
    "feature_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "target_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Fit the scaler on the training data and transform\n",
    "X_train_scaled = feature_scaler.fit_transform(X_train)\n",
    "X_val_scaled = feature_scaler.transform(X_val)  # Use transform, not fit_transform\n",
    "X_test_scaled = feature_scaler.transform(X_test)  # Use transform, not fit_transform\n",
    "\n",
    "# Scale the target variable using the target scaler\n",
    "y_train_scaled = target_scaler.fit_transform(y_train).flatten()\n",
    "y_val_scaled = target_scaler.transform(y_val).flatten()  # Use transform, not fit_transform\n",
    "y_test_scaled = target_scaler.transform(y_test).flatten()  # Use transform, not fit_transform\n",
    "\n",
    "# Convert the scaled features to sequences (after scaling)\n",
    "def create_sequences(data, target, sequence_length=10):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        sequences.append(data[i:i+sequence_length])\n",
    "        targets.append(target[i+sequence_length])\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "# Create sequences\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled, sequence_length=10)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val_scaled, y_val_scaled, sequence_length=10)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_scaled, sequence_length=10)\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(64, return_sequences=True, input_shape=(X_train_seq.shape[1], X_train_seq.shape[2])),  # LSTM layer with 64 units\n",
    "    Dropout(0.3),  # Dropout rate set to 0.3\n",
    "    LSTM(64, return_sequences=False),  # Second LSTM layer with 64 units\n",
    "    Dropout(0.3),  # Dropout rate set to 0.3\n",
    "    Dense(1)  # Final output layer for regression (predicting continuous values)\n",
    "])\n",
    "\n",
    "# Compile the model with a low learning rate for stable training\n",
    "model.compile(optimizer=Nadam(learning_rate=0.001), loss='mse')\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n",
    "\n",
    "# Train the model using the sequences\n",
    "history = model.fit(\n",
    "    X_train_seq, y_train_seq,  # Use sequences for training\n",
    "    epochs=50, \n",
    "    batch_size=128, \n",
    "    validation_data=(X_val_seq, y_val_seq),  # Use sequences for validation\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n",
    "\n",
    "# Get predictions on the test set (scaled values)\n",
    "y_pred_scaled = model.predict(X_test_seq)\n",
    "\n",
    "# Calculate metrics on the scaled values\n",
    "mse_scaled = mean_squared_error(y_test_seq, y_pred_scaled)\n",
    "rmse_scaled = np.sqrt(mse_scaled)\n",
    "mae_scaled = mean_absolute_error(y_test_seq, y_pred_scaled)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE) on Scaled Data: {mse_scaled}\")\n",
    "print(f\"Root Mean Squared Error (RMSE) on Scaled Data: {rmse_scaled}\")\n",
    "print(f\"Mean Absolute Error (MAE) on Scaled Data: {mae_scaled}\")\n",
    "\n",
    "# Define the path where you want to save the plot\n",
    "save_path = '/home/u894059/.local/plot1.png'\n",
    "\n",
    "# Evaluate the model on the scaled test set\n",
    "test_loss = model.evaluate(X_test_seq, y_test_seq)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "\n",
    "# Make predictions on the scaled test set\n",
    "predictions_scaled = model.predict(X_test_seq)\n",
    "\n",
    "# Optionally, inverse transform predictions and actual values to the original scale\n",
    "predictions = target_scaler.inverse_transform(predictions_scaled.reshape(-1, 1)).flatten()\n",
    "y_test_original = target_scaler.inverse_transform(y_test_seq.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Plot the scaled values (0-1 range) or original values\n",
    "plt.plot(y_test_original, label='Actual')\n",
    "plt.plot(predictions, label='Predicted')\n",
    "plt.legend()\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Energy Consumption')\n",
    "plt.title('Actual vs Predicted Energy Consumption')\n",
    "\n",
    "# Save the plot to the specified path\n",
    "plt.savefig(save_path)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.1.2 LSTM - Weekly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "stf = pd.read_csv('/home/u894059/.local/dataset/mid_term_waf.csv')\n",
    "\n",
    "# Select the features and target variable\n",
    "features = stf.drop(columns=['total_weekly_energy_consumption'])\n",
    "target = stf['total_weekly_energy_consumption'].values.reshape(-1, 1)  # reshape for scaling\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(features, target, test_size=0.2, shuffle=False)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.125, shuffle=False)\n",
    "\n",
    "# Initialize scalers for features and target\n",
    "feature_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "target_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Fit the scaler on the training data and transform\n",
    "X_train_scaled = feature_scaler.fit_transform(X_train)\n",
    "X_val_scaled = feature_scaler.transform(X_val)  # Use transform, not fit_transform\n",
    "X_test_scaled = feature_scaler.transform(X_test)  # Use transform, not fit_transform\n",
    "\n",
    "# Scale the target variable using the target scaler\n",
    "y_train_scaled = target_scaler.fit_transform(y_train).flatten()\n",
    "y_val_scaled = target_scaler.transform(y_val).flatten()  # Use transform, not fit_transform\n",
    "y_test_scaled = target_scaler.transform(y_test).flatten()  # Use transform, not fit_transform\n",
    "\n",
    "# Convert the scaled features to sequences (after scaling)\n",
    "def create_sequences(data, target, sequence_length=10):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        sequences.append(data[i:i+sequence_length])\n",
    "        targets.append(target[i+sequence_length])\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "# Create sequences\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled, sequence_length=10)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val_scaled, y_val_scaled, sequence_length=10)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_scaled, sequence_length=10)\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(64, return_sequences=True, input_shape=(X_train_seq.shape[1], X_train_seq.shape[2])),  # LSTM layer with 64 units\n",
    "    Dropout(0.3),  # Dropout rate set to 0.3\n",
    "    LSTM(64, return_sequences=False),  # Second LSTM layer with 64 units\n",
    "    Dropout(0.3),  # Dropout rate set to 0.3\n",
    "    Dense(1)  # Final output layer for regression (predicting continuous values)\n",
    "])\n",
    "\n",
    "# Compile the model with a low learning rate for stable training\n",
    "model.compile(optimizer=Nadam(learning_rate=0.001), loss='mse')\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n",
    "\n",
    "# Train the model using the sequences\n",
    "history = model.fit(\n",
    "    X_train_seq, y_train_seq,  # Use sequences for training\n",
    "    epochs=50, \n",
    "    batch_size=128, \n",
    "    validation_data=(X_val_seq, y_val_seq),  # Use sequences for validation\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n",
    "\n",
    "# Get predictions on the test set (scaled values)\n",
    "y_pred_scaled = model.predict(X_test_seq)\n",
    "\n",
    "# Calculate metrics on the scaled values\n",
    "mse_scaled = mean_squared_error(y_test_seq, y_pred_scaled)\n",
    "rmse_scaled = np.sqrt(mse_scaled)\n",
    "mae_scaled = mean_absolute_error(y_test_seq, y_pred_scaled)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE) on Scaled Data: {mse_scaled}\")\n",
    "print(f\"Root Mean Squared Error (RMSE) on Scaled Data: {rmse_scaled}\")\n",
    "print(f\"Mean Absolute Error (MAE) on Scaled Data: {mae_scaled}\")\n",
    "\n",
    "# Define the path where you want to save the plot\n",
    "save_path = '/home/u894059/.local/plot2.png'\n",
    "\n",
    "# Evaluate the model on the scaled test set\n",
    "test_loss = model.evaluate(X_test_seq, y_test_seq)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "\n",
    "# Make predictions on the scaled test set\n",
    "predictions_scaled = model.predict(X_test_seq)\n",
    "\n",
    "# Optionally, inverse transform predictions and actual values to the original scale\n",
    "predictions = target_scaler.inverse_transform(predictions_scaled.reshape(-1, 1)).flatten()\n",
    "y_test_original = target_scaler.inverse_transform(y_test_seq.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Plot the scaled values (0-1 range) or original values\n",
    "plt.plot(y_test_original, label='Actual')\n",
    "plt.plot(predictions, label='Predicted')\n",
    "plt.legend()\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Energy Consumption')\n",
    "plt.title('Actual vs Predicted Energy Consumption')\n",
    "\n",
    "# Save the plot to the specified path\n",
    "plt.savefig(save_path)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2.1 Transformer - Daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "stf = pd.read_csv('/home/u894059/.local/dataset/mid_term_daf.csv')\n",
    "stf = stf.drop(columns = ['date', 'week'])\n",
    "\n",
    "# Select the features and target variable\n",
    "features = stf.drop(columns=['total_weekly_energy_consumption'])\n",
    "target = stf['total_weekly_energy_consumption'].values.reshape(-1, 1)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(features, target, test_size=0.2, shuffle=False)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.125, shuffle=False)\n",
    "\n",
    "# Initialize scalers for features and target\n",
    "feature_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "target_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Fit the scaler on the training data and transform\n",
    "X_train_scaled = feature_scaler.fit_transform(X_train)\n",
    "X_val_scaled = feature_scaler.transform(X_val)\n",
    "X_test_scaled = feature_scaler.transform(X_test)\n",
    "\n",
    "# Scale the target variable using the target scaler\n",
    "y_train_scaled = target_scaler.fit_transform(y_train).flatten()\n",
    "y_val_scaled = target_scaler.transform(y_val).flatten()\n",
    "y_test_scaled = target_scaler.transform(y_test).flatten()\n",
    "\n",
    "# Create sequences\n",
    "def create_sequences(data, target, sequence_length=10):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        sequences.append(data[i:i+sequence_length])\n",
    "        targets.append(target[i+sequence_length])\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "# Create sequences\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled, sequence_length=10)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val_scaled, y_val_scaled, sequence_length=10)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_scaled, sequence_length=10)\n",
    "\n",
    "# Positional Encoding Function\n",
    "def get_positional_encoding(seq_len, d_model):\n",
    "    pos = np.arange(seq_len)[:, np.newaxis]\n",
    "    i = np.arange(d_model)[np.newaxis, :]\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    angle_rads = pos * angle_rates\n",
    "    pos_encoding = np.concatenate([np.sin(angle_rads[:, 0::2]), np.cos(angle_rads[:, 1::2])], axis=-1)\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "# Define Transformer Encoder block with Positional Encoding\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout_rate, seq_len, d_model):\n",
    "    pos_encoding = get_positional_encoding(seq_len, d_model)\n",
    "    inputs += pos_encoding\n",
    "    \n",
    "    attention = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout_rate\n",
    "    )(inputs, inputs)\n",
    "    \n",
    "    attention = layers.LayerNormalization(epsilon=1e-6)(attention + inputs)\n",
    "    \n",
    "    ff = layers.Dense(ff_dim, activation='relu')(attention)\n",
    "    ff = layers.Dense(inputs.shape[-1])(ff)\n",
    "    \n",
    "    return layers.LayerNormalization(epsilon=1e-6)(ff + attention)\n",
    "\n",
    "# Define Transformer Decoder block\n",
    "def transformer_decoder(inputs, encoder_outputs, head_size, num_heads, ff_dim, dropout_rate):\n",
    "    attention_1 = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout_rate\n",
    "    )(inputs, inputs)\n",
    "    \n",
    "    attention_1 = layers.LayerNormalization(epsilon=1e-6)(attention_1 + inputs)\n",
    "    \n",
    "    attention_2 = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout_rate\n",
    "    )(attention_1, encoder_outputs)\n",
    "    \n",
    "    attention_2 = layers.LayerNormalization(epsilon=1e-6)(attention_2 + attention_1)\n",
    "    \n",
    "    ff = layers.Dense(ff_dim, activation='relu')(attention_2)\n",
    "    ff = layers.Dense(inputs.shape[-1])(ff)\n",
    "    \n",
    "    return layers.LayerNormalization(epsilon=1e-6)(ff + attention_2)\n",
    "\n",
    "# Define the Input Layer for Encoder (3D input: samples, time_steps, features)\n",
    "encoder_inputs = layers.Input(shape=(X_train_seq.shape[1], X_train_seq.shape[2]))  # (time_steps, features)\n",
    "encoder_outputs = transformer_encoder(encoder_inputs, head_size=32, num_heads=4, ff_dim=64, dropout_rate=0.1,\n",
    "                                      seq_len=X_train_seq.shape[1], d_model=X_train_seq.shape[2])\n",
    "\n",
    "# Define the Input Layer for Decoder (3D input: samples, time_steps, features)\n",
    "decoder_inputs = layers.Input(shape=(X_train_seq.shape[1], X_train_seq.shape[2]))  # (time_steps, features)\n",
    "decoder_outputs = transformer_decoder(decoder_inputs, encoder_outputs, head_size=32, num_heads=4, ff_dim=64, dropout_rate=0.1)\n",
    "\n",
    "# Global Average Pooling to reduce the time dimension\n",
    "x = layers.GlobalAveragePooling1D()(decoder_outputs)\n",
    "\n",
    "# Output layer (for regression, a single neuron)\n",
    "outputs = layers.Dense(1)(x)\n",
    "\n",
    "# Create the model\n",
    "model = keras.Model(inputs=[encoder_inputs, decoder_inputs], outputs=outputs)\n",
    "\n",
    "# Compile the model\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', patience=5, restore_best_weights=True\n",
    ")\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    [X_train_seq, X_train_seq], y_train_seq,\n",
    "    epochs=50,\n",
    "    batch_size=128,\n",
    "    validation_data=([X_val_seq, X_val_seq], y_val_seq),\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n",
    "\n",
    "# Get predictions on the test set\n",
    "y_pred_scaled = model.predict([X_test_seq, X_test_seq])\n",
    "\n",
    "# Calculate metrics on the scaled values\n",
    "mse_scaled = mean_squared_error(y_test_seq, y_pred_scaled)\n",
    "rmse_scaled = np.sqrt(mse_scaled)\n",
    "mae_scaled = mean_absolute_error(y_test_seq, y_pred_scaled)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE) on Scaled Data: {mse_scaled}\")\n",
    "print(f\"Root Mean Squared Error (RMSE) on Scaled Data: {rmse_scaled}\")\n",
    "print(f\"Mean Absolute Error (MAE) on Scaled Data: {mae_scaled}\")\n",
    "\n",
    "# Reverse the scaling for predictions and actual values\n",
    "y_pred = target_scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_original = target_scaler.inverse_transform(y_test_seq.reshape(-1, 1))\n",
    "\n",
    "# Plot the actual vs predicted values\n",
    "plt.plot(y_test_original, label='Actual')\n",
    "plt.plot(y_pred, label='Predicted')\n",
    "plt.legend()\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Energy Consumption')\n",
    "plt.title('Actual vs Predicted Energy Consumption')\n",
    "plt.savefig('/home/u894059/.local/plot1.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2.2 Transformer - Weekly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "stf = pd.read_csv('/home/u894059/.local/dataset/mid_term_waf.csv')\n",
    "\n",
    "# Select the features and target variable\n",
    "features = stf.drop(columns=['total_weekly_energy_consumption'])\n",
    "target = stf['total_weekly_energy_consumption'].values.reshape(-1, 1)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(features, target, test_size=0.2, shuffle=False)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.125, shuffle=False)\n",
    "\n",
    "# Initialize scalers for features and target\n",
    "feature_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "target_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Fit the scaler on the training data and transform\n",
    "X_train_scaled = feature_scaler.fit_transform(X_train)\n",
    "X_val_scaled = feature_scaler.transform(X_val)\n",
    "X_test_scaled = feature_scaler.transform(X_test)\n",
    "\n",
    "# Scale the target variable using the target scaler\n",
    "y_train_scaled = target_scaler.fit_transform(y_train).flatten()\n",
    "y_val_scaled = target_scaler.transform(y_val).flatten()\n",
    "y_test_scaled = target_scaler.transform(y_test).flatten()\n",
    "\n",
    "# Create sequences\n",
    "def create_sequences(data, target, sequence_length=10):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        sequences.append(data[i:i+sequence_length])\n",
    "        targets.append(target[i+sequence_length])\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "# Create sequences\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled, sequence_length=10)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val_scaled, y_val_scaled, sequence_length=10)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_scaled, sequence_length=10)\n",
    "\n",
    "# Positional Encoding Function\n",
    "def get_positional_encoding(seq_len, d_model):\n",
    "    pos = np.arange(seq_len)[:, np.newaxis]\n",
    "    i = np.arange(d_model)[np.newaxis, :]\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    angle_rads = pos * angle_rates\n",
    "    pos_encoding = np.concatenate([np.sin(angle_rads[:, 0::2]), np.cos(angle_rads[:, 1::2])], axis=-1)\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "# Define Transformer Encoder block with Positional Encoding\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout_rate, seq_len, d_model):\n",
    "    pos_encoding = get_positional_encoding(seq_len, d_model)\n",
    "    inputs += pos_encoding\n",
    "    \n",
    "    attention = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout_rate\n",
    "    )(inputs, inputs)\n",
    "    \n",
    "    attention = layers.LayerNormalization(epsilon=1e-6)(attention + inputs)\n",
    "    \n",
    "    ff = layers.Dense(ff_dim, activation='relu')(attention)\n",
    "    ff = layers.Dense(inputs.shape[-1])(ff)\n",
    "    \n",
    "    return layers.LayerNormalization(epsilon=1e-6)(ff + attention)\n",
    "\n",
    "# Define Transformer Decoder block\n",
    "def transformer_decoder(inputs, encoder_outputs, head_size, num_heads, ff_dim, dropout_rate):\n",
    "    attention_1 = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout_rate\n",
    "    )(inputs, inputs)\n",
    "    \n",
    "    attention_1 = layers.LayerNormalization(epsilon=1e-6)(attention_1 + inputs)\n",
    "    \n",
    "    attention_2 = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout_rate\n",
    "    )(attention_1, encoder_outputs)\n",
    "    \n",
    "    attention_2 = layers.LayerNormalization(epsilon=1e-6)(attention_2 + attention_1)\n",
    "    \n",
    "    ff = layers.Dense(ff_dim, activation='relu')(attention_2)\n",
    "    ff = layers.Dense(inputs.shape[-1])(ff)\n",
    "    \n",
    "    return layers.LayerNormalization(epsilon=1e-6)(ff + attention_2)\n",
    "\n",
    "# Define the Input Layer for Encoder (3D input: samples, time_steps, features)\n",
    "encoder_inputs = layers.Input(shape=(X_train_seq.shape[1], X_train_seq.shape[2]))  # (time_steps, features)\n",
    "encoder_outputs = transformer_encoder(encoder_inputs, head_size=32, num_heads=4, ff_dim=64, dropout_rate=0.1,\n",
    "                                      seq_len=X_train_seq.shape[1], d_model=X_train_seq.shape[2])\n",
    "\n",
    "# Define the Input Layer for Decoder (3D input: samples, time_steps, features)\n",
    "decoder_inputs = layers.Input(shape=(X_train_seq.shape[1], X_train_seq.shape[2]))  # (time_steps, features)\n",
    "decoder_outputs = transformer_decoder(decoder_inputs, encoder_outputs, head_size=32, num_heads=4, ff_dim=64, dropout_rate=0.1)\n",
    "\n",
    "# Global Average Pooling to reduce the time dimension\n",
    "x = layers.GlobalAveragePooling1D()(decoder_outputs)\n",
    "\n",
    "# Output layer (for regression, a single neuron)\n",
    "outputs = layers.Dense(1)(x)\n",
    "\n",
    "# Create the model\n",
    "model = keras.Model(inputs=[encoder_inputs, decoder_inputs], outputs=outputs)\n",
    "\n",
    "# Compile the model\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', patience=5, restore_best_weights=True\n",
    ")\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    [X_train_seq, X_train_seq], y_train_seq,\n",
    "    epochs=50,\n",
    "    batch_size=128,\n",
    "    validation_data=([X_val_seq, X_val_seq], y_val_seq),\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n",
    "\n",
    "# Get predictions on the test set\n",
    "y_pred_scaled = model.predict([X_test_seq, X_test_seq])\n",
    "\n",
    "# Calculate metrics on the scaled values\n",
    "mse_scaled = mean_squared_error(y_test_seq, y_pred_scaled)\n",
    "rmse_scaled = np.sqrt(mse_scaled)\n",
    "mae_scaled = mean_absolute_error(y_test_seq, y_pred_scaled)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE) on Scaled Data: {mse_scaled}\")\n",
    "print(f\"Root Mean Squared Error (RMSE) on Scaled Data: {rmse_scaled}\")\n",
    "print(f\"Mean Absolute Error (MAE) on Scaled Data: {mae_scaled}\")\n",
    "\n",
    "# Reverse the scaling for predictions and actual values\n",
    "y_pred = target_scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_original = target_scaler.inverse_transform(y_test_seq.reshape(-1, 1))\n",
    "\n",
    "# Plot the actual vs predicted values\n",
    "plt.plot(y_test_original, label='Actual')\n",
    "plt.plot(y_pred, label='Predicted')\n",
    "plt.legend()\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Energy Consumption')\n",
    "plt.title('Actual vs Predicted Energy Consumption')\n",
    "plt.savefig('/home/u894059/.local/plot2.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.3.1 Hybrid Model - Daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "mtf = pd.read_csv('/home/u894059/.local/dataset/mid_term_daf.csv')\n",
    "\n",
    "# Select the features and target variable\n",
    "features = mtf.drop(columns=['total_weekly_energy_consumption'])\n",
    "target = mtf['total_weekly_energy_consumption'].values.reshape(-1, 1)  # reshape for scaling\n",
    "\n",
    "# Preprocess the data\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(features, target, test_size=0.2, shuffle=False)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.125, shuffle=False)\n",
    "\n",
    "# Normalize the data\n",
    "feature_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "target_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "X_train_scaled = feature_scaler.fit_transform(X_train)\n",
    "X_val_scaled = feature_scaler.transform(X_val)\n",
    "X_test_scaled = feature_scaler.transform(X_test)\n",
    "\n",
    "y_train_scaled = target_scaler.fit_transform(y_train).flatten()\n",
    "y_val_scaled = target_scaler.transform(y_val).flatten()\n",
    "y_test_scaled = target_scaler.transform(y_test).flatten()\n",
    "\n",
    "# Create sliding window sequences\n",
    "def create_sequences(data, target, sequence_length):\n",
    "    sequences, targets = [], []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        sequences.append(data[i:i + sequence_length])\n",
    "        targets.append(target[i + sequence_length])\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "# Experiment with longer sequences\n",
    "sequence_length = 5\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled, sequence_length)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val_scaled, y_val_scaled, sequence_length)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_scaled, sequence_length)\n",
    "\n",
    "# LSTM Block\n",
    "def lstm_block(inputs, lstm_units):\n",
    "    x = layers.LSTM(lstm_units, return_sequences=True, kernel_regularizer='l2')(inputs)\n",
    "    x = layers.Dropout(0.3)(x)  # Increased dropout for regularization\n",
    "    return x\n",
    "\n",
    "# Transformer Encoder\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout_rate, num_blocks):\n",
    "    for _ in range(num_blocks):\n",
    "        # Multi-Head Attention\n",
    "        x = layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads)(inputs, inputs)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(x + inputs)  # Residual Connection\n",
    "        \n",
    "        # Feedforward Block\n",
    "        ff = layers.Conv1D(filters=ff_dim, kernel_size=1, activation='relu')(x)\n",
    "        ff = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(ff)\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(x + ff)  # Residual Connection\n",
    "    \n",
    "    return x\n",
    "\n",
    "# Build the RS-LSTM-Transformer Model\n",
    "def build_model(input_shape, lstm_units, head_size, num_heads, ff_dim, num_blocks, dropout_rate):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # LSTM Block\n",
    "    lstm_out = lstm_block(inputs, lstm_units)\n",
    "    \n",
    "    # Transformer Encoder\n",
    "    transformer_out = transformer_encoder(lstm_out, head_size, num_heads, ff_dim, dropout_rate, num_blocks)\n",
    "    \n",
    "    # Global Average Pooling + Dense Layers\n",
    "    gap = layers.GlobalAveragePooling1D()(transformer_out)\n",
    "    dense = layers.Dense(64, activation='relu')(gap)\n",
    "    outputs = layers.Dense(1)(dense)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# Example Hyperparameters\n",
    "lstm_units = 128\n",
    "head_size = 64\n",
    "num_heads = 8\n",
    "ff_dim = 256\n",
    "num_blocks = 3\n",
    "dropout_rate = 0.2\n",
    "input_shape = (sequence_length, features.shape[1])\n",
    "\n",
    "model = build_model(input_shape, lstm_units, head_size, num_heads, ff_dim, num_blocks, dropout_rate)\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(optimizer=optimizer, \n",
    "              loss='mse', \n",
    "              metrics=['mae'])\n",
    "\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "\n",
    "# Train the Model\n",
    "history = model.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    validation_data=(X_val_seq, y_val_seq),\n",
    "    epochs=50,\n",
    "    batch_size=32,  # Experiment with batch size if needed\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n",
    "\n",
    "# Get predictions on the test set\n",
    "y_pred_scaled = model.predict(X_test_seq)  # Fixing this line\n",
    "\n",
    "# Calculate metrics on the scaled values\n",
    "mse_scaled = mean_squared_error(y_test_seq, y_pred_scaled)\n",
    "rmse_scaled = np.sqrt(mse_scaled)\n",
    "mae_scaled = mean_absolute_error(y_test_seq, y_pred_scaled)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE) on Scaled Data: {mse_scaled}\")\n",
    "print(f\"Root Mean Squared Error (RMSE) on Scaled Data: {rmse_scaled}\")\n",
    "print(f\"Mean Absolute Error (MAE) on Scaled Data: {mae_scaled}\")\n",
    "\n",
    "# Reverse the scaling for predictions and actual values\n",
    "y_pred = target_scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_original = target_scaler.inverse_transform(y_test_seq.reshape(-1, 1))\n",
    "\n",
    "# Plot the actual vs predicted values\n",
    "plt.plot(y_test_original, label='Actual')\n",
    "plt.plot(y_pred, label='Predicted')\n",
    "plt.legend()\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Energy Consumption')\n",
    "plt.title('Actual vs Predicted Energy Consumption')\n",
    "plt.savefig('/home/u894059/.local/plot1.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.3.2 Hybrid Model - Weekly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "mtf = pd.read_csv('/home/u894059/.local/dataset/mid_term_waf.csv')\n",
    "\n",
    "# Select the features and target variable\n",
    "features = mtf.drop(columns=['total_weekly_energy_consumption'])\n",
    "target = mtf['total_weekly_energy_consumption'].values.reshape(-1, 1)  # reshape for scaling\n",
    "\n",
    "# Preprocess the data\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(features, target, test_size=0.2, shuffle=False)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.125, shuffle=False)\n",
    "\n",
    "# Normalize the data\n",
    "feature_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "target_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "X_train_scaled = feature_scaler.fit_transform(X_train)\n",
    "X_val_scaled = feature_scaler.transform(X_val)\n",
    "X_test_scaled = feature_scaler.transform(X_test)\n",
    "\n",
    "y_train_scaled = target_scaler.fit_transform(y_train).flatten()\n",
    "y_val_scaled = target_scaler.transform(y_val).flatten()\n",
    "y_test_scaled = target_scaler.transform(y_test).flatten()\n",
    "\n",
    "# Create sliding window sequences\n",
    "def create_sequences(data, target, sequence_length):\n",
    "    sequences, targets = [], []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        sequences.append(data[i:i + sequence_length])\n",
    "        targets.append(target[i + sequence_length])\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "# Experiment with longer sequences\n",
    "sequence_length = 5\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled, sequence_length)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val_scaled, y_val_scaled, sequence_length)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_scaled, sequence_length)\n",
    "\n",
    "# LSTM Block\n",
    "def lstm_block(inputs, lstm_units):\n",
    "    x = layers.LSTM(lstm_units, return_sequences=True, kernel_regularizer='l2')(inputs)\n",
    "    x = layers.Dropout(0.3)(x)  # Increased dropout for regularization\n",
    "    return x\n",
    "\n",
    "# Transformer Encoder\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout_rate, num_blocks):\n",
    "    for _ in range(num_blocks):\n",
    "        # Multi-Head Attention\n",
    "        x = layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads)(inputs, inputs)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(x + inputs)  # Residual Connection\n",
    "        \n",
    "        # Feedforward Block\n",
    "        ff = layers.Conv1D(filters=ff_dim, kernel_size=1, activation='relu')(x)\n",
    "        ff = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(ff)\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(x + ff)  # Residual Connection\n",
    "    \n",
    "    return x\n",
    "\n",
    "# Build the RS-LSTM-Transformer Model\n",
    "def build_model(input_shape, lstm_units, head_size, num_heads, ff_dim, num_blocks, dropout_rate):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # LSTM Block\n",
    "    lstm_out = lstm_block(inputs, lstm_units)\n",
    "    \n",
    "    # Transformer Encoder\n",
    "    transformer_out = transformer_encoder(lstm_out, head_size, num_heads, ff_dim, dropout_rate, num_blocks)\n",
    "    \n",
    "    # Global Average Pooling + Dense Layers\n",
    "    gap = layers.GlobalAveragePooling1D()(transformer_out)\n",
    "    dense = layers.Dense(64, activation='relu')(gap)\n",
    "    outputs = layers.Dense(1)(dense)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# Example Hyperparameters\n",
    "lstm_units = 128\n",
    "head_size = 64\n",
    "num_heads = 8\n",
    "ff_dim = 256\n",
    "num_blocks = 3\n",
    "dropout_rate = 0.2\n",
    "input_shape = (sequence_length, features.shape[1])\n",
    "\n",
    "model = build_model(input_shape, lstm_units, head_size, num_heads, ff_dim, num_blocks, dropout_rate)\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(optimizer=optimizer, \n",
    "              loss='mse', \n",
    "              metrics=['mae'])\n",
    "\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "\n",
    "# Train the Model\n",
    "history = model.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    validation_data=(X_val_seq, y_val_seq),\n",
    "    epochs=50,\n",
    "    batch_size=32,  # Experiment with batch size if needed\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n",
    "\n",
    "# Get predictions on the test set\n",
    "y_pred_scaled = model.predict(X_test_seq)  # Fixing this line\n",
    "\n",
    "# Calculate metrics on the scaled values\n",
    "mse_scaled = mean_squared_error(y_test_seq, y_pred_scaled)\n",
    "rmse_scaled = np.sqrt(mse_scaled)\n",
    "mae_scaled = mean_absolute_error(y_test_seq, y_pred_scaled)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE) on Scaled Data: {mse_scaled}\")\n",
    "print(f\"Root Mean Squared Error (RMSE) on Scaled Data: {rmse_scaled}\")\n",
    "print(f\"Mean Absolute Error (MAE) on Scaled Data: {mae_scaled}\")\n",
    "\n",
    "# Reverse the scaling for predictions and actual values\n",
    "y_pred = target_scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_original = target_scaler.inverse_transform(y_test_seq.reshape(-1, 1))\n",
    "\n",
    "# Plot the actual vs predicted values\n",
    "plt.plot(y_test_original, label='Actual')\n",
    "plt.plot(y_pred, label='Predicted')\n",
    "plt.legend()\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Energy Consumption')\n",
    "plt.title('Actual vs Predicted Energy Consumption')\n",
    "plt.savefig('/home/u894059/.local/plot1.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "stf = pd.read_csv('/home/u894059/.local/dataset/short_term.csv')\n",
    "\n",
    "# Select the features and target\n",
    "features = stf.drop(columns=['total_daily_energy_consumption'])\n",
    "target = stf['total_daily_energy_consumption'].values.reshape(-1, 1)\n",
    "\n",
    "# Preprocess the data\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(features, target, test_size=0.2, shuffle=False)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.125, shuffle=False)\n",
    "\n",
    "# Normalize the data\n",
    "feature_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "target_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "X_train_scaled = feature_scaler.fit_transform(X_train)\n",
    "X_val_scaled = feature_scaler.transform(X_val)\n",
    "X_test_scaled = feature_scaler.transform(X_test)\n",
    "\n",
    "y_train_scaled = target_scaler.fit_transform(y_train).flatten()\n",
    "y_val_scaled = target_scaler.transform(y_val).flatten()\n",
    "y_test_scaled = target_scaler.transform(y_test).flatten()\n",
    "\n",
    "# Create sliding window sequences\n",
    "def create_sequences(data, target, sequence_length):\n",
    "    sequences, targets = [], []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        sequences.append(data[i:i + sequence_length])\n",
    "        targets.append(target[i + sequence_length])\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "# Experiment with longer sequences\n",
    "sequence_length = 30\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled, sequence_length)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val_scaled, y_val_scaled, sequence_length)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_scaled, sequence_length)\n",
    "\n",
    "# LSTM Block\n",
    "def lstm_block(inputs, lstm_units):\n",
    "    x = layers.LSTM(lstm_units, return_sequences=True, kernel_regularizer='l2')(inputs)\n",
    "    x = layers.Dropout(0.3)(x)  # Increased dropout for regularization\n",
    "    return x\n",
    "\n",
    "# Transformer Encoder\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout_rate, num_blocks):\n",
    "    for _ in range(num_blocks):\n",
    "        # Multi-Head Attention\n",
    "        x = layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads)(inputs, inputs)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(x + inputs)  # Residual Connection\n",
    "        \n",
    "        # Feedforward Block\n",
    "        ff = layers.Conv1D(filters=ff_dim, kernel_size=1, activation='relu')(x)\n",
    "        ff = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(ff)\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(x + ff)  # Residual Connection\n",
    "    \n",
    "    return x\n",
    "\n",
    "# Build the RS-LSTM-Transformer Model\n",
    "def build_model(input_shape, lstm_units, head_size, num_heads, ff_dim, num_blocks, dropout_rate):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # LSTM Block\n",
    "    lstm_out = lstm_block(inputs, lstm_units)\n",
    "    \n",
    "    # Transformer Encoder\n",
    "    transformer_out = transformer_encoder(lstm_out, head_size, num_heads, ff_dim, dropout_rate, num_blocks)\n",
    "    \n",
    "    # Global Average Pooling + Dense Layers\n",
    "    gap = layers.GlobalAveragePooling1D()(transformer_out)\n",
    "    dense = layers.Dense(64, activation='relu')(gap)\n",
    "    outputs = layers.Dense(1)(dense)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# Example Hyperparameters\n",
    "lstm_units = 128\n",
    "head_size = 64\n",
    "num_heads = 8\n",
    "ff_dim = 256\n",
    "num_blocks = 3\n",
    "dropout_rate = 0.2\n",
    "input_shape = (sequence_length, features.shape[1])\n",
    "\n",
    "model = build_model(input_shape, lstm_units, head_size, num_heads, ff_dim, num_blocks, dropout_rate)\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(optimizer=optimizer, \n",
    "              loss='mse', \n",
    "              metrics=['mae'])\n",
    "\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "\n",
    "# Train the Model\n",
    "history = model.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    validation_data=(X_val_seq, y_val_seq),\n",
    "    epochs=100,\n",
    "    batch_size=32,  # Experiment with batch size if needed\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred_scaled = model.predict(X_test_seq)\n",
    "mse = mean_squared_error(y_test_seq, y_pred_scaled)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test_seq, y_pred_scaled)\n",
    "\n",
    "# Inverse transform predictions\n",
    "y_pred = target_scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_original = target_scaler.inverse_transform(y_test_seq.reshape(-1, 1))\n",
    "\n",
    "print(f\"MSE: {mse}, RMSE: {rmse}, MAE: {mae}\")\n",
    "\n",
    "# Plot predictions vs actual values\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(y_test_original, label='Actual')\n",
    "plt.plot(y_pred, label='Predicted')\n",
    "plt.legend()\n",
    "plt.title(\"Flood Forecasting: Actual vs Predicted\")\n",
    "plt.xlabel(\"Time Steps\")\n",
    "plt.ylabel(\"Runoff\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.1.1 LSTM Hyperparameters - Short Term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "stf = pd.read_csv('/home/u894059/.local/dataset/short_term.csv')\n",
    "\n",
    "# Select the features and target variable\n",
    "features = stf.drop(columns=['total_daily_energy_consumption'])\n",
    "target = stf['total_daily_energy_consumption'].values.reshape(-1, 1)  # reshape for scaling\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(features, target, test_size=0.2, shuffle=False)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.125, shuffle=False)\n",
    "\n",
    "# Initialize scalers for features and target\n",
    "feature_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "target_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Fit the scaler on the training data and transform\n",
    "X_train_scaled = feature_scaler.fit_transform(X_train)\n",
    "X_val_scaled = feature_scaler.transform(X_val)  # Use transform, not fit_transform\n",
    "X_test_scaled = feature_scaler.transform(X_test)  # Use transform, not fit_transform\n",
    "\n",
    "# Scale the target variable using the target scaler\n",
    "y_train_scaled = target_scaler.fit_transform(y_train).flatten()\n",
    "y_val_scaled = target_scaler.transform(y_val).flatten()  # Use transform, not fit_transform\n",
    "y_test_scaled = target_scaler.transform(y_test).flatten()  # Use transform, not fit_transform\n",
    "\n",
    "# Convert the scaled features to sequences (after scaling)\n",
    "def create_sequences(data, target, sequence_length=5):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        sequences.append(data[i:i+sequence_length])\n",
    "        targets.append(target[i+sequence_length])\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "# Create sequences\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled, sequence_length=5)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val_scaled, y_val_scaled, sequence_length=5)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_scaled, sequence_length=5)\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Hyperparameter tuning\n",
    "    lstm_units = trial.suggest_int('lstm_units', 32, 128)  # LSTM units range\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.2, 0.5)  # Dropout rate range\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)  \n",
    "  # Log scale for learning rate\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])  # Batch size options\n",
    "\n",
    "    # Build the model with the suggested hyperparameters\n",
    "    model = Sequential([\n",
    "        Input(shape=(X_train_seq.shape[1], X_train_seq.shape[2])),  # Input layer\n",
    "        LSTM(lstm_units, return_sequences=True),\n",
    "        Dropout(dropout_rate),\n",
    "        LSTM(lstm_units, return_sequences=False),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(1)])\n",
    "\n",
    "    model.compile(optimizer=Nadam(learning_rate=learning_rate), loss='mse')\n",
    "\n",
    "    # Define early stopping and learning rate reduction callbacks\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(\n",
    "        X_train_seq, y_train_seq, \n",
    "        epochs=30, \n",
    "        batch_size=batch_size, \n",
    "        validation_data=(X_val_seq, y_val_seq), \n",
    "        callbacks=[early_stopping, reduce_lr], \n",
    "        verbose=1  # Suppress training output\n",
    "    )\n",
    "\n",
    "    # Get predictions and calculate the MSE on the validation set\n",
    "    y_pred_scaled = model.predict(X_val_seq)\n",
    "    mse = mean_squared_error(y_val_seq, y_pred_scaled)\n",
    "    return mse  # We want to minimize the MSE\n",
    "\n",
    "# Create an Optuna study to optimize the objective function\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=20)  # Perform 50 trials\n",
    "\n",
    "# Print the best hyperparameters found\n",
    "print(\"Best hyperparameters: \", study.best_params)\n",
    "\n",
    "# Use the best hyperparameters to train the final model\n",
    "best_params = study.best_params\n",
    "\n",
    "# Final model definition with the best hyperparameters\n",
    "final_model = Sequential([\n",
    "    LSTM(best_params['lstm_units'], return_sequences=True, input_shape=(X_train_seq.shape[1], X_train_seq.shape[2])),  \n",
    "    Dropout(best_params['dropout_rate']),  \n",
    "    LSTM(best_params['lstm_units'], return_sequences=False),  \n",
    "    Dropout(best_params['dropout_rate']),  \n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "final_model.compile(optimizer=Nadam(learning_rate=best_params['learning_rate']), loss='mse')\n",
    "\n",
    "# Train the final model\n",
    "final_model.fit(\n",
    "    X_train_seq, y_train_seq, \n",
    "    epochs=50, \n",
    "    batch_size=best_params['batch_size'], \n",
    "    validation_data=(X_val_seq, y_val_seq),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Make predictions on the test set (scaled values)\n",
    "y_pred_scaled = final_model.predict(X_test_seq)\n",
    "\n",
    "# Calculate metrics on the scaled values\n",
    "mse_scaled = mean_squared_error(y_test_seq, y_pred_scaled)\n",
    "rmse_scaled = np.sqrt(mse_scaled)\n",
    "mae_scaled = mean_absolute_error(y_test_seq, y_pred_scaled)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE) on Scaled Data: {mse_scaled}\")\n",
    "print(f\"Root Mean Squared Error (RMSE) on Scaled Data: {rmse_scaled}\")\n",
    "print(f\"Mean Absolute Error (MAE) on Scaled Data: {mae_scaled}\")\n",
    "\n",
    "# Plot predictions vs actual values\n",
    "save_path = '/home/u894059/.local/LSTM_STG_hourly.png'\n",
    "\n",
    "# Evaluate the model on the scaled test set\n",
    "test_loss = final_model.evaluate(X_test_seq, y_test_seq)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "\n",
    "# Make predictions on the scaled test set\n",
    "predictions_scaled = final_model.predict(X_test_seq)\n",
    "\n",
    "# Optionally, inverse transform predictions and actual values to the original scale\n",
    "predictions = target_scaler.inverse_transform(predictions_scaled.reshape(-1, 1)).flatten()\n",
    "y_test_original = target_scaler.inverse_transform(y_test_seq.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Plot the scaled values (0-1 range) or original values\n",
    "plt.plot(y_test_original, label='Actual')\n",
    "plt.plot(predictions, label='Predicted')\n",
    "plt.legend()\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Energy Consumption')\n",
    "plt.title('Actual vs Predicted Energy Consumption')\n",
    "\n",
    "# Save the plot to the specified path\n",
    "plt.savefig(save_path)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.1.2 LSTM Hyperparameters - Mid Term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "mtf = pd.read_csv('/home/u894059/.local/dataset/mid_term.csv')\n",
    "\n",
    "# Select the features and target variable\n",
    "features = mtf.drop(columns=['total_weekly_energy_consumption'])\n",
    "target = mtf['total_weekly_energy_consumption'].values.reshape(-1, 1)  # reshape for scaling\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(features, target, test_size=0.2, shuffle=False)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.125, shuffle=False)\n",
    "\n",
    "# Initialize scalers for features and target\n",
    "feature_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "target_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Fit the scaler on the training data and transform\n",
    "X_train_scaled = feature_scaler.fit_transform(X_train)\n",
    "X_val_scaled = feature_scaler.transform(X_val)  # Use transform, not fit_transform\n",
    "X_test_scaled = feature_scaler.transform(X_test)  # Use transform, not fit_transform\n",
    "\n",
    "# Scale the target variable using the target scaler\n",
    "y_train_scaled = target_scaler.fit_transform(y_train).flatten()\n",
    "y_val_scaled = target_scaler.transform(y_val).flatten()  # Use transform, not fit_transform\n",
    "y_test_scaled = target_scaler.transform(y_test).flatten()  # Use transform, not fit_transform\n",
    "\n",
    "# Convert the scaled features to sequences (after scaling)\n",
    "def create_sequences(data, target, sequence_length=5):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        sequences.append(data[i:i+sequence_length])\n",
    "        targets.append(target[i+sequence_length])\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "# Create sequences\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled, sequence_length=5)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val_scaled, y_val_scaled, sequence_length=5)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_scaled, sequence_length=5)\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Hyperparameter tuning\n",
    "    lstm_units = trial.suggest_int('lstm_units', 32, 128)  # LSTM units range\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.2, 0.5)  # Dropout rate range\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)  \n",
    "  # Log scale for learning rate\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])  # Batch size options\n",
    "\n",
    "    # Build the model with the suggested hyperparameters\n",
    "    model = Sequential([\n",
    "        Input(shape=(X_train_seq.shape[1], X_train_seq.shape[2])),  # Input layer\n",
    "        LSTM(lstm_units, return_sequences=True),\n",
    "        Dropout(dropout_rate),\n",
    "        LSTM(lstm_units, return_sequences=False),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(1)])\n",
    "\n",
    "    model.compile(optimizer=Nadam(learning_rate=learning_rate), loss='mse')\n",
    "\n",
    "    # Define early stopping and learning rate reduction callbacks\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(\n",
    "        X_train_seq, y_train_seq, \n",
    "        epochs=30, \n",
    "        batch_size=batch_size, \n",
    "        validation_data=(X_val_seq, y_val_seq), \n",
    "        callbacks=[early_stopping, reduce_lr], \n",
    "        verbose=1  # Suppress training output\n",
    "    )\n",
    "\n",
    "    # Get predictions and calculate the MSE on the validation set\n",
    "    y_pred_scaled = model.predict(X_val_seq)\n",
    "    mse = mean_squared_error(y_val_seq, y_pred_scaled)\n",
    "    return mse  # We want to minimize the MSE\n",
    "\n",
    "# Create an Optuna study to optimize the objective function\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=20)  # Perform 50 trials\n",
    "\n",
    "# Print the best hyperparameters found\n",
    "print(\"Best hyperparameters: \", study.best_params)\n",
    "\n",
    "# Use the best hyperparameters to train the final model\n",
    "best_params = study.best_params\n",
    "\n",
    "# Final model definition with the best hyperparameters\n",
    "final_model = Sequential([\n",
    "    LSTM(best_params['lstm_units'], return_sequences=True, input_shape=(X_train_seq.shape[1], X_train_seq.shape[2])),  \n",
    "    Dropout(best_params['dropout_rate']),  \n",
    "    LSTM(best_params['lstm_units'], return_sequences=False),  \n",
    "    Dropout(best_params['dropout_rate']),  \n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "final_model.compile(optimizer=Nadam(learning_rate=best_params['learning_rate']), loss='mse')\n",
    "\n",
    "# Train the final model\n",
    "final_model.fit(\n",
    "    X_train_seq, y_train_seq, \n",
    "    epochs=50, \n",
    "    batch_size=best_params['batch_size'], \n",
    "    validation_data=(X_val_seq, y_val_seq),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Make predictions on the test set (scaled values)\n",
    "y_pred_scaled = final_model.predict(X_test_seq)\n",
    "\n",
    "# Calculate metrics on the scaled values\n",
    "mse_scaled = mean_squared_error(y_test_seq, y_pred_scaled)\n",
    "rmse_scaled = np.sqrt(mse_scaled)\n",
    "mae_scaled = mean_absolute_error(y_test_seq, y_pred_scaled)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE) on Scaled Data: {mse_scaled}\")\n",
    "print(f\"Root Mean Squared Error (RMSE) on Scaled Data: {rmse_scaled}\")\n",
    "print(f\"Mean Absolute Error (MAE) on Scaled Data: {mae_scaled}\")\n",
    "\n",
    "# Plot predictions vs actual values\n",
    "save_path = '/home/u894059/.local/LSTM_STG_hourly.png'\n",
    "\n",
    "# Evaluate the model on the scaled test set\n",
    "test_loss = final_model.evaluate(X_test_seq, y_test_seq)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "\n",
    "# Make predictions on the scaled test set\n",
    "predictions_scaled = final_model.predict(X_test_seq)\n",
    "\n",
    "# Optionally, inverse transform predictions and actual values to the original scale\n",
    "predictions = target_scaler.inverse_transform(predictions_scaled.reshape(-1, 1)).flatten()\n",
    "y_test_original = target_scaler.inverse_transform(y_test_seq.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Plot the scaled values (0-1 range) or original values\n",
    "plt.plot(y_test_original, label='Actual')\n",
    "plt.plot(predictions, label='Predicted')\n",
    "plt.legend()\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Energy Consumption')\n",
    "plt.title('Actual vs Predicted Energy Consumption')\n",
    "\n",
    "# Save the plot to the specified path\n",
    "plt.savefig(save_path)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.2.1 LSTM Optimiser - Short Term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of optimizers to test\n",
    "optimizers = {\n",
    "    \"adam\": Adam,\n",
    "    \"sgd\": SGD,\n",
    "    \"rmsprop\": RMSprop,\n",
    "    \"adagrad\": Adagrad,\n",
    "    \"adamax\": Adamax,\n",
    "    \"nadam\": Nadam\n",
    "}\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Load dataset\n",
    "stf = pd.read_csv('/home/u894059/.local/dataset/short_term.csv')\n",
    "\n",
    "# Select the features and target variable\n",
    "features = stf.drop(columns=['total_daily_energy_consumption'])\n",
    "target = stf['total_daily_energy_consumption'].values.reshape(-1, 1)  # reshape for scaling\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(features, target, test_size=0.2, shuffle=False)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.125, shuffle=False)\n",
    "\n",
    "# Initialize scalers for features and target\n",
    "feature_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "target_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Fit the scaler on the training data and transform\n",
    "X_train_scaled = feature_scaler.fit_transform(X_train)\n",
    "X_val_scaled = feature_scaler.transform(X_val)  # Use transform, not fit_transform\n",
    "X_test_scaled = feature_scaler.transform(X_test)  # Use transform, not fit_transform\n",
    "\n",
    "# Scale the target variable using the target scaler\n",
    "y_train_scaled = target_scaler.fit_transform(y_train).flatten()\n",
    "y_val_scaled = target_scaler.transform(y_val).flatten()  # Use transform, not fit_transform\n",
    "y_test_scaled = target_scaler.transform(y_test).flatten()  # Use transform, not fit_transform\n",
    "\n",
    "# Convert the scaled features to sequences (after scaling)\n",
    "def create_sequences(data, target, sequence_length=5):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        sequences.append(data[i:i+sequence_length])\n",
    "        targets.append(target[i+sequence_length])\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "# Create sequences\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled, sequence_length=5)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val_scaled, y_val_scaled, sequence_length=5)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_scaled, sequence_length=5)\n",
    "\n",
    "# Iterate over each optimizer\n",
    "for opt_name in optimizers:\n",
    "    optimizer = optimizers[opt_name]\n",
    "    print(f\"Using optimizer: {opt_name}\")\n",
    "    \n",
    "    # Define the LSTM model\n",
    "    model = Sequential([\n",
    "        keras.Input(shape=(X_train_seq.shape[1], X_train_seq.shape[2])),\n",
    "        LSTM(109, return_sequences=True),\n",
    "        Dropout(0.2053906727416861),  \n",
    "        LSTM(109, return_sequences=False),  \n",
    "        Dropout(0.2053906727416861),  \n",
    "        Dense(1)  \n",
    "    ])\n",
    "\n",
    "    # Compile the model with the current optimizer\n",
    "    model.compile(optimizer=opt_name, loss='mse')\n",
    "    \n",
    "    # Define callbacks\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n",
    "    \n",
    "    # Train the model using the sequences\n",
    "    history = model.fit(\n",
    "        X_train_seq, y_train_seq,  \n",
    "        epochs=50, \n",
    "        batch_size=32, \n",
    "        validation_data=(X_val_seq, y_val_seq),  \n",
    "        callbacks=[early_stopping, reduce_lr],\n",
    "        verbose=1  # Suppress detailed logs for readability\n",
    "    )\n",
    "    \n",
    "    # Evaluate the model on the test set\n",
    "    test_loss = model.evaluate(X_test_seq, y_test_seq, verbose=1)  # MSE on test data\n",
    "    \n",
    "    # Get predictions and calculate metrics\n",
    "    predictions_scaled = model.predict(X_test_seq)\n",
    "    mse_scaled = mean_squared_error(y_test_seq, predictions_scaled)\n",
    "    rmse_scaled = np.sqrt(mse_scaled)\n",
    "    mae_scaled = mean_absolute_error(y_test_seq, predictions_scaled)\n",
    "    \n",
    "    # Store results\n",
    "    results[opt_name] = {\n",
    "        \"Test Loss (MSE)\": test_loss,\n",
    "        \"RMSE (scaled)\": rmse_scaled,\n",
    "        \"MAE (scaled)\": mae_scaled\n",
    "    }\n",
    "\n",
    "# Display results for comparison\n",
    "print(\"\\nOptimizer Performance Comparison:\")\n",
    "for opt_name, metrics in results.items():\n",
    "    print(f\"{opt_name}: {metrics}\")\n",
    "\n",
    "# Example Plot (Using the best optimizer's predictions)\n",
    "best_optimizer = min(results, key=lambda k: results[k][\"RMSE (scaled)\"])\n",
    "print(f\"\\nBest optimizer based on RMSE: {best_optimizer}\")\n",
    "\n",
    "# Re-train and visualize using the best optimizer\n",
    "model.compile(optimizer=best_optimizer, loss='mse')\n",
    "model.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val_seq, y_val_seq),\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n",
    "final_predictions_scaled = model.predict(X_test_seq)\n",
    "final_predictions = target_scaler.inverse_transform(final_predictions_scaled.reshape(-1, 1)).flatten()\n",
    "y_test_original = target_scaler.inverse_transform(y_test_seq.reshape(-1, 1)).flatten()\n",
    "\n",
    "plt.plot(y_test_original, label='Actual')\n",
    "plt.plot(final_predictions, label='Predicted')\n",
    "plt.legend()\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Energy Consumption')\n",
    "plt.title('Actual vs Predicted Energy Consumption')\n",
    "plt.savefig('/home/u894059/.local/plot_best_optimizer.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.2.2 LSTM Optimiser - Mid Term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of optimizers to test\n",
    "optimizers = {\n",
    "    \"adam\": Adam,\n",
    "    \"sgd\": SGD,\n",
    "    \"rmsprop\": RMSprop,\n",
    "    \"adagrad\": Adagrad,\n",
    "    \"adamax\": Adamax,\n",
    "    \"nadam\": Nadam\n",
    "}\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Load dataset\n",
    "mtf = pd.read_csv('/home/u894059/.local/dataset/mid_term.csv')\n",
    "\n",
    "# Select the features and target variable\n",
    "features = mtf.drop(columns=['total_weekly_energy_consumption'])\n",
    "target = mtf['total_weekly_energy_consumption'].values.reshape(-1, 1)  # reshape for scaling\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(features, target, test_size=0.2, shuffle=False)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.125, shuffle=False)\n",
    "\n",
    "# Initialize scalers for features and target\n",
    "feature_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "target_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Fit the scaler on the training data and transform\n",
    "X_train_scaled = feature_scaler.fit_transform(X_train)\n",
    "X_val_scaled = feature_scaler.transform(X_val)  # Use transform, not fit_transform\n",
    "X_test_scaled = feature_scaler.transform(X_test)  # Use transform, not fit_transform\n",
    "\n",
    "# Scale the target variable using the target scaler\n",
    "y_train_scaled = target_scaler.fit_transform(y_train).flatten()\n",
    "y_val_scaled = target_scaler.transform(y_val).flatten()  # Use transform, not fit_transform\n",
    "y_test_scaled = target_scaler.transform(y_test).flatten()  # Use transform, not fit_transform\n",
    "\n",
    "# Convert the scaled features to sequences (after scaling)\n",
    "def create_sequences(data, target, sequence_length=5):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        sequences.append(data[i:i+sequence_length])\n",
    "        targets.append(target[i+sequence_length])\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "# Create sequences\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled, sequence_length=5)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val_scaled, y_val_scaled, sequence_length=5)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_scaled, sequence_length=5)\n",
    "\n",
    "# Iterate over each optimizer\n",
    "for opt_name in optimizers:\n",
    "    optimizer = optimizers[opt_name]\n",
    "    print(f\"Using optimizer: {opt_name}\")\n",
    "    \n",
    "    # Define the LSTM model\n",
    "    model = Sequential([\n",
    "        keras.Input(shape=(X_train_seq.shape[1], X_train_seq.shape[2])),\n",
    "        LSTM(126, return_sequences=True),\n",
    "        Dropout(0.4449093799784903),  \n",
    "        LSTM(126, return_sequences=False),  \n",
    "        Dropout(0.4449093799784903),  \n",
    "        Dense(1)  \n",
    "    ])\n",
    " \n",
    "    # Compile the model with the current optimizer\n",
    "    model.compile(optimizer=opt_name, loss='mse')\n",
    "    \n",
    "    # Define callbacks\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n",
    "    \n",
    "    # Train the model using the sequences\n",
    "    history = model.fit(\n",
    "        X_train_seq, y_train_seq,  \n",
    "        epochs=50, \n",
    "        batch_size=128, \n",
    "        validation_data=(X_val_seq, y_val_seq),  \n",
    "        callbacks=[early_stopping, reduce_lr],\n",
    "        verbose=1  # Suppress detailed logs for readability\n",
    "    )\n",
    "    \n",
    "    # Evaluate the model on the test set\n",
    "    test_loss = model.evaluate(X_test_seq, y_test_seq, verbose=1)  # MSE on test data\n",
    "    \n",
    "    # Get predictions and calculate metrics\n",
    "    predictions_scaled = model.predict(X_test_seq)\n",
    "    mse_scaled = mean_squared_error(y_test_seq, predictions_scaled)\n",
    "    rmse_scaled = np.sqrt(mse_scaled)\n",
    "    mae_scaled = mean_absolute_error(y_test_seq, predictions_scaled)\n",
    "    \n",
    "    # Store results\n",
    "    results[opt_name] = {\n",
    "        \"Test Loss (MSE)\": test_loss,\n",
    "        \"RMSE (scaled)\": rmse_scaled,\n",
    "        \"MAE (scaled)\": mae_scaled\n",
    "    }\n",
    "\n",
    "# Display results for comparison\n",
    "print(\"\\nOptimizer Performance Comparison:\")\n",
    "for opt_name, metrics in results.items():\n",
    "    print(f\"{opt_name}: {metrics}\")\n",
    "\n",
    "# Example Plot (Using the best optimizer's predictions)\n",
    "best_optimizer = min(results, key=lambda k: results[k][\"RMSE (scaled)\"])\n",
    "print(f\"\\nBest optimizer based on RMSE: {best_optimizer}\")\n",
    "\n",
    "# Re-train and visualize using the best optimizer\n",
    "model.compile(optimizer=best_optimizer, loss='mse')\n",
    "model.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val_seq, y_val_seq),\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n",
    "final_predictions_scaled = model.predict(X_test_seq)\n",
    "final_predictions = target_scaler.inverse_transform(final_predictions_scaled.reshape(-1, 1)).flatten()\n",
    "y_test_original = target_scaler.inverse_transform(y_test_seq.reshape(-1, 1)).flatten()\n",
    "\n",
    "plt.plot(y_test_original, label='Actual')\n",
    "plt.plot(final_predictions, label='Predicted')\n",
    "plt.legend()\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Energy Consumption')\n",
    "plt.title('Actual vs Predicted Energy Consumption')\n",
    "plt.savefig('/home/u894059/.local/plot_best_optimizer.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.3.1 Transformer Hyperparameters - Short Term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "stf = pd.read_csv('/home/u894059/.local/dataset/short_term.csv')\n",
    "\n",
    "# Select the features and target variable\n",
    "features = stf.drop(columns=['total_daily_energy_consumption'])\n",
    "target = stf['total_daily_energy_consumption'].values.reshape(-1, 1)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(features, target, test_size=0.2, shuffle=False)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.125, shuffle=False)\n",
    "\n",
    "# Initialize scalers for features and target\n",
    "feature_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "target_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Fit the scaler on the training data and transform\n",
    "X_train_scaled = feature_scaler.fit_transform(X_train)\n",
    "X_val_scaled = feature_scaler.transform(X_val)\n",
    "X_test_scaled = feature_scaler.transform(X_test)\n",
    "\n",
    "# Scale the target variable using the target scaler\n",
    "y_train_scaled = target_scaler.fit_transform(y_train).flatten()\n",
    "y_val_scaled = target_scaler.transform(y_val).flatten()\n",
    "y_test_scaled = target_scaler.transform(y_test).flatten()\n",
    "\n",
    "# Create sequences\n",
    "def create_sequences(data, target, sequence_length=5):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        sequences.append(data[i:i+sequence_length])\n",
    "        targets.append(target[i+sequence_length])\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "# Create sequences\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled, sequence_length=5)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val_scaled, y_val_scaled, sequence_length=5)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_scaled, sequence_length=5)\n",
    "\n",
    "# Positional Encoding Function\n",
    "def get_positional_encoding(seq_len, d_model):\n",
    "    pos = np.arange(seq_len)[:, np.newaxis]\n",
    "    i = np.arange(d_model)[np.newaxis, :]\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    angle_rads = pos * angle_rates\n",
    "    pos_encoding = np.concatenate([np.sin(angle_rads[:, 0::2]), np.cos(angle_rads[:, 1::2])], axis=-1)\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "# Define Transformer Encoder block with Positional Encoding\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout_rate, seq_len, d_model):\n",
    "    pos_encoding = get_positional_encoding(seq_len, d_model)\n",
    "    inputs += pos_encoding\n",
    "    \n",
    "    attention = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout_rate\n",
    "    )(inputs, inputs)\n",
    "    \n",
    "    attention = layers.LayerNormalization(epsilon=1e-6)(attention + inputs)\n",
    "    \n",
    "    ff = layers.Dense(ff_dim, activation='relu')(attention)\n",
    "    ff = layers.Dense(inputs.shape[-1])(ff)\n",
    "    \n",
    "    return layers.LayerNormalization(epsilon=1e-6)(ff + attention)\n",
    "\n",
    "# Define Transformer Decoder block\n",
    "def transformer_decoder(inputs, encoder_outputs, head_size, num_heads, ff_dim, dropout_rate):\n",
    "    attention_1 = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout_rate\n",
    "    )(inputs, inputs)\n",
    "    \n",
    "    attention_1 = layers.LayerNormalization(epsilon=1e-6)(attention_1 + inputs)\n",
    "    \n",
    "    attention_2 = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout_rate\n",
    "    )(attention_1, encoder_outputs)\n",
    "    \n",
    "    attention_2 = layers.LayerNormalization(epsilon=1e-6)(attention_2 + attention_1)\n",
    "    \n",
    "    ff = layers.Dense(ff_dim, activation='relu')(attention_2)\n",
    "    ff = layers.Dense(inputs.shape[-1])(ff)\n",
    "    \n",
    "    return layers.LayerNormalization(epsilon=1e-6)(ff + attention_2)\n",
    "\n",
    "# Define the function to build and compile the model (for use with Optuna)\n",
    "def build_model(trial):\n",
    "    # Sample hyperparameters from Optuna trial\n",
    "    head_size = trial.suggest_int('head_size', 16, 64)\n",
    "    num_heads = trial.suggest_int('num_heads', 2, 8)\n",
    "    ff_dim = trial.suggest_int('ff_dim', 32, 128)\n",
    "    num_blocks = trial.suggest_int('num_blocks', 1, 4)\n",
    "    dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.5)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 128)\n",
    "    \n",
    "    encoder_inputs = layers.Input(shape=(X_train_seq.shape[1], X_train_seq.shape[2]))  # (time_steps, features)\n",
    "    encoder_outputs = transformer_encoder(encoder_inputs, head_size, num_heads, ff_dim, dropout_rate,\n",
    "                                      seq_len=X_train_seq.shape[1], d_model=X_train_seq.shape[2])\n",
    "\n",
    "    # Decoder also uses the same sequence data for simplicity\n",
    "    decoder_inputs = layers.Input(shape=(X_train_seq.shape[1], X_train_seq.shape[2]))  # (time_steps, features)\n",
    "    decoder_outputs = transformer_decoder(decoder_inputs, encoder_outputs, head_size, num_heads, ff_dim, dropout_rate)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D()(encoder_outputs)\n",
    "    outputs = layers.Dense(1)(x)\n",
    "\n",
    "    model = keras.Model(inputs=encoder_inputs, outputs=outputs)\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Modify the `objective` function to use the correct data structure\n",
    "def objective(trial):\n",
    "    model = build_model(trial)\n",
    "\n",
    "    # Define callbacks\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train_seq, y_train_seq, epochs=30, batch_size=128,\n",
    "          validation_data=(X_val_seq, y_val_seq),\n",
    "          callbacks=[early_stopping, reduce_lr], verbose=1)\n",
    "\n",
    "    # Get predictions on the validation set\n",
    "    y_pred_scaled = model.predict([X_val_seq])\n",
    "\n",
    "    # Calculate metrics on the scaled values\n",
    "    mse_scaled = mean_squared_error(y_val_seq, y_pred_scaled)\n",
    "    rmse_scaled = np.sqrt(mse_scaled)\n",
    "\n",
    "    return rmse_scaled\n",
    "\n",
    "# Create an Optuna study\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "# Get the best trial and print the best hyperparameters\n",
    "print(f\"Best trial: {study.best_trial.number}\")\n",
    "print(f\"Best hyperparameters: {study.best_trial.params}\")\n",
    "\n",
    "# Build the model using the best hyperparameters\n",
    "best_model = build_model(study.best_trial)\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', patience=5, restore_best_weights=True\n",
    ")\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n",
    "\n",
    "# Retrain the model with the best hyperparameters\n",
    "best_model.fit(X_train_seq, y_train_seq, epochs=50, batch_size=128,\n",
    "               validation_data=(X_val_seq, y_val_seq),\n",
    "               callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "# Get predictions on the test set\n",
    "y_pred_scaled = best_model.predict([X_test_seq])\n",
    "\n",
    "# Calculate metrics on the scaled values\n",
    "mse_scaled = mean_squared_error(y_test_seq, y_pred_scaled)\n",
    "rmse_scaled = np.sqrt(mse_scaled)\n",
    "mae_scaled = mean_absolute_error(y_test_seq, y_pred_scaled)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE) on Scaled Data: {mse_scaled}\")\n",
    "print(f\"Root Mean Squared Error (RMSE) on Scaled Data: {rmse_scaled}\")\n",
    "print(f\"Mean Absolute Error (MAE) on Scaled Data: {mae_scaled}\")\n",
    "\n",
    "# Reverse the scaling for predictions and actual values\n",
    "y_pred = target_scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_original = target_scaler.inverse_transform(y_test_seq.reshape(-1, 1))\n",
    "\n",
    "# Plot the actual vs predicted values\n",
    "plt.plot(y_test_original, label='Actual')\n",
    "plt.plot(y_pred, label='Predicted')\n",
    "plt.legend()\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Energy Consumption')\n",
    "plt.title('Actual vs Predicted Energy Consumption')\n",
    "plt.savefig('/home/u894059/.local/plot1.png')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.3.2 Transformer Hyperparameters - Mid Term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "mtf = pd.read_csv('/home/u894059/.local/dataset/mid_term.csv')\n",
    "\n",
    "# Select the features and target variable\n",
    "features = mtf.drop(columns=['total_weekly_energy_consumption'])\n",
    "target = mtf['total_weekly_energy_consumption'].values.reshape(-1, 1)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(features, target, test_size=0.2, shuffle=False)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.125, shuffle=False)\n",
    "\n",
    "# Initialize scalers for features and target\n",
    "feature_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "target_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Fit the scaler on the training data and transform\n",
    "X_train_scaled = feature_scaler.fit_transform(X_train)\n",
    "X_val_scaled = feature_scaler.transform(X_val)\n",
    "X_test_scaled = feature_scaler.transform(X_test)\n",
    "\n",
    "# Scale the target variable using the target scaler\n",
    "y_train_scaled = target_scaler.fit_transform(y_train).flatten()\n",
    "y_val_scaled = target_scaler.transform(y_val).flatten()\n",
    "y_test_scaled = target_scaler.transform(y_test).flatten()\n",
    "\n",
    "# Create sequences\n",
    "def create_sequences(data, target, sequence_length=5):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        sequences.append(data[i:i+sequence_length])\n",
    "        targets.append(target[i+sequence_length])\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "# Create sequences\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled, sequence_length=5)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val_scaled, y_val_scaled, sequence_length=5)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_scaled, sequence_length=5)\n",
    "\n",
    "# Positional Encoding Function\n",
    "def get_positional_encoding(seq_len, d_model):\n",
    "    pos = np.arange(seq_len)[:, np.newaxis]\n",
    "    i = np.arange(d_model)[np.newaxis, :]\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    angle_rads = pos * angle_rates\n",
    "    pos_encoding = np.concatenate([np.sin(angle_rads[:, 0::2]), np.cos(angle_rads[:, 1::2])], axis=-1)\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "# Define Transformer Encoder block with Positional Encoding\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout_rate, seq_len, d_model):\n",
    "    pos_encoding = get_positional_encoding(seq_len, d_model)\n",
    "    inputs += pos_encoding\n",
    "    \n",
    "    attention = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout_rate\n",
    "    )(inputs, inputs)\n",
    "    \n",
    "    attention = layers.LayerNormalization(epsilon=1e-6)(attention + inputs)\n",
    "    \n",
    "    ff = layers.Dense(ff_dim, activation='relu')(attention)\n",
    "    ff = layers.Dense(inputs.shape[-1])(ff)\n",
    "    \n",
    "    return layers.LayerNormalization(epsilon=1e-6)(ff + attention)\n",
    "\n",
    "# Define Transformer Decoder block\n",
    "def transformer_decoder(inputs, encoder_outputs, head_size, num_heads, ff_dim, dropout_rate):\n",
    "    attention_1 = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout_rate\n",
    "    )(inputs, inputs)\n",
    "    \n",
    "    attention_1 = layers.LayerNormalization(epsilon=1e-6)(attention_1 + inputs)\n",
    "    \n",
    "    attention_2 = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout_rate\n",
    "    )(attention_1, encoder_outputs)\n",
    "    \n",
    "    attention_2 = layers.LayerNormalization(epsilon=1e-6)(attention_2 + attention_1)\n",
    "    \n",
    "    ff = layers.Dense(ff_dim, activation='relu')(attention_2)\n",
    "    ff = layers.Dense(inputs.shape[-1])(ff)\n",
    "    \n",
    "    return layers.LayerNormalization(epsilon=1e-6)(ff + attention_2)\n",
    "\n",
    "# Define the function to build and compile the model (for use with Optuna)\n",
    "def build_model(trial):\n",
    "    # Sample hyperparameters from Optuna trial\n",
    "    head_size = trial.suggest_int('head_size', 16, 64)\n",
    "    num_heads = trial.suggest_int('num_heads', 2, 8)\n",
    "    ff_dim = trial.suggest_int('ff_dim', 32, 128)\n",
    "    num_blocks = trial.suggest_int('num_blocks', 1, 4)\n",
    "    dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.5)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 128)\n",
    "\n",
    "    encoder_inputs = layers.Input(shape=(X_train_seq.shape[1], X_train_seq.shape[2]))  # (time_steps, features)\n",
    "    encoder_outputs = transformer_encoder(encoder_inputs, head_size, num_heads, ff_dim, dropout_rate,\n",
    "                                      seq_len=X_train_seq.shape[1], d_model=X_train_seq.shape[2])\n",
    "\n",
    "    # Decoder also uses the same sequence data for simplicity\n",
    "    decoder_inputs = layers.Input(shape=(X_train_seq.shape[1], X_train_seq.shape[2]))  # (time_steps, features)\n",
    "    decoder_outputs = transformer_decoder(decoder_inputs, encoder_outputs, head_size, num_heads, ff_dim, dropout_rate)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D()(encoder_outputs)\n",
    "    outputs = layers.Dense(1)(x)\n",
    "\n",
    "    model = keras.Model(inputs=encoder_inputs, outputs=outputs)\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Modify the `objective` function to use the correct data structure\n",
    "def objective(trial):\n",
    "    model = build_model(trial)\n",
    "\n",
    "    # Define callbacks\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train_seq, y_train_seq, epochs=30, batch_size=128,\n",
    "          validation_data=(X_val_seq, y_val_seq),\n",
    "          callbacks=[early_stopping, reduce_lr], verbose=1)\n",
    "\n",
    "    # Get predictions on the validation set\n",
    "    y_pred_scaled = model.predict([X_val_seq])\n",
    "\n",
    "    # Calculate metrics on the scaled values\n",
    "    mse_scaled = mean_squared_error(y_val_seq, y_pred_scaled)\n",
    "    rmse_scaled = np.sqrt(mse_scaled)\n",
    "\n",
    "    return rmse_scaled\n",
    "\n",
    "# Create an Optuna study\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "# Get the best trial and print the best hyperparameters\n",
    "print(f\"Best trial: {study.best_trial.number}\")\n",
    "print(f\"Best hyperparameters: {study.best_trial.params}\")\n",
    "\n",
    "# Build the model using the best hyperparameters\n",
    "best_model = build_model(study.best_trial)\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', patience=5, restore_best_weights=True\n",
    ")\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n",
    "\n",
    "# Retrain the model with the best hyperparameters\n",
    "best_model.fit(X_train_seq, y_train_seq, epochs=50, batch_size=128,\n",
    "               validation_data=(X_val_seq, y_val_seq),\n",
    "               callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "# Get predictions on the test set\n",
    "y_pred_scaled = best_model.predict([X_test_seq])\n",
    "\n",
    "# Calculate metrics on the scaled values\n",
    "mse_scaled = mean_squared_error(y_test_seq, y_pred_scaled)\n",
    "rmse_scaled = np.sqrt(mse_scaled)\n",
    "mae_scaled = mean_absolute_error(y_test_seq, y_pred_scaled)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE) on Scaled Data: {mse_scaled}\")\n",
    "print(f\"Root Mean Squared Error (RMSE) on Scaled Data: {rmse_scaled}\")\n",
    "print(f\"Mean Absolute Error (MAE) on Scaled Data: {mae_scaled}\")\n",
    "\n",
    "# Reverse the scaling for predictions and actual values\n",
    "y_pred = target_scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_original = target_scaler.inverse_transform(y_test_seq.reshape(-1, 1))\n",
    "\n",
    "# Plot the actual vs predicted values\n",
    "plt.plot(y_test_original, label='Actual')\n",
    "plt.plot(y_pred, label='Predicted')\n",
    "plt.legend()\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Energy Consumption')\n",
    "plt.title('Actual vs Predicted Energy Consumption')\n",
    "plt.savefig('/home/u894059/.local/plot1.png')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.4.1 Transformer Optimiser - Short Term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of optimizers to test\n",
    "optimizers = {\n",
    "    \"adam\": Adam,\n",
    "    \"sgd\": SGD,\n",
    "    \"rmsprop\": RMSprop,\n",
    "    \"adagrad\": Adagrad,\n",
    "    \"adamax\": Adamax,\n",
    "    \"nadam\": Nadam\n",
    "}\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Load the dataset\n",
    "stf = pd.read_csv('/home/u894059/.local/dataset/short_term.csv')\n",
    "\n",
    "# Select the features and target variable\n",
    "features = stf.drop(columns=['total_daily_energy_consumption'])\n",
    "target = stf['total_daily_energy_consumption'].values.reshape(-1, 1)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(features, target, test_size=0.2, shuffle=False)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.125, shuffle=False)\n",
    "\n",
    "# Initialize scalers for features and target\n",
    "feature_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "target_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Fit the scaler on the training data and transform\n",
    "X_train_scaled = feature_scaler.fit_transform(X_train)\n",
    "X_val_scaled = feature_scaler.transform(X_val)\n",
    "X_test_scaled = feature_scaler.transform(X_test)\n",
    "\n",
    "# Scale the target variable using the target scaler\n",
    "y_train_scaled = target_scaler.fit_transform(y_train).flatten()\n",
    "y_val_scaled = target_scaler.transform(y_val).flatten()\n",
    "y_test_scaled = target_scaler.transform(y_test).flatten()\n",
    "\n",
    "# Create sequences\n",
    "def create_sequences(data, target, sequence_length=5):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        sequences.append(data[i:i+sequence_length])\n",
    "        targets.append(target[i+sequence_length])\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "# Create sequences\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled, sequence_length=5)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val_scaled, y_val_scaled, sequence_length=5)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_scaled, sequence_length=5)\n",
    "\n",
    "# Positional Encoding Function\n",
    "def get_positional_encoding(seq_len, d_model):\n",
    "    pos = np.arange(seq_len)[:, np.newaxis]\n",
    "    i = np.arange(d_model)[np.newaxis, :]\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    angle_rads = pos * angle_rates\n",
    "    pos_encoding = np.concatenate([np.sin(angle_rads[:, 0::2]), np.cos(angle_rads[:, 1::2])], axis=-1)\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "# Define Transformer Encoder block with Positional Encoding\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout_rate, seq_len, d_model):\n",
    "    pos_encoding = get_positional_encoding(seq_len, d_model)\n",
    "    inputs += pos_encoding\n",
    "    \n",
    "    attention = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout_rate\n",
    "    )(inputs, inputs)\n",
    "    \n",
    "    attention = layers.LayerNormalization(epsilon=1e-6)(attention + inputs)\n",
    "    \n",
    "    ff = layers.Dense(ff_dim, activation='relu')(attention)\n",
    "    ff = layers.Dense(inputs.shape[-1])(ff)\n",
    "    \n",
    "    return layers.LayerNormalization(epsilon=1e-6)(ff + attention)\n",
    "\n",
    "# Define Transformer Decoder block\n",
    "def transformer_decoder(inputs, encoder_outputs, head_size, num_heads, ff_dim, dropout_rate):\n",
    "    attention_1 = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout_rate\n",
    "    )(inputs, inputs)\n",
    "    \n",
    "    attention_1 = layers.LayerNormalization(epsilon=1e-6)(attention_1 + inputs)\n",
    "    \n",
    "    attention_2 = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout_rate\n",
    "    )(attention_1, encoder_outputs)\n",
    "    \n",
    "    attention_2 = layers.LayerNormalization(epsilon=1e-6)(attention_2 + attention_1)\n",
    "    \n",
    "    ff = layers.Dense(ff_dim, activation='relu')(attention_2)\n",
    "    ff = layers.Dense(inputs.shape[-1])(ff)\n",
    "    \n",
    "    return layers.LayerNormalization(epsilon=1e-6)(ff + attention_2)\n",
    "\n",
    "# Define the Input Layer for Encoder (3D input: samples, time_steps, features)\n",
    "encoder_inputs = layers.Input(shape=(X_train_seq.shape[1], X_train_seq.shape[2]))  # (time_steps, features)\n",
    "encoder_outputs = transformer_encoder(encoder_inputs, head_size=28, num_heads=6, ff_dim=79, dropout_rate=0.3397155165626943,\n",
    "                                      seq_len=X_train_seq.shape[1], d_model=X_train_seq.shape[2])\n",
    "\n",
    "# Define the Input Layer for Decoder (3D input: samples, time_steps, features)\n",
    "decoder_inputs = layers.Input(shape=(X_train_seq.shape[1], X_train_seq.shape[2]))  # (time_steps, features)\n",
    "decoder_outputs = transformer_decoder(decoder_inputs, encoder_outputs, head_size=28, num_heads=6, ff_dim=79, dropout_rate=0.3397155165626943)\n",
    "\n",
    "# Global Average Pooling to reduce the time dimension\n",
    "x = layers.GlobalAveragePooling1D()(decoder_outputs)\n",
    "\n",
    "# Output layer (for regression, a single neuron)\n",
    "outputs = layers.Dense(1)(x)\n",
    "\n",
    "# Iterate over each optimizer\n",
    "for opt_name in optimizers:\n",
    "    optimizer = optimizers[opt_name]\n",
    "    print(f\"Using optimizer: {opt_name}\")\n",
    "\n",
    "    # Create the model\n",
    "    model = keras.Model(inputs=[encoder_inputs, decoder_inputs], outputs=outputs)\n",
    "\n",
    "    model.compile(optimizer=opt_name, loss='mse', metrics=['mae'])\n",
    "\n",
    "    # Define callbacks\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss', patience=5, restore_best_weights=True\n",
    "    )\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        [X_train_seq, X_train_seq], y_train_seq,\n",
    "        epochs=50,\n",
    "        batch_size=67,\n",
    "        validation_data=([X_val_seq, X_val_seq], y_val_seq),\n",
    "        callbacks=[early_stopping, reduce_lr],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluate the model on the test set\n",
    "    test_loss = model.evaluate([X_test_seq, X_test_seq], y_test_seq, verbose=1)  # MSE on test data\n",
    "    \n",
    "    # Get predictions on the test set\n",
    "    y_pred_scaled = model.predict([X_test_seq, X_test_seq])\n",
    "\n",
    "    # Calculate metrics on the scaled values\n",
    "    mse_scaled = mean_squared_error(y_test_seq, y_pred_scaled)\n",
    "    rmse_scaled = np.sqrt(mse_scaled)\n",
    "    mae_scaled = mean_absolute_error(y_test_seq, y_pred_scaled)\n",
    "\n",
    "    # Store results\n",
    "    results[opt_name] = {\n",
    "        \"Test Loss (MSE)\": test_loss,\n",
    "        \"RMSE (scaled)\": rmse_scaled,\n",
    "        \"MAE (scaled)\": mae_scaled\n",
    "    }\n",
    "\n",
    "# Display results for comparison\n",
    "print(\"\\nOptimizer Performance Comparison:\")\n",
    "for opt_name, metrics in results.items():\n",
    "    print(f\"{opt_name}: {metrics}\")\n",
    "\n",
    "# Example Plot (Using the best optimizer's predictions)\n",
    "best_optimizer = min(results, key=lambda k: results[k][\"RMSE (scaled)\"])\n",
    "print(f\"\\nBest optimizer based on RMSE: {best_optimizer}\")\n",
    "\n",
    "# Re-train and visualize using the best optimizer\n",
    "model.compile(optimizer=best_optimizer, loss='mse')\n",
    "model.fit(\n",
    "    [X_train_seq, X_train_seq], y_train_seq,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=([X_val_seq, X_val_seq], y_val_seq),\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n",
    "\n",
    "final_predictions_scaled = model.predict([X_test_seq, X_test_seq])\n",
    "final_predictions = target_scaler.inverse_transform(final_predictions_scaled.reshape(-1, 1)).flatten()\n",
    "y_test_original = target_scaler.inverse_transform(y_test_seq.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Plot the actual vs predicted values\n",
    "plt.plot(y_test_original, label='Actual')\n",
    "plt.plot(final_predictions, label='Predicted')\n",
    "plt.legend()\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Energy Consumption')\n",
    "plt.title('Actual vs Predicted Energy Consumption')\n",
    "plt.savefig('/home/u894059/.local/Trans_STG_optimiser.png')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.4.2 Transformer Optimiser - Mid Term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of optimizers to test\n",
    "optimizers = {\n",
    "    \"adam\": Adam,\n",
    "    \"sgd\": SGD,\n",
    "    \"rmsprop\": RMSprop,\n",
    "    \"adagrad\": Adagrad,\n",
    "    \"adamax\": Adamax,\n",
    "    \"nadam\": Nadam\n",
    "}\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Load the dataset\n",
    "mtf = pd.read_csv('/home/u894059/.local/dataset/mid_term.csv')\n",
    "\n",
    "# Select the features and target variable\n",
    "features = mtf.drop(columns=['total_weekly_energy_consumption'])\n",
    "target = mtf['total_weekly_energy_consumption'].values.reshape(-1, 1)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(features, target, test_size=0.2, shuffle=False)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.125, shuffle=False)\n",
    "\n",
    "# Initialize scalers for features and target\n",
    "feature_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "target_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Fit the scaler on the training data and transform\n",
    "X_train_scaled = feature_scaler.fit_transform(X_train)\n",
    "X_val_scaled = feature_scaler.transform(X_val)\n",
    "X_test_scaled = feature_scaler.transform(X_test)\n",
    "\n",
    "# Scale the target variable using the target scaler\n",
    "y_train_scaled = target_scaler.fit_transform(y_train).flatten()\n",
    "y_val_scaled = target_scaler.transform(y_val).flatten()\n",
    "y_test_scaled = target_scaler.transform(y_test).flatten()\n",
    "\n",
    "# Create sequences\n",
    "def create_sequences(data, target, sequence_length=5):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        sequences.append(data[i:i+sequence_length])\n",
    "        targets.append(target[i+sequence_length])\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "# Create sequences\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled, sequence_length=5)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val_scaled, y_val_scaled, sequence_length=5)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_scaled, sequence_length=5)\n",
    "\n",
    "# Positional Encoding Function\n",
    "def get_positional_encoding(seq_len, d_model):\n",
    "    pos = np.arange(seq_len)[:, np.newaxis]\n",
    "    i = np.arange(d_model)[np.newaxis, :]\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    angle_rads = pos * angle_rates\n",
    "    pos_encoding = np.concatenate([np.sin(angle_rads[:, 0::2]), np.cos(angle_rads[:, 1::2])], axis=-1)\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "# Define Transformer Encoder block with Positional Encoding\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout_rate, seq_len, d_model):\n",
    "    pos_encoding = get_positional_encoding(seq_len, d_model)\n",
    "    inputs += pos_encoding\n",
    "    \n",
    "    attention = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout_rate\n",
    "    )(inputs, inputs)\n",
    "    \n",
    "    attention = layers.LayerNormalization(epsilon=1e-6)(attention + inputs)\n",
    "    \n",
    "    ff = layers.Dense(ff_dim, activation='relu')(attention)\n",
    "    ff = layers.Dense(inputs.shape[-1])(ff)\n",
    "    \n",
    "    return layers.LayerNormalization(epsilon=1e-6)(ff + attention)\n",
    "\n",
    "# Define Transformer Decoder block\n",
    "def transformer_decoder(inputs, encoder_outputs, head_size, num_heads, ff_dim, dropout_rate):\n",
    "    attention_1 = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout_rate\n",
    "    )(inputs, inputs)\n",
    "    \n",
    "    attention_1 = layers.LayerNormalization(epsilon=1e-6)(attention_1 + inputs)\n",
    "    \n",
    "    attention_2 = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout_rate\n",
    "    )(attention_1, encoder_outputs)\n",
    "    \n",
    "    attention_2 = layers.LayerNormalization(epsilon=1e-6)(attention_2 + attention_1)\n",
    "    \n",
    "    ff = layers.Dense(ff_dim, activation='relu')(attention_2)\n",
    "    ff = layers.Dense(inputs.shape[-1])(ff)\n",
    "    \n",
    "    return layers.LayerNormalization(epsilon=1e-6)(ff + attention_2)\n",
    "\n",
    "# Define the Input Layer for Encoder (3D input: samples, time_steps, features)\n",
    "encoder_inputs = layers.Input(shape=(X_train_seq.shape[1], X_train_seq.shape[2]))  # (time_steps, features)\n",
    "encoder_outputs = transformer_encoder(encoder_inputs, head_size=37, num_heads=6, ff_dim=98, dropout_rate=0.11543766121575369,\n",
    "                                      seq_len=X_train_seq.shape[1], d_model=X_train_seq.shape[2])\n",
    "\n",
    "# Define the Input Layer for Decoder (3D input: samples, time_steps, features)\n",
    "decoder_inputs = layers.Input(shape=(X_train_seq.shape[1], X_train_seq.shape[2]))  # (time_steps, features)\n",
    "decoder_outputs = transformer_decoder(decoder_inputs, encoder_outputs, head_size=28, num_heads=6, ff_dim=79, dropout_rate=0.3397155165626943)\n",
    "\n",
    "# Global Average Pooling to reduce the time dimension\n",
    "x = layers.GlobalAveragePooling1D()(decoder_outputs)\n",
    "\n",
    "# Output layer (for regression, a single neuron)\n",
    "outputs = layers.Dense(1)(x)\n",
    "\n",
    "# Iterate over each optimizer\n",
    "for opt_name in optimizers:\n",
    "    optimizer = optimizers[opt_name]\n",
    "    print(f\"Using optimizer: {opt_name}\")\n",
    "\n",
    "    # Create the model\n",
    "    model = keras.Model(inputs=[encoder_inputs, decoder_inputs], outputs=outputs)\n",
    "\n",
    "    model.compile(optimizer=opt_name, loss='mse', metrics=['mae'])\n",
    "\n",
    "    # Define callbacks\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss', patience=5, restore_best_weights=True\n",
    "    )\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        [X_train_seq, X_train_seq], y_train_seq,\n",
    "        epochs=50,\n",
    "        batch_size=72,\n",
    "        validation_data=([X_val_seq, X_val_seq], y_val_seq),\n",
    "        callbacks=[early_stopping, reduce_lr],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluate the model on the test set\n",
    "    test_loss = model.evaluate([X_test_seq, X_test_seq], y_test_seq, verbose=1)  # MSE on test data\n",
    "    \n",
    "    # Get predictions on the test set\n",
    "    y_pred_scaled = model.predict([X_test_seq, X_test_seq])\n",
    "\n",
    "    # Calculate metrics on the scaled values\n",
    "    mse_scaled = mean_squared_error(y_test_seq, y_pred_scaled)\n",
    "    rmse_scaled = np.sqrt(mse_scaled)\n",
    "    mae_scaled = mean_absolute_error(y_test_seq, y_pred_scaled)\n",
    "\n",
    "    # Store results\n",
    "    results[opt_name] = {\n",
    "        \"Test Loss (MSE)\": test_loss,\n",
    "        \"RMSE (scaled)\": rmse_scaled,\n",
    "        \"MAE (scaled)\": mae_scaled\n",
    "    }\n",
    "\n",
    "# Display results for comparison\n",
    "print(\"\\nOptimizer Performance Comparison:\")\n",
    "for opt_name, metrics in results.items():\n",
    "    print(f\"{opt_name}: {metrics}\")\n",
    "\n",
    "# Example Plot (Using the best optimizer's predictions)\n",
    "best_optimizer = min(results, key=lambda k: results[k][\"RMSE (scaled)\"])\n",
    "print(f\"\\nBest optimizer based on RMSE: {best_optimizer}\")\n",
    "\n",
    "# Re-train and visualize using the best optimizer\n",
    "model.compile(optimizer=best_optimizer, loss='mse')\n",
    "model.fit(\n",
    "    [X_train_seq, X_train_seq], y_train_seq,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=([X_val_seq, X_val_seq], y_val_seq),\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n",
    "\n",
    "final_predictions_scaled = model.predict([X_test_seq, X_test_seq])\n",
    "final_predictions = target_scaler.inverse_transform(final_predictions_scaled.reshape(-1, 1)).flatten()\n",
    "y_test_original = target_scaler.inverse_transform(y_test_seq.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Plot the actual vs predicted values\n",
    "plt.plot(y_test_original, label='Actual')\n",
    "plt.plot(final_predictions, label='Predicted')\n",
    "plt.legend()\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Energy Consumption')\n",
    "plt.title('Actual vs Predicted Energy Consumption')\n",
    "plt.savefig('/home/u894059/.local/Trans_STG_optimiser.png')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.5.1 Hybrid Model Hyperparameters - Short Term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "stf = pd.read_csv('/home/u894059/.local/dataset/short_term.csv')\n",
    "\n",
    "# Select the features and target variable\n",
    "features = stf.drop(columns=['total_daily_energy_consumption'])\n",
    "target = stf['total_daily_energy_consumption'].values.reshape(-1, 1)  # reshape for scaling\n",
    "\n",
    "# Preprocess the data\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(features, target, test_size=0.2, shuffle=False)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.125, shuffle=False)\n",
    "\n",
    "# Normalize the data\n",
    "feature_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "target_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "X_train_scaled = feature_scaler.fit_transform(X_train)\n",
    "X_val_scaled = feature_scaler.transform(X_val)\n",
    "X_test_scaled = feature_scaler.transform(X_test)\n",
    "\n",
    "y_train_scaled = target_scaler.fit_transform(y_train).flatten()\n",
    "y_val_scaled = target_scaler.transform(y_val).flatten()\n",
    "y_test_scaled = target_scaler.transform(y_test).flatten()\n",
    "\n",
    "# Create sliding window sequences\n",
    "def create_sequences(data, target, sequence_length):\n",
    "    sequences, targets = [], []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        sequences.append(data[i:i + sequence_length])\n",
    "        targets.append(target[i + sequence_length])  # This might be an issue\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "# Experiment with longer sequences\n",
    "sequence_length = 5\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled, sequence_length)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val_scaled, y_val_scaled, sequence_length)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_scaled, sequence_length)\n",
    "\n",
    "# LSTM Block\n",
    "def lstm_block(inputs, lstm_units):\n",
    "    x = layers.LSTM(lstm_units, return_sequences=True, kernel_regularizer='l2')(inputs)\n",
    "    x = layers.Dropout(0.3)(x)  # Increased dropout for regularization\n",
    "    return x\n",
    "\n",
    "# Transformer Encoder\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout_rate, num_blocks):\n",
    "    for _ in range(num_blocks):\n",
    "        # Multi-Head Attention\n",
    "        x = layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads)(inputs, inputs)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(x + inputs)  # Residual Connection\n",
    "        \n",
    "        # Feedforward Block\n",
    "        ff = layers.Conv1D(filters=ff_dim, kernel_size=1, activation='relu')(x)\n",
    "        ff = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(ff)\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(x + ff)  # Residual Connection\n",
    "    \n",
    "    return x\n",
    "\n",
    "# Build the RS-LSTM-Transformer Model\n",
    "def build_model(input_shape, lstm_units, head_size, num_heads, ff_dim, num_blocks, dropout_rate):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # LSTM Block\n",
    "    lstm_out = lstm_block(inputs, lstm_units)\n",
    "    \n",
    "    # Transformer Encoder\n",
    "    transformer_out = transformer_encoder(lstm_out, head_size, num_heads, ff_dim, dropout_rate, num_blocks)\n",
    "    \n",
    "    # Global Average Pooling + Dense Layers\n",
    "    gap = layers.GlobalAveragePooling1D()(transformer_out)\n",
    "    dense = layers.Dense(64, activation='relu')(gap)\n",
    "    outputs = layers.Dense(1)(dense)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# Objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Hyperparameter Search Space\n",
    "    lstm_units = trial.suggest_int('lstm_units', 32, 256)\n",
    "    head_size = trial.suggest_int('head_size', 32, 128)\n",
    "    num_heads = trial.suggest_int('num_heads', 4, 12)\n",
    "    ff_dim = trial.suggest_int('ff_dim', 128, 512)\n",
    "    num_blocks = trial.suggest_int('num_blocks', 1, 4)\n",
    "    dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.5)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 128)\n",
    "    \n",
    "    # Build the model with the suggested hyperparameters\n",
    "    input_shape = (sequence_length, features.shape[1])\n",
    "    model = build_model(input_shape, lstm_units, head_size, num_heads, ff_dim, num_blocks, dropout_rate)\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    \n",
    "    # Callbacks for early stopping and learning rate reduction\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train_seq, y_train_seq,\n",
    "        validation_data=(X_val_seq, y_val_seq),\n",
    "        epochs=30,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=[early_stopping, reduce_lr],\n",
    "        verbose = 1  \n",
    "    )\n",
    "    \n",
    "    # Get predictions on the validation set\n",
    "    y_pred_scaled = model.predict(X_val_seq)\n",
    "    \n",
    "    # Calculate MSE (Mean Squared Error) on the validation set\n",
    "    mse = mean_squared_error(y_val_seq, y_pred_scaled)\n",
    "    return mse  # Return the MSE to Optuna for optimization\n",
    "\n",
    "# Create an Optuna study to minimize the objective (MSE)\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=20)  # Number of trials to run\n",
    "\n",
    "# Print the best hyperparameters found by Optuna\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(study.best_params)\n",
    "\n",
    "input_shape = (sequence_length, features.shape[1])\n",
    "\n",
    "# You can now use the best parameters to train a final model:\n",
    "best_params = study.best_params\n",
    "final_model = build_model(\n",
    "    input_shape, \n",
    "    best_params['lstm_units'],\n",
    "    best_params['head_size'],\n",
    "    best_params['num_heads'],\n",
    "    best_params['ff_dim'],\n",
    "    best_params['num_blocks'],\n",
    "    best_params['dropout_rate']\n",
    ")\n",
    "\n",
    "# Final model compilation and training\n",
    "final_model.compile(optimizer=Adam(learning_rate=best_params['learning_rate']), loss='mse', metrics=['mae'])\n",
    "final_history = final_model.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    validation_data=(X_val_seq, y_val_seq),\n",
    "    epochs=50,\n",
    "    batch_size=best_params['batch_size'],\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose = 1\n",
    ")\n",
    "\n",
    "# Get predictions on the test set\n",
    "y_pred_scaled = final_model.predict(X_test_seq)\n",
    "mse_scaled = mean_squared_error(y_test_seq, y_pred_scaled)\n",
    "rmse_scaled = np.sqrt(mse_scaled)\n",
    "mae_scaled = mean_absolute_error(y_test_seq, y_pred_scaled)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE) on Scaled Data: {mse_scaled}\")\n",
    "print(f\"Root Mean Squared Error (RMSE) on Scaled Data: {rmse_scaled}\")\n",
    "print(f\"Mean Absolute Error (MAE) on Scaled Data: {mae_scaled}\")\n",
    "\n",
    "# Reverse the scaling for predictions and actual values\n",
    "y_pred = target_scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_original = target_scaler.inverse_transform(y_test_seq.reshape(-1, 1))\n",
    "\n",
    "# Plot the actual vs predicted values\n",
    "plt.plot(y_test_original, label='Actual')\n",
    "plt.plot(y_pred, label='Predicted')\n",
    "plt.legend()\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Energy Consumption')\n",
    "plt.title('Actual vs Predicted Energy Consumption')\n",
    "plt.savefig('/home/u894059/.local/plot1.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.5.2 Hybrid Model Hyperparameters - Mid Term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "mtf = pd.read_csv('/home/u894059/.local/dataset/mid_term.csv')\n",
    "\n",
    "# Select the features and target variable\n",
    "features = stf.drop(columns=['total_weekly_energy_consumption'])\n",
    "target = stf['total_weekly_energy_consumption'].values.reshape(-1, 1)  # reshape for scaling\n",
    "\n",
    "# Preprocess the data\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(features, target, test_size=0.2, shuffle=False)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.125, shuffle=False)\n",
    "\n",
    "# Normalize the data\n",
    "feature_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "target_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "X_train_scaled = feature_scaler.fit_transform(X_train)\n",
    "X_val_scaled = feature_scaler.transform(X_val)\n",
    "X_test_scaled = feature_scaler.transform(X_test)\n",
    "\n",
    "y_train_scaled = target_scaler.fit_transform(y_train).flatten()\n",
    "y_val_scaled = target_scaler.transform(y_val).flatten()\n",
    "y_test_scaled = target_scaler.transform(y_test).flatten()\n",
    "\n",
    "# Create sliding window sequences\n",
    "def create_sequences(data, target, sequence_length):\n",
    "    sequences, targets = [], []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        sequences.append(data[i:i + sequence_length])\n",
    "        targets.append(target[i + sequence_length])  # This might be an issue\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "# Experiment with longer sequences\n",
    "sequence_length = 5\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled, sequence_length)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val_scaled, y_val_scaled, sequence_length)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_scaled, sequence_length)\n",
    "\n",
    "# LSTM Block\n",
    "def lstm_block(inputs, lstm_units):\n",
    "    x = layers.LSTM(lstm_units, return_sequences=True, kernel_regularizer='l2')(inputs)\n",
    "    x = layers.Dropout(0.3)(x)  # Increased dropout for regularization\n",
    "    return x\n",
    "\n",
    "# Transformer Encoder\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout_rate, num_blocks):\n",
    "    for _ in range(num_blocks):\n",
    "        # Multi-Head Attention\n",
    "        x = layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads)(inputs, inputs)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(x + inputs)  # Residual Connection\n",
    "        \n",
    "        # Feedforward Block\n",
    "        ff = layers.Conv1D(filters=ff_dim, kernel_size=1, activation='relu')(x)\n",
    "        ff = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(ff)\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(x + ff)  # Residual Connection\n",
    "    \n",
    "    return x\n",
    "\n",
    "# Build the RS-LSTM-Transformer Model\n",
    "def build_model(input_shape, lstm_units, head_size, num_heads, ff_dim, num_blocks, dropout_rate):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # LSTM Block\n",
    "    lstm_out = lstm_block(inputs, lstm_units)\n",
    "    \n",
    "    # Transformer Encoder\n",
    "    transformer_out = transformer_encoder(lstm_out, head_size, num_heads, ff_dim, dropout_rate, num_blocks)\n",
    "    \n",
    "    # Global Average Pooling + Dense Layers\n",
    "    gap = layers.GlobalAveragePooling1D()(transformer_out)\n",
    "    dense = layers.Dense(64, activation='relu')(gap)\n",
    "    outputs = layers.Dense(1)(dense)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# Objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Hyperparameter Search Space\n",
    "    lstm_units = trial.suggest_int('lstm_units', 32, 256)\n",
    "    head_size = trial.suggest_int('head_size', 32, 128)\n",
    "    num_heads = trial.suggest_int('num_heads', 4, 12)\n",
    "    ff_dim = trial.suggest_int('ff_dim', 128, 512)\n",
    "    num_blocks = trial.suggest_int('num_blocks', 1, 4)\n",
    "    dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.5)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 128)\n",
    "    \n",
    "    # Build the model with the suggested hyperparameters\n",
    "    input_shape = (sequence_length, features.shape[1])\n",
    "    model = build_model(input_shape, lstm_units, head_size, num_heads, ff_dim, num_blocks, dropout_rate)\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    \n",
    "    # Callbacks for early stopping and learning rate reduction\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train_seq, y_train_seq,\n",
    "        validation_data=(X_val_seq, y_val_seq),\n",
    "        epochs=30,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=[early_stopping, reduce_lr],\n",
    "        verbose = 1  \n",
    "    )\n",
    "    \n",
    "    # Get predictions on the validation set\n",
    "    y_pred_scaled = model.predict(X_val_seq)\n",
    "    \n",
    "    # Calculate MSE (Mean Squared Error) on the validation set\n",
    "    mse = mean_squared_error(y_val_seq, y_pred_scaled)\n",
    "    return mse  # Return the MSE to Optuna for optimization\n",
    "\n",
    "# Create an Optuna study to minimize the objective (MSE)\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=20)  # Number of trials to run\n",
    "\n",
    "# Print the best hyperparameters found by Optuna\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(study.best_params)\n",
    "\n",
    "input_shape = (sequence_length, features.shape[1])\n",
    "\n",
    "# You can now use the best parameters to train a final model:\n",
    "best_params = study.best_params\n",
    "final_model = build_model(\n",
    "    input_shape, \n",
    "    best_params['lstm_units'],\n",
    "    best_params['head_size'],\n",
    "    best_params['num_heads'],\n",
    "    best_params['ff_dim'],\n",
    "    best_params['num_blocks'],\n",
    "    best_params['dropout_rate']\n",
    ")\n",
    "\n",
    "# Final model compilation and training\n",
    "final_model.compile(optimizer=Adam(learning_rate=best_params['learning_rate']), loss='mse', metrics=['mae'])\n",
    "final_history = final_model.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    validation_data=(X_val_seq, y_val_seq),\n",
    "    epochs=50,\n",
    "    batch_size=best_params['batch_size'],\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose = 1\n",
    ")\n",
    "\n",
    "# Get predictions on the test set\n",
    "y_pred_scaled = final_model.predict(X_test_seq)\n",
    "mse_scaled = mean_squared_error(y_test_seq, y_pred_scaled)\n",
    "rmse_scaled = np.sqrt(mse_scaled)\n",
    "mae_scaled = mean_absolute_error(y_test_seq, y_pred_scaled)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE) on Scaled Data: {mse_scaled}\")\n",
    "print(f\"Root Mean Squared Error (RMSE) on Scaled Data: {rmse_scaled}\")\n",
    "print(f\"Mean Absolute Error (MAE) on Scaled Data: {mae_scaled}\")\n",
    "\n",
    "# Reverse the scaling for predictions and actual values\n",
    "y_pred = target_scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_original = target_scaler.inverse_transform(y_test_seq.reshape(-1, 1))\n",
    "\n",
    "# Plot the actual vs predicted values\n",
    "plt.plot(y_test_original, label='Actual')\n",
    "plt.plot(y_pred, label='Predicted')\n",
    "plt.legend()\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Energy Consumption')\n",
    "plt.title('Actual vs Predicted Energy Consumption')\n",
    "plt.savefig('/home/u894059/.local/Hybrid_MTG_daily.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.6.1 Hybrid Model Optimiser - Short Term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of optimizers to test\n",
    "optimizers = {\n",
    "    \"adam\": Adam,\n",
    "    \"sgd\": SGD,\n",
    "    \"rmsprop\": RMSprop,\n",
    "    \"adagrad\": Adagrad,\n",
    "    \"adamax\": Adamax,\n",
    "    \"nadam\": Nadam\n",
    "}\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Load dataset\n",
    "stf = pd.read_csv('/home/u894059/.local/dataset/short_term.csv')\n",
    "\n",
    "# Select the features and target variable\n",
    "features = stf.drop(columns=['total_daily_energy_consumption'])\n",
    "target = stf['total_daily_energy_consumption'].values.reshape(-1, 1)  # reshape for scaling\n",
    "\n",
    "# Preprocess the data\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(features, target, test_size=0.2, shuffle=False)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.125, shuffle=False)\n",
    "\n",
    "# Normalize the data\n",
    "feature_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "target_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "X_train_scaled = feature_scaler.fit_transform(X_train)\n",
    "X_val_scaled = feature_scaler.transform(X_val)\n",
    "X_test_scaled = feature_scaler.transform(X_test)\n",
    "\n",
    "y_train_scaled = target_scaler.fit_transform(y_train).flatten()\n",
    "y_val_scaled = target_scaler.transform(y_val).flatten()\n",
    "y_test_scaled = target_scaler.transform(y_test).flatten()\n",
    "\n",
    "# Create sliding window sequences\n",
    "def create_sequences(data, target, sequence_length):\n",
    "    sequences, targets = [], []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        sequences.append(data[i:i + sequence_length])\n",
    "        targets.append(target[i + sequence_length])\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "# Experiment with longer sequences\n",
    "sequence_length = 5\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled, sequence_length)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val_scaled, y_val_scaled, sequence_length)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_scaled, sequence_length)\n",
    "\n",
    "# LSTM Block\n",
    "def lstm_block(inputs, lstm_units):\n",
    "    x = layers.LSTM(lstm_units, return_sequences=True, kernel_regularizer='l2')(inputs)\n",
    "    x = layers.Dropout(0.3)(x)  # Increased dropout for regularization\n",
    "    return x\n",
    "\n",
    "# Transformer Encoder\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout_rate, num_blocks):\n",
    "    for _ in range(num_blocks):\n",
    "        # Multi-Head Attention\n",
    "        x = layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads)(inputs, inputs)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(x + inputs)  # Residual Connection\n",
    "        \n",
    "        # Feedforward Block\n",
    "        ff = layers.Conv1D(filters=ff_dim, kernel_size=1, activation='relu')(x)\n",
    "        ff = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(ff)\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(x + ff)  # Residual Connection\n",
    "    \n",
    "    return x\n",
    "\n",
    "# Build the RS-LSTM-Transformer Model\n",
    "def build_model(input_shape, lstm_units, head_size, num_heads, ff_dim, num_blocks, dropout_rate):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # LSTM Block\n",
    "    lstm_out = lstm_block(inputs, lstm_units)\n",
    "    \n",
    "    # Transformer Encoder\n",
    "    transformer_out = transformer_encoder(lstm_out, head_size, num_heads, ff_dim, dropout_rate, num_blocks)\n",
    "    \n",
    "    # Global Average Pooling + Dense Layers\n",
    "    gap = layers.GlobalAveragePooling1D()(transformer_out)\n",
    "    dense = layers.Dense(64, activation='relu')(gap)\n",
    "    outputs = layers.Dense(1)(dense)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "lstm_units = 130\n",
    "head_size = 66\n",
    "num_heads = 10\n",
    "ff_dim = 133\n",
    "num_blocks = 3\n",
    "dropout_rate = 0.11640596573903705\n",
    "learning_rate = 0.00039155660875927047\n",
    "batch_size = 55\n",
    "\n",
    "# Iterate over each optimizer\n",
    "for opt_name in optimizers:\n",
    "    optimizer = optimizers[opt_name]\n",
    "    print(f\"Using optimizer: {opt_name}\")\n",
    "\n",
    "    input_shape = (sequence_length, features.shape[1])\n",
    "    model = build_model(input_shape, lstm_units, head_size, num_heads, ff_dim, num_blocks, dropout_rate)\n",
    "    \n",
    "    model.compile(optimizer=opt_name, loss='mse', metrics=['mae'])\n",
    "    \n",
    "    # Callbacks for early stopping and learning rate reduction\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train_seq, y_train_seq,\n",
    "        validation_data=(X_val_seq, y_val_seq),\n",
    "        epochs=50,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=[early_stopping, reduce_lr],\n",
    "        verbose=1 \n",
    "    )\n",
    "    # Evaluate the model on the test set\n",
    "    test_loss = model.evaluate(X_test_seq, y_test_seq, verbose=1)\n",
    "\n",
    "    # Get predictions on the validation set\n",
    "    y_pred_scaled = model.predict(X_test_seq)\n",
    "    \n",
    "    # Calculate metrics on the scaled values\n",
    "    mse_scaled = mean_squared_error(y_test_seq, y_pred_scaled)\n",
    "    rmse_scaled = np.sqrt(mse_scaled)\n",
    "    mae_scaled = mean_absolute_error(y_test_seq, y_pred_scaled)\n",
    "\n",
    "    # Store results\n",
    "    results[opt_name] = {\n",
    "        \"Test Loss (MSE)\": test_loss,\n",
    "        \"RMSE (scaled)\": rmse_scaled,\n",
    "        \"MAE (scaled)\": mae_scaled\n",
    "    }\n",
    "\n",
    "# Display results for comparison\n",
    "print(\"\\nOptimizer Performance Comparison:\")\n",
    "for opt_name, metrics in results.items():\n",
    "    print(f\"{opt_name}: {metrics}\")\n",
    "\n",
    "# Example Plot (Using the best optimizer's predictions)\n",
    "best_optimizer = min(results, key=lambda k: results[k][\"RMSE (scaled)\"])\n",
    "print(f\"\\nBest optimizer based on RMSE: {best_optimizer}\")\n",
    "\n",
    "# Re-train and visualize using the best optimizer\n",
    "model.compile(optimizer=best_optimizer, loss='mse')\n",
    "model.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val_seq, y_val_seq),\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n",
    "\n",
    "final_predictions_scaled = model.predict(X_test_seq)\n",
    "final_predictions = target_scaler.inverse_transform(final_predictions_scaled.reshape(-1, 1)).flatten()\n",
    "y_test_original = target_scaler.inverse_transform(y_test_seq.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Plot the actual vs predicted values\n",
    "plt.plot(y_test_original, label='Actual')\n",
    "plt.plot(final_predictions, label='Predicted')\n",
    "plt.legend()\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Energy Consumption')\n",
    "plt.title('Actual vs Predicted Energy Consumption')\n",
    "plt.savefig('/home/u894059/.local/plot1.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.6.2 Hybrid Model Optimiser - Mid Term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of optimizers to test\n",
    "optimizers = {\n",
    "    \"adam\": Adam,\n",
    "    \"sgd\": SGD,\n",
    "    \"rmsprop\": RMSprop,\n",
    "    \"adagrad\": Adagrad,\n",
    "    \"adamax\": Adamax,\n",
    "    \"nadam\": Nadam\n",
    "}\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Load dataset\n",
    "mtf = pd.read_csv('/home/u894059/.local/dataset/mid_term.csv')\n",
    "\n",
    "# Select the features and target variable\n",
    "features = mtf.drop(columns=['total_weekly_energy_consumption'])\n",
    "target = mtf['total_weekly_energy_consumption'].values.reshape(-1, 1)  # reshape for scaling\n",
    "\n",
    "# Preprocess the data\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(features, target, test_size=0.2, shuffle=False)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.125, shuffle=False)\n",
    "\n",
    "# Normalize the data\n",
    "feature_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "target_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "X_train_scaled = feature_scaler.fit_transform(X_train)\n",
    "X_val_scaled = feature_scaler.transform(X_val)\n",
    "X_test_scaled = feature_scaler.transform(X_test)\n",
    "\n",
    "y_train_scaled = target_scaler.fit_transform(y_train).flatten()\n",
    "y_val_scaled = target_scaler.transform(y_val).flatten()\n",
    "y_test_scaled = target_scaler.transform(y_test).flatten()\n",
    "\n",
    "# Create sliding window sequences\n",
    "def create_sequences(data, target, sequence_length):\n",
    "    sequences, targets = [], []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        sequences.append(data[i:i + sequence_length])\n",
    "        targets.append(target[i + sequence_length])\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "# Experiment with longer sequences\n",
    "sequence_length = 5\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled, sequence_length)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val_scaled, y_val_scaled, sequence_length)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_scaled, sequence_length)\n",
    "\n",
    "# LSTM Block\n",
    "def lstm_block(inputs, lstm_units):\n",
    "    x = layers.LSTM(lstm_units, return_sequences=True, kernel_regularizer='l2')(inputs)\n",
    "    x = layers.Dropout(0.3)(x)  # Increased dropout for regularization\n",
    "    return x\n",
    "\n",
    "# Transformer Encoder\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout_rate, num_blocks):\n",
    "    for _ in range(num_blocks):\n",
    "        # Multi-Head Attention\n",
    "        x = layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads)(inputs, inputs)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(x + inputs)  # Residual Connection\n",
    "        \n",
    "        # Feedforward Block\n",
    "        ff = layers.Conv1D(filters=ff_dim, kernel_size=1, activation='relu')(x)\n",
    "        ff = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(ff)\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(x + ff)  # Residual Connection\n",
    "    \n",
    "    return x\n",
    "\n",
    "# Build the RS-LSTM-Transformer Model\n",
    "def build_model(input_shape, lstm_units, head_size, num_heads, ff_dim, num_blocks, dropout_rate):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # LSTM Block\n",
    "    lstm_out = lstm_block(inputs, lstm_units)\n",
    "    \n",
    "    # Transformer Encoder\n",
    "    transformer_out = transformer_encoder(lstm_out, head_size, num_heads, ff_dim, dropout_rate, num_blocks)\n",
    "    \n",
    "    # Global Average Pooling + Dense Layers\n",
    "    gap = layers.GlobalAveragePooling1D()(transformer_out)\n",
    "    dense = layers.Dense(64, activation='relu')(gap)\n",
    "    outputs = layers.Dense(1)(dense)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "lstm_units = 98\n",
    "head_size = 86\n",
    "num_heads = 9\n",
    "ff_dim = 134\n",
    "num_blocks = 4\n",
    "dropout_rate = 0.42901614838022517\n",
    "learning_rate = 2.018835159150557e-05\n",
    "batch_size = 17\n",
    "\n",
    "# Iterate over each optimizer\n",
    "for opt_name in optimizers:\n",
    "    optimizer = optimizers[opt_name]\n",
    "    print(f\"Using optimizer: {opt_name}\")\n",
    "\n",
    "    input_shape = (sequence_length, features.shape[1])\n",
    "    model = build_model(input_shape, lstm_units, head_size, num_heads, ff_dim, num_blocks, dropout_rate)\n",
    "    \n",
    "    model.compile(optimizer=opt_name, loss='mse', metrics=['mae'])\n",
    "    \n",
    "    # Callbacks for early stopping and learning rate reduction\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train_seq, y_train_seq,\n",
    "        validation_data=(X_val_seq, y_val_seq),\n",
    "        epochs=50,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=[early_stopping, reduce_lr],\n",
    "        verbose=1 \n",
    "    )\n",
    "    # Evaluate the model on the test set\n",
    "    test_loss = model.evaluate(X_test_seq, y_test_seq, verbose=1)\n",
    "\n",
    "    # Get predictions on the validation set\n",
    "    y_pred_scaled = model.predict(X_test_seq)\n",
    "    \n",
    "    # Calculate metrics on the scaled values\n",
    "    mse_scaled = mean_squared_error(y_test_seq, y_pred_scaled)\n",
    "    rmse_scaled = np.sqrt(mse_scaled)\n",
    "    mae_scaled = mean_absolute_error(y_test_seq, y_pred_scaled)\n",
    "\n",
    "    # Store results\n",
    "    results[opt_name] = {\n",
    "        \"Test Loss (MSE)\": test_loss,\n",
    "        \"RMSE (scaled)\": rmse_scaled,\n",
    "        \"MAE (scaled)\": mae_scaled\n",
    "    }\n",
    "\n",
    "# Display results for comparison\n",
    "print(\"\\nOptimizer Performance Comparison:\")\n",
    "for opt_name, metrics in results.items():\n",
    "    print(f\"{opt_name}: {metrics}\")\n",
    "\n",
    "# Example Plot (Using the best optimizer's predictions)\n",
    "best_optimizer = min(results, key=lambda k: results[k][\"RMSE (scaled)\"])\n",
    "print(f\"\\nBest optimizer based on RMSE: {best_optimizer}\")\n",
    "\n",
    "# Re-train and visualize using the best optimizer\n",
    "model.compile(optimizer=best_optimizer, loss='mse')\n",
    "model.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val_seq, y_val_seq),\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n",
    "\n",
    "final_predictions_scaled = model.predict(X_test_seq)\n",
    "final_predictions = target_scaler.inverse_transform(final_predictions_scaled.reshape(-1, 1)).flatten()\n",
    "y_test_original = target_scaler.inverse_transform(y_test_seq.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Plot the actual vs predicted values\n",
    "plt.plot(y_test_original, label='Actual')\n",
    "plt.plot(final_predictions, label='Predicted')\n",
    "plt.legend()\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Energy Consumption')\n",
    "plt.title('Actual vs Predicted Energy Consumption')\n",
    "plt.savefig('/home/u894059/.local/plot1.png')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
